{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aec042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import re # Added for cleaning plot names\n",
    "import json # Added to handle potential JSON loading if paths were used\n",
    "\n",
    "# Set default plotly template for better aesthetics\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# Create directory for CSV exports if it doesn't exist\n",
    "csv_export_dir = \"plot_data_csv_exports\"\n",
    "os.makedirs(csv_export_dir, exist_ok=True)\n",
    "\n",
    "# --- Helper function for cleaning model names for plots ---\n",
    "def clean_plot_name(name):\n",
    "    \"\"\"Cleans model names for display in plots.\"\"\"\n",
    "    if name is None: # Handle potential None input\n",
    "        return \"Unknown\"\n",
    "    name_str = str(name)\n",
    "\n",
    "    # For merged models, remove the trailing _XX\n",
    "    # This condition handles names like \"Linear_24\" -> \"Linear\"\n",
    "    if not name_str.startswith(\"Qwen2.5\"):\n",
    "        name_str = re.sub(r'_\\d+$', '', name_str)\n",
    "    return name_str\n",
    "\n",
    "# --- Font configuration for plots ---\n",
    "font_config = {\n",
    "    \"title_font_size\": 24,\n",
    "    \"font_size\": 18,\n",
    "    \"xaxis_title_font_size\": 18,\n",
    "    \"yaxis_title_font_size\": 18,\n",
    "    \"xaxis_tickfont_size\": 16,\n",
    "    \"yaxis_tickfont_size\": 16,\n",
    "    \"legend_title_font_size\": 24,\n",
    "    \"legend_font_size\": 22,\n",
    "}\n",
    "\n",
    "# --- Default Plot Dimensions ---\n",
    "default_plot_height = 520\n",
    "default_plot_width = 2300\n",
    "\n",
    "# --- Helper Function from process_results.py (adapted) ---\n",
    "def process_frame(frame):\n",
    "    \"\"\"\n",
    "    Processes the DataFrame by selecting relevant columns,\n",
    "    handling potential renaming and grouping data.\n",
    "    \"\"\"\n",
    "    if \"Unnamed: 0\" in frame.columns:\n",
    "        del frame[\"Unnamed: 0\"]\n",
    "\n",
    "    # Standardize column names for linguistic competencies\n",
    "    if \"linguistic subfield\" in frame.columns and \"linguistic competencies\" not in frame.columns:\n",
    "        frame[\"linguistic competencies\"] = frame[\"linguistic subfield\"]\n",
    "        del frame[\"linguistic subfield\"]\n",
    "    elif \"linguistic subfield\" in frame.columns and \"linguistic competencies\" in frame.columns:\n",
    "        del frame[\"linguistic subfield\"]\n",
    "    return frame\n",
    "\n",
    "# --- Core Analysis Function ---\n",
    "def run_analysis_for_experiment(experiment_name, models, short_names, abs_data_file, group_data_file,\n",
    "                                main_task_col, subtask_col, subtask_group_col, subtask_phenomena_col):\n",
    "    \"\"\"\n",
    "    Runs the full data loading and analysis pipeline for a given experiment.\n",
    "    Returns the processed summary and subtasks dataframes.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\" PROCESSING EXPERIMENT: {experiment_name.upper()} \")\n",
    "    print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "    # --- 1. Model Categorization ---\n",
    "    instruct_model = None\n",
    "    specialist_model = None # Coder or Math model\n",
    "    merged_models = []\n",
    "    base_model = None\n",
    "\n",
    "    for m_full_name in models:\n",
    "        m_short = short_names.get(m_full_name, \"\")\n",
    "        is_instruct = (m_short == \"Qwen2.5 Instruct\")\n",
    "        is_specialist = (m_short.startswith(\"Qwen2.5 Coder\") or m_short.startswith(\"Qwen2.5 Math\"))\n",
    "        is_base = (m_short == \"Qwen2.5 Base\")\n",
    "        is_merged = not (is_instruct or is_specialist or is_base)\n",
    "\n",
    "        if is_instruct:\n",
    "            instruct_model = m_full_name\n",
    "        elif is_specialist:\n",
    "            specialist_model = m_full_name\n",
    "        elif is_base:\n",
    "            base_model = m_full_name\n",
    "        elif is_merged:\n",
    "            if m_full_name in models:\n",
    "                merged_models.append(m_full_name)\n",
    "\n",
    "    if not instruct_model: print(\"CRITICAL ERROR: Instruct model not identified.\"); exit()\n",
    "    if not specialist_model: print(f\"CRITICAL ERROR: Specialist model for {experiment_name} not identified.\"); exit()\n",
    "\n",
    "    print(f\"--- Model Categorization ({experiment_name}) ---\")\n",
    "    if base_model: print(f\"Base Model: {base_model} ({short_names.get(base_model, 'N/A')})\")\n",
    "    print(f\"Instruct Model: {instruct_model} ({short_names.get(instruct_model, 'N/A')})\")\n",
    "    print(f\"Specialist Model: {specialist_model} ({short_names.get(specialist_model, 'N/A')})\")\n",
    "    print(f\"Merged Models ({len(merged_models)}):\")\n",
    "    for m in merged_models: print(f\"  - {m} ({short_names.get(m, 'N/A')})\")\n",
    "    print(\"-\" * 25)\n",
    "\n",
    "    comparison_models_ordered = []\n",
    "    if base_model: comparison_models_ordered.append(base_model)\n",
    "    if instruct_model: comparison_models_ordered.append(instruct_model)\n",
    "    if specialist_model: comparison_models_ordered.append(specialist_model)\n",
    "    comparison_models_ordered.extend([m for m in merged_models if m])\n",
    "    comparison_models = list(dict.fromkeys(m for m in comparison_models_ordered if m))\n",
    "    print(f\"Models for comparison: {[clean_plot_name(short_names.get(m, m)) for m in comparison_models]}\")\n",
    "    print(\"-\" * 25)\n",
    "\n",
    "    # --- 2. Data Loading ---\n",
    "    def load_data_from_csv(abs_filepath, group_filepath, model_list):\n",
    "        try:\n",
    "            raw_abs_df = pd.read_csv(abs_filepath)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Absolute scores file not found at {abs_filepath}\")\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "        if \"Unnamed: 0\" in raw_abs_df.columns:\n",
    "            del raw_abs_df[\"Unnamed: 0\"]\n",
    "\n",
    "        raw_abs_df.rename(columns={'probing_dataset': 'subtask_cleaned', 'model_name': 'model'}, inplace=True)\n",
    "        raw_abs_df = raw_abs_df[raw_abs_df['encoding'] == 'full'].copy()\n",
    "        raw_abs_df['score'] = pd.to_numeric(raw_abs_df['score'], errors='coerce')\n",
    "        abs_df_grouped = raw_abs_df.groupby(['subtask_cleaned', 'model'])['score'].mean().reset_index()\n",
    "        abs_pivot_df = abs_df_grouped.pivot_table(index='subtask_cleaned', columns='model', values='score').reset_index()\n",
    "\n",
    "        for model_col in model_list:\n",
    "            if model_col not in abs_pivot_df.columns:\n",
    "                abs_pivot_df[model_col] = np.nan\n",
    "\n",
    "        cols_to_keep_abs = ['subtask_cleaned'] + [m for m in model_list if m in abs_pivot_df.columns]\n",
    "        abs_final_df = abs_pivot_df[cols_to_keep_abs].copy()\n",
    "\n",
    "        try:\n",
    "            raw_group_df = pd.read_csv(group_filepath)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Group info file not found at {group_filepath}.\")\n",
    "            return pd.DataFrame(), abs_final_df\n",
    "\n",
    "        group_df_processed = process_frame(raw_group_df.copy())\n",
    "        rename_map_group = {\n",
    "            subtask_col: 'subtask_cleaned',\n",
    "            subtask_group_col: 'group',\n",
    "            main_task_col: 'main_task_category'\n",
    "        }\n",
    "        group_df_processed.rename(columns=rename_map_group, inplace=True)\n",
    "        \n",
    "        id_cols_group = ['main_task_category', 'subtask_cleaned', 'group']\n",
    "        id_cols_group_present = [col for col in id_cols_group if col in group_df_processed.columns]\n",
    "        group_info_to_merge = group_df_processed[id_cols_group_present].drop_duplicates(subset=['subtask_cleaned'])\n",
    "        \n",
    "        subtasks_df = pd.merge(abs_final_df, group_info_to_merge, on='subtask_cleaned', how='left')\n",
    "\n",
    "        models_in_subtasks_df = [m for m in model_list if m in subtasks_df.columns]\n",
    "        for model_col in models_in_subtasks_df:\n",
    "            subtasks_df[model_col] = pd.to_numeric(subtasks_df[model_col], errors='coerce') * 100\n",
    "\n",
    "        summary_df = pd.DataFrame()\n",
    "        # CORRECTED: Group by the 'group' column which corresponds to 'linguistic competencies' for the summary\n",
    "        if 'group' in subtasks_df.columns and models_in_subtasks_df:\n",
    "            # UPDATED LOGIC: Filter out tasks that couldn't be mapped to a competency group (where 'group' is NaN)\n",
    "            known_subtasks = subtasks_df.dropna(subset=['group']).copy()\n",
    "            \n",
    "            # Additional check to remove any literal 'nan' strings if they exist after conversion\n",
    "            known_subtasks = known_subtasks[known_subtasks['group'].astype(str).str.lower() != 'nan']\n",
    "\n",
    "            if not known_subtasks.empty:\n",
    "                summary_df = known_subtasks.groupby('group')[models_in_subtasks_df].mean()\n",
    "                # Rename the index to 'main_task_category' as the rest of the script expects this name\n",
    "                summary_df.index.name = 'main_task_category'\n",
    "        \n",
    "        return summary_df, subtasks_df\n",
    "\n",
    "    summary_df, subtasks_df = load_data_from_csv(abs_data_file, group_data_file, comparison_models)\n",
    "\n",
    "    print(f\"\\n--- Summary DataFrame ({experiment_name}) ---\")\n",
    "    if not summary_df.empty: print(summary_df.head())\n",
    "    else: print(\"Summary DataFrame is empty.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    return summary_df, subtasks_df, comparison_models, instruct_model, specialist_model, base_model\n",
    "\n",
    "# --- Configuration ---\n",
    "abs_data_file = \"results_flash-holmes.csv\"\n",
    "group_data_file = \"transformed_results.csv\"\n",
    "MAIN_TASK_COL = \"probing dataset\"\n",
    "SUBTASK_COL = \"probe\"\n",
    "SUBTASK_GROUP_COL = \"linguistic competencies\"\n",
    "SUBTASK_PHENOMENA_COL = \"linguistic phenomena\"\n",
    "\n",
    "# --- CODER Experiment Config ---\n",
    "models_coder = [\n",
    "    \"Qwen__Qwen2.5-7B\", \"Qwen__Qwen2.5-7B-Instruct\", \"Qwen__Qwen2.5-Coder-7B\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-task_arithmetic-29\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-dare_ties-29\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-ties-29\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-slerp-29\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-linear-29\"\n",
    "]\n",
    "short_names_coder = {\n",
    "    \"Qwen__Qwen2.5-7B\": \"Qwen2.5 Base\",\n",
    "    \"Qwen__Qwen2.5-7B-Instruct\": \"Qwen2.5 Instruct\",\n",
    "    \"Qwen__Qwen2.5-Coder-7B\": \"Qwen2.5 Coder\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-task_arithmetic-29\": \"Task Arithmetic_29\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-dare_ties-29\": \"DARE Ties_29\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-ties-29\": \"Ties_29\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-slerp-29\": \"Slerp_29\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-linear-29\": \"Linear_29\"\n",
    "}\n",
    "\n",
    "# --- MATH Experiment Config ---\n",
    "models_math = [\n",
    "    \"Qwen__Qwen2.5-7B\", \"Qwen__Qwen2.5-7B-Instruct\", \"Qwen__Qwen2.5-Math-7B\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-task_arithmetic-26\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-dare_ties-27\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-ties-26\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-slerp-24\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-linear-24\"\n",
    "]\n",
    "short_names_math = {\n",
    "    \"Qwen__Qwen2.5-7B\": \"Qwen2.5 Base\",\n",
    "    \"Qwen__Qwen2.5-7B-Instruct\": \"Qwen2.5 Instruct\",\n",
    "    \"Qwen__Qwen2.5-Math-7B\": \"Qwen2.5 Math\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-task_arithmetic-26\": \"Task Arithmetic_26\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-dare_ties-27\": \"DARE Ties_27\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-ties-26\": \"Ties_26\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-slerp-24\": \"Slerp_24\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-linear-24\": \"Linear_24\"\n",
    "}\n",
    "\n",
    "# --- Run Analysis for Both Experiments ---\n",
    "summary_df_coder, _, comparison_models_coder, instruct_model_coder, specialist_model_coder, base_model_coder = run_analysis_for_experiment(\n",
    "    \"Coder\", models_coder, short_names_coder, abs_data_file, group_data_file, MAIN_TASK_COL, SUBTASK_COL, SUBTASK_GROUP_COL, SUBTASK_PHENOMENA_COL\n",
    ")\n",
    "summary_df_math, _, comparison_models_math, instruct_model_math, specialist_model_math, base_model_math = run_analysis_for_experiment(\n",
    "    \"Math\", models_math, short_names_math, abs_data_file, group_data_file, MAIN_TASK_COL, SUBTASK_COL, SUBTASK_GROUP_COL, SUBTASK_PHENOMENA_COL\n",
    ")\n",
    "\n",
    "# --- Combined Plotting Section ---\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" GENERATING COMBINED PLOT \")\n",
    "print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "def prepare_plot_data(df, comparison_models, short_names_map, base_model, instruct_model, specialist_model):\n",
    "    \"\"\"Prepares data for the combined faceted plot from a summary dataframe.\"\"\"\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    models_to_plot = [m for m in comparison_models if m in df.columns and not df[m].isna().all()]\n",
    "    \n",
    "    # Melt the dataframe to long format\n",
    "    df_melted = df.reset_index().melt(\n",
    "        id_vars='main_task_category', \n",
    "        value_vars=models_to_plot,\n",
    "        var_name='model_full_name',\n",
    "        value_name='Score'\n",
    "    )\n",
    "    df_melted.rename(columns={'main_task_category': 'Task'}, inplace=True)\n",
    "\n",
    "    # Map short names and model types\n",
    "    df_melted['Model Short Name'] = df_melted['model_full_name'].map(lambda x: clean_plot_name(short_names_map.get(x, x)))\n",
    "    \n",
    "    def get_model_type(full_name):\n",
    "        if full_name == base_model: return \"Base\"\n",
    "        if full_name == instruct_model: return \"Instruct\"\n",
    "        if full_name == specialist_model: return \"Specialist\"\n",
    "        return \"Merged\"\n",
    "        \n",
    "    df_melted['Model Type'] = df_melted['model_full_name'].apply(get_model_type)\n",
    "    \n",
    "    return df_melted\n",
    "\n",
    "# Prepare data for both experiments\n",
    "plot_df_coder = prepare_plot_data(summary_df_coder, comparison_models_coder, short_names_coder, base_model_coder, instruct_model_coder, specialist_model_coder)\n",
    "plot_df_math = prepare_plot_data(summary_df_math, comparison_models_math, short_names_math, base_model_math, instruct_model_math, specialist_model_math)\n",
    "\n",
    "# Define model order with core models at the bottom\n",
    "all_tasks = sorted(list(set(plot_df_coder['Task'].unique()) | set(plot_df_math['Task'].unique())))\n",
    "\n",
    "# For Coder plot y-axis order\n",
    "all_coder_models = plot_df_coder['Model Short Name'].unique()\n",
    "core_coder = ['Qwen2.5 Base', 'Qwen2.5 Instruct', 'Qwen2.5 Coder']\n",
    "core_coder_present = [m for m in core_coder if m in all_coder_models]\n",
    "merged_coder = sorted([m for m in all_coder_models if m not in core_coder_present])\n",
    "models_coder_plot_order = merged_coder + core_coder_present[::-1] # Reverse core for bottom display\n",
    "\n",
    "# For Math plot y-axis order\n",
    "all_math_models = plot_df_math['Model Short Name'].unique()\n",
    "core_math = ['Qwen2.5 Base', 'Qwen2.5 Instruct', 'Qwen2.5 Math']\n",
    "core_math_present = [m for m in core_math if m in all_math_models]\n",
    "merged_math = sorted([m for m in all_math_models if m not in core_math_present])\n",
    "models_math_plot_order = merged_math + core_math_present[::-1] # Reverse core for bottom display\n",
    "\n",
    "\n",
    "if not all_tasks or (plot_df_coder.empty and plot_df_math.empty):\n",
    "    print(\"No data available to generate the combined plot. Exiting plot generation.\")\n",
    "else:\n",
    "    # UPDATED: Create subplot titles for each competency\n",
    "    subplot_titles = [f\"<b>{task}</b>\" for task in all_tasks for _ in (1,2)]\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=len(all_tasks),\n",
    "        cols=2,\n",
    "        subplot_titles=subplot_titles,\n",
    "        horizontal_spacing=0.02,\n",
    "        vertical_spacing=0.04 # Reduced spacing\n",
    "    )\n",
    "\n",
    "    color_map = {\n",
    "        \"Base\": 'rgb(100, 149, 237)',\n",
    "        \"Instruct\": 'rgb(50, 205, 50)',\n",
    "        \"Specialist\": 'rgb(255, 165, 0)',\n",
    "        \"Merged\": 'rgb(192, 192, 192)'\n",
    "    }\n",
    "    \n",
    "    # Loop through each task to create a row of subplots\n",
    "    for i, task in enumerate(all_tasks):\n",
    "        row_num = i + 1\n",
    "        \n",
    "        # --- Column 1: Coder ---\n",
    "        df_c = plot_df_coder[plot_df_coder['Task'] == task]\n",
    "        if not df_c.empty:\n",
    "            # Text styling logic\n",
    "            text_threshold = 20\n",
    "            text_positions = ['outside' if score < text_threshold else 'inside' for score in df_c['Score']]\n",
    "            text_colors = ['black' if score < text_threshold else 'white' for score in df_c['Score']]\n",
    "\n",
    "            fig.add_trace(go.Bar(\n",
    "                y=df_c['Model Short Name'],\n",
    "                x=df_c['Score'],\n",
    "                marker_color=[color_map.get(t) for t in df_c['Model Type']],\n",
    "                orientation='h',\n",
    "                text=df_c.apply(lambda row: f\"<b>{row['Model Short Name']}</b>: {row['Score']:.1f}%\", axis=1),\n",
    "                textposition=text_positions,\n",
    "                textfont=dict(size=12, color=text_colors),\n",
    "                insidetextanchor='middle',\n",
    "                cliponaxis=False,\n",
    "                hoverinfo='none'\n",
    "            ), row=row_num, col=1)\n",
    "            \n",
    "            instruct_score = summary_df_coder.loc[task, instruct_model_coder] if task in summary_df_coder.index else np.nan\n",
    "            specialist_score = summary_df_coder.loc[task, specialist_model_coder] if task in summary_df_coder.index else np.nan\n",
    "            \n",
    "            if pd.notna(instruct_score) and pd.notna(specialist_score):\n",
    "                fig.add_shape(type=\"rect\", x0=min(instruct_score, specialist_score), x1=max(instruct_score, specialist_score), y0=-0.5, y1=len(models_coder_plot_order)-0.5, fillcolor=\"rgba(255, 128, 128, 0.2)\", line_width=0, layer=\"below\", row=row_num, col=1)\n",
    "                fig.add_shape(type=\"line\", x0=instruct_score, x1=instruct_score, y0=-0.5, y1=len(models_coder_plot_order)-0.5, line=dict(color=color_map['Instruct'], dash=\"dash\", width=2), layer=\"above\", row=row_num, col=1)\n",
    "                fig.add_shape(type=\"line\", x0=specialist_score, x1=specialist_score, y0=-0.5, y1=len(models_coder_plot_order)-0.5, line=dict(color=color_map['Specialist'], dash=\"dash\", width=2), layer=\"above\", row=row_num, col=1)\n",
    "\n",
    "            max_score_c = df_c['Score'].max()\n",
    "            fig.update_xaxes(range=[0, max_score_c * 1.25], row=row_num, col=1)\n",
    "\n",
    "        # --- Column 2: Math ---\n",
    "        df_m = plot_df_math[plot_df_math['Task'] == task]\n",
    "        if not df_m.empty:\n",
    "            # Text styling logic\n",
    "            text_threshold = 20\n",
    "            text_positions = ['outside' if score < text_threshold else 'inside' for score in df_m['Score']]\n",
    "            text_colors = ['black' if score < text_threshold else 'white' for score in df_m['Score']]\n",
    "\n",
    "            fig.add_trace(go.Bar(\n",
    "                y=df_m['Model Short Name'],\n",
    "                x=df_m['Score'],\n",
    "                marker_color=[color_map.get(t) for t in df_m['Model Type']],\n",
    "                orientation='h',\n",
    "                text=df_m.apply(lambda row: f\"<b>{row['Model Short Name']}</b>: {row['Score']:.1f}%\", axis=1),\n",
    "                textposition=text_positions,\n",
    "                textfont=dict(size=12, color=text_colors),\n",
    "                insidetextanchor='middle',\n",
    "                cliponaxis=False,\n",
    "                hoverinfo='none'\n",
    "            ), row=row_num, col=2)\n",
    "\n",
    "            instruct_score = summary_df_math.loc[task, instruct_model_math] if task in summary_df_math.index else np.nan\n",
    "            specialist_score = summary_df_math.loc[task, specialist_model_math] if task in summary_df_math.index else np.nan\n",
    "\n",
    "            if pd.notna(instruct_score) and pd.notna(specialist_score):\n",
    "                fig.add_shape(type=\"rect\", x0=min(instruct_score, specialist_score), x1=max(instruct_score, specialist_score), y0=-0.5, y1=len(models_math_plot_order)-0.5, fillcolor=\"rgba(255, 128, 128, 0.2)\", line_width=0, layer=\"below\", row=row_num, col=2)\n",
    "                fig.add_shape(type=\"line\", x0=instruct_score, x1=instruct_score, y0=-0.5, y1=len(models_math_plot_order)-0.5, line=dict(color=color_map['Instruct'], dash=\"dash\", width=2), layer=\"above\", row=row_num, col=2)\n",
    "                fig.add_shape(type=\"line\", x0=specialist_score, x1=specialist_score, y0=-0.5, y1=len(models_math_plot_order)-0.5, line=dict(color=color_map['Specialist'], dash=\"dash\", width=2), layer=\"above\", row=row_num, col=2)\n",
    "\n",
    "            max_score_m = df_m['Score'].max()\n",
    "            fig.update_xaxes(range=[0, max_score_m * 1.25], row=row_num, col=2)\n",
    "\n",
    "    # Update layout for the entire figure\n",
    "    # UPDATED: Increased height per bar and adjusted facet height calculation\n",
    "    height_per_facet = max(len(models_coder_plot_order), len(models_math_plot_order)) * 25 + 40 \n",
    "    total_height = height_per_facet * len(all_tasks)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text='<b>Mean Absolute Performance by Linguistic Competency: Coder vs. Math Experiments</b>',\n",
    "        height=total_height,\n",
    "        width=2000,\n",
    "        showlegend=False,\n",
    "        margin=dict(t=120, l=10, r=10, b=50), # Increased top margin for titles\n",
    "        plot_bgcolor='white',\n",
    "        **font_config\n",
    "    )\n",
    "\n",
    "    # Update all axes and text properties\n",
    "    fig.update_yaxes(categoryorder='array', categoryarray=models_coder_plot_order, showticklabels=False, col=1)\n",
    "    fig.update_yaxes(categoryorder='array', categoryarray=models_math_plot_order, showticklabels=False, col=2)\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGray', zeroline=False)\n",
    "    \n",
    "    # Format subplot titles\n",
    "    fig.for_each_annotation(lambda a: a.update(font=dict(size=16)))\n",
    "    \n",
    "    # Add main column titles\n",
    "    fig.add_annotation(x=0.25, y=1.02, yanchor='bottom', text=\"<b>Coder Experiment</b>\", showarrow=False, xref=\"paper\", yref=\"paper\", font=dict(size=20))\n",
    "    fig.add_annotation(x=0.75, y=1.02, yanchor='bottom', text=\"<b>Math Experiment</b>\", showarrow=False, xref=\"paper\", yref=\"paper\", font=dict(size=20))\n",
    "    \n",
    "    fig.show()\n",
    "    print(\"Generated combined plot: Mean Absolute Performance by Linguistic Competency\")\n",
    "    \n",
    "    # Export the plot data to CSV\n",
    "    plot_df_coder['Experiment'] = 'Coder'\n",
    "    plot_df_math['Experiment'] = 'Math'\n",
    "    combined_plot_data = pd.concat([plot_df_coder, plot_df_math])\n",
    "    csv_filename = os.path.join(csv_export_dir, \"combined_performance_coder_vs_math.csv\")\n",
    "    combined_plot_data.to_csv(csv_filename, index=False)\n",
    "    print(f\"Exported combined plot data to: {csv_filename}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
