{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb2cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import re # Added for cleaning plot names\n",
    "\n",
    "# Set default plotly template for better aesthetics if needed\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# --- Helper function for cleaning model names for plots ---\n",
    "def clean_plot_name(name):\n",
    "    if name is None: # Handle potential None input\n",
    "        return \"Unknown\"\n",
    "    name_str = str(name)\n",
    "\n",
    "    # For merged models (which won't start with \"Qwen2.5\" after short_names mapping)\n",
    "    # remove the trailing _XX\n",
    "    # Also handles cases where \"Merged_\" might still be there if short_names wasn't fully applied before this function\n",
    "    if not name_str.startswith(\"Qwen2.5\") or \"Merged_\" in name_str : # Handles names like \"Linear_24\" -> \"Linear\" or \"Merged_Linear_24\" -> \"Merged_Linear\"\n",
    "        name_str = re.sub(r'_\\d+$', '', name_str)\n",
    "    return name_str\n",
    "\n",
    "# --- Font configuration for plots ---\n",
    "font_config = {\n",
    "    \"title_font_size\": 30,\n",
    "    \"font_size\": 20,\n",
    "    \"xaxis_title_font_size\": 20,\n",
    "    \"yaxis_title_font_size\": 20,\n",
    "    \"xaxis_tickfont_size\": 20, # Adjusted for potentially dense plots\n",
    "    \"yaxis_tickfont_size\": 20, # Adjusted for potentially dense plots\n",
    "    \"legend_title_font_size\": 20, # Kept for other plots that might use legends\n",
    "    \"legend_font_size\": 15,       # Kept for other plots\n",
    "}\n",
    "# --- Default Plot Dimensions ---\n",
    "default_plot_height = 400\n",
    "default_plot_width = 1400 # Added for wider plots\n",
    "\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "\n",
    "# --- Paths and Model Definitions ---\n",
    "# Please define the models and their short names here, which you want to analyze.\n",
    "models = [\n",
    "    \"Qwen2.5-7B\",\n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-Coder-7B\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-task_arithmetic-29\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-dare_ties-29\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-ties-29\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-slerp-29\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-linear-29\"\n",
    "]\n",
    "\n",
    "# Updated short_names for harness script\n",
    "# Della and DARE_Ties entries have been removed\n",
    "short_names = {\n",
    "    \"Qwen2.5-7B\": \"Qwen2.5 Base\",\n",
    "    \"Qwen2.5-7B-Instruct\": \"Qwen2.5 Instruct\",\n",
    "    \"Qwen2.5-Coder-7B\": \"Qwen2.5 Coder\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-task_arithmetic-29\": \"Task Arithmetic\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-dare_ties-29\": \"DARE Ties\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-ties-25\": \"Ties\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-slerp-24\": \"Slerp\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-linear-24\": \"Linear \"\n",
    "}\n",
    "\n",
    "# MODIFIED: Define the specific leaderboard tasks to be treated as main tasks\n",
    "leaderboard_main_tasks = [\n",
    "    \"leaderboard_mmlu_pro\",\n",
    "    \"leaderboard_bbh\",\n",
    "    \"leaderboard_gpqa\",\n",
    "    \"leaderboard_Coder_hard\",\n",
    "    \"leaderboard_ifeval\",\n",
    "    \"leaderboard_musr\"\n",
    "]\n",
    "# MODIFIED: This list now defines the order and names of tasks for the entire analysis\n",
    "tasks = [t.replace('leaderboard_', '') for t in leaderboard_main_tasks]\n",
    "# MODIFIED: The path points to the overall leaderboard result file, which contains the sub-group scores we need.\n",
    "tasks_for_paths = [\"leaderboard\"]\n",
    "paths = {m: {t: f\"organized_results/{t}/{m}/result.json\" for t in tasks_for_paths} for m in models}\n",
    "\n",
    "\n",
    "# Updated model categorization logic for harness script\n",
    "instruct_model = None; coder_model = None; merged_models = []; base_model = None\n",
    "for m_full_name in models: # Iterate through the original full model names\n",
    "    m_short = short_names.get(m_full_name, \"\") # Get the NEW short name\n",
    "\n",
    "    is_instruct = (m_short == \"Qwen2.5 Instruct\")\n",
    "    is_coder = (m_short == \"Qwen2.5 Coder\")\n",
    "    is_base = (m_short == \"Qwen2.5 Base\")\n",
    "    \n",
    "    # A model is considered merged if its short name does not match Base, Instruct, or Coder\n",
    "    is_merged = not (is_instruct or is_coder or is_base)\n",
    "\n",
    "    if is_instruct:\n",
    "        instruct_model = m_full_name\n",
    "    elif is_coder:\n",
    "        coder_model = m_full_name\n",
    "    elif is_base:\n",
    "        base_model = m_full_name\n",
    "    elif is_merged:\n",
    "        # This ensures we only add models from our initial `models` list\n",
    "        # that are intended to be merged.\n",
    "        if m_full_name in models:\n",
    "            merged_models.append(m_full_name)\n",
    "\n",
    "\n",
    "if not instruct_model: print(\"CRITICAL ERROR: Instruct model not identified.\"); exit()\n",
    "if not coder_model: print(\"CRITICAL ERROR: Coder model not identified.\"); exit()\n",
    "if not merged_models: print(\"WARNING: No merged models identified.\")\n",
    "if not base_model: print(\"WARNING: Base model not identified in harness script setup.\")\n",
    "\n",
    "\n",
    "print(\"--- Model Categorization (Harness) ---\")\n",
    "if base_model: print(f\"Base Model: {base_model} ({short_names.get(base_model, 'N/A')})\")\n",
    "print(f\"Instruct Model: {instruct_model} ({short_names.get(instruct_model, 'N/A')})\")\n",
    "print(f\"Coder Model: {coder_model} ({short_names.get(coder_model, 'N/A')})\")\n",
    "print(f\"Merged Models ({len(merged_models)}):\")\n",
    "for m in merged_models: print(f\"  - {m} ({short_names.get(m, 'N/A')})\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Define the order and inclusion for comparison_models\n",
    "comparison_models_ordered = []\n",
    "if base_model:\n",
    "    comparison_models_ordered.append(base_model)\n",
    "if instruct_model:\n",
    "    comparison_models_ordered.append(instruct_model)\n",
    "if coder_model:\n",
    "    comparison_models_ordered.append(coder_model)\n",
    "comparison_models_ordered.extend([m for m in merged_models if m])\n",
    "comparison_models = list(dict.fromkeys(m for m in comparison_models_ordered if m)) # Filter out None and duplicates\n",
    "\n",
    "print(f\"Models for comparison (in order): {[clean_plot_name(short_names.get(m, m)) for m in comparison_models]}\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# --- 2. Data Loading ---\n",
    "\n",
    "# MODIFIED: This function now loads the scores for the sub-groups within the leaderboard files.\n",
    "def load_leaderboard_with_groups(paths_dict, model_list):\n",
    "    agg = defaultdict(dict); inv_group = {}\n",
    "    # Use the 'leaderboard' path for all models\n",
    "    leaderboard_paths = {m: paths_dict.get(m, {}).get('leaderboard') for m in model_list}\n",
    "    leaderboard_paths = {m: p for m, p in leaderboard_paths.items() if p}\n",
    "    if not any(os.path.isfile(fp) for fp in leaderboard_paths.values() if fp):\n",
    "        return pd.DataFrame(columns=['subtask'] + model_list + ['group'])\n",
    "    first_valid_file_checked_for_groups = False\n",
    "    for m, fp in leaderboard_paths.items():\n",
    "        if not fp or not os.path.isfile(fp): continue\n",
    "        try:\n",
    "            with open(fp, 'r') as f: data = json.load(f)\n",
    "            if 'group_subtasks' in data and not inv_group and not first_valid_file_checked_for_groups:\n",
    "                for grp, subs in data['group_subtasks'].items():\n",
    "                    clean_grp_name = grp.replace('leaderboard_', '') if isinstance(grp, str) else grp\n",
    "                    for sub in subs: inv_group[sub] = clean_grp_name\n",
    "                first_valid_file_checked_for_groups = True\n",
    "            for key, metrics in data.get('results', {}).items():\n",
    "                # We are interested in keys that are the leaderboard sub-groups themselves\n",
    "                if isinstance(key, str) and key.startswith('leaderboard_') and key != 'leaderboard':\n",
    "                    score = metrics.get('acc_norm,none', metrics.get('acc,none', metrics.get('exact_match,none', np.nan)))\n",
    "                    if not pd.isna(score): agg[key][m] = score * 100\n",
    "        except Exception as e: print(f\"Error processing file {fp} for model {m}: {e}\")\n",
    "    if not agg: return pd.DataFrame(columns=['subtask'] + model_list + ['group'])\n",
    "    df = pd.DataFrame.from_dict(agg, orient='index')\n",
    "    for m_col in model_list:\n",
    "        if m_col not in df.columns: df[m_col] = np.nan\n",
    "    present_models_in_agg = [m for m in model_list if m in df.columns]; df = df[present_models_in_agg]\n",
    "    df = df.dropna(subset=present_models_in_agg, how='all')\n",
    "    if df.empty: return pd.DataFrame(columns=['subtask'] + model_list + ['group', 'subtask_cleaned'])\n",
    "    df['group'] = df.index.map(lambda x: inv_group.get(x, 'Unknown'))\n",
    "    df['subtask_cleaned'] = df.index.str.replace('leaderboard_', '', regex=False)\n",
    "    final_cols = ['group', 'subtask_cleaned'] + present_models_in_agg\n",
    "    return df.reset_index().rename(columns={'index': 'subtask'})[final_cols + ['subtask']]\n",
    "\n",
    "# MODIFIED: Data loading logic is changed to use the filtered leaderboard sub-groups as the main tasks.\n",
    "all_subtasks_df = load_leaderboard_with_groups(paths, models)\n",
    "\n",
    "if not all_subtasks_df.empty and 'subtask' in all_subtasks_df.columns:\n",
    "    summary_df = all_subtasks_df[all_subtasks_df['subtask'].isin(leaderboard_main_tasks)].copy()\n",
    "    summary_df.set_index('subtask_cleaned', inplace=True)\n",
    "    model_cols_present = [m for m in models if m in summary_df.columns]\n",
    "    summary_df = summary_df[model_cols_present]\n",
    "    # Ensure the order of tasks (rows) matches the one defined at the start\n",
    "    summary_df = summary_df.reindex(tasks)\n",
    "else:\n",
    "    summary_df = pd.DataFrame()\n",
    "\n",
    "# The concept of a separate, more granular subtask analysis is removed.\n",
    "subtasks_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "models_in_summary_data = [m for m in comparison_models if m in summary_df.columns]\n",
    "summary_comp_df = summary_df.loc[:, models_in_summary_data].copy() if models_in_summary_data else pd.DataFrame()\n",
    "\n",
    "# This will now be empty, which is correct for the new analysis focus.\n",
    "subtasks_comp_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "print(\"\\n--- Summary DataFrame (Comparison Models) ---\"); print(summary_comp_df); print(\"-\" * 50)\n",
    "if not subtasks_comp_df.empty: print(\"\\n--- Subtasks DataFrame (Comparison Models Head) ---\"); print(subtasks_comp_df.head()); print(\"-\" * 50)\n",
    "else: print(\"\\n--- Subtasks DataFrame is empty (This is expected as analysis is focused on main leaderboard tasks) ---\")\n",
    "\n",
    "\n",
    "# --- 3. Calculate Differences ---\n",
    "can_calc_diffs = True\n",
    "if instruct_model not in summary_comp_df.columns or coder_model not in summary_comp_df.columns:\n",
    "    print(\"Warning: Instruct or Coder model data missing from summary_comp_df. Difference calculations involving them will be skipped or result in NaN.\"); can_calc_diffs = False\n",
    "diff_cols_main = []; diff_cols_subtasks = []\n",
    "\n",
    "if can_calc_diffs:\n",
    "    if instruct_model in summary_comp_df.columns and coder_model in summary_comp_df.columns:\n",
    "        summary_comp_df['d_coder'] = summary_comp_df[instruct_model] - summary_comp_df[coder_model]; diff_cols_main.append('d_coder')\n",
    "    else:\n",
    "        summary_comp_df['d_coder'] = np.nan\n",
    "\n",
    "    for merged_m in merged_models:\n",
    "        if merged_m in summary_comp_df.columns and instruct_model in summary_comp_df.columns:\n",
    "            merged_short_name = clean_plot_name(short_names.get(merged_m, merged_m)) # Apply clean_plot_name\n",
    "            col_name = f\"d_merged_{merged_short_name}\"\n",
    "            summary_comp_df[col_name] = summary_comp_df[instruct_model] - summary_comp_df[merged_m]; diff_cols_main.append(col_name)\n",
    "        \n",
    "    print(\"\\n--- Summary DataFrame with Differences ---\");\n",
    "    diff_cols_main_present = [col for col in diff_cols_main if col in summary_comp_df.columns]\n",
    "    if diff_cols_main_present: print(summary_comp_df[diff_cols_main_present])\n",
    "    else: print(\"No difference columns to show for main tasks.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # This block will be skipped as subtasks_comp_df is empty.\n",
    "    if not subtasks_comp_df.empty:\n",
    "        if instruct_model in subtasks_comp_df.columns and coder_model in subtasks_comp_df.columns:\n",
    "            subtasks_comp_df['d_coder'] = subtasks_comp_df[instruct_model] - subtasks_comp_df[coder_model]; diff_cols_subtasks.append('d_coder')\n",
    "        else: subtasks_comp_df['d_coder'] = np.nan\n",
    "\n",
    "        for merged_m in merged_models:\n",
    "            merged_short_name = clean_plot_name(short_names.get(merged_m, merged_m)) # Apply clean_plot_name\n",
    "            col_name = f\"d_merged_{merged_short_name}\"\n",
    "            if instruct_model in subtasks_comp_df.columns and merged_m in subtasks_comp_df.columns:\n",
    "                subtasks_comp_df[col_name] = subtasks_comp_df[instruct_model] - subtasks_comp_df[merged_m]; diff_cols_subtasks.append(col_name)\n",
    "            \n",
    "        print(\"\\n--- Subtasks DataFrame with Differences (Head) ---\")\n",
    "        diff_cols_sub_present = [col for col in diff_cols_subtasks if col in subtasks_comp_df.columns]\n",
    "        cols_to_show_sub_diff = [c for c in ['subtask_cleaned', 'group'] + models_in_subtasks_data + diff_cols_sub_present if c in subtasks_comp_df.columns]\n",
    "        if cols_to_show_sub_diff: print(subtasks_comp_df[cols_to_show_sub_diff].head())\n",
    "        else: print(\"No difference columns to show for subtasks or base columns missing.\")\n",
    "        print(\"-\" * 50)\n",
    "else: print(\"Skipping difference calculations as instruct or coder model data is critically missing from summary_comp_df.\")\n",
    "\n",
    "\n",
    "# --- 4. Ranking Generation ---\n",
    "def generate_rankings(summary_data, subtask_data, model_names_for_ranking, short_names_map):\n",
    "    ranking_results = {};\n",
    "    models_in_summary_for_ranking = [m for m in model_names_for_ranking if m in summary_data.columns]\n",
    "\n",
    "    if not models_in_summary_for_ranking:\n",
    "        print(\"No models available in summary_data for ranking.\")\n",
    "    else:\n",
    "        main_rankings = []\n",
    "        for task_item in summary_data.index:\n",
    "            scores = summary_data.loc[task_item, models_in_summary_for_ranking]; ranked_models = scores.sort_values(ascending=False, na_position='last').index.tolist()\n",
    "            ranked_short_names = [clean_plot_name(short_names_map.get(m, m)) for m in ranked_models]; row = {'Task': task_item} # Apply clean_plot_name\n",
    "            for i, name in enumerate(ranked_short_names): row[f'Rank {i+1}'] = name\n",
    "            main_rankings.append(row)\n",
    "        if main_rankings: ranking_results['main_tasks'] = pd.DataFrame(main_rankings).set_index('Task')\n",
    "        else: ranking_results['main_tasks'] = pd.DataFrame()\n",
    "\n",
    "    # This block will be skipped as subtask_data is empty\n",
    "    if not subtask_data.empty:\n",
    "        models_in_subtasks_for_ranking = [m for m in model_names_for_ranking if m in subtask_data.columns]\n",
    "        if not models_in_subtasks_for_ranking:\n",
    "            print(\"No models available in subtask_data for ranking.\")\n",
    "        else:\n",
    "            subtask_rankings = []; subtask_name_col = 'subtask_cleaned' if 'subtask_cleaned' in subtask_data.columns else 'subtask'; group_col = 'group' if 'group' in subtask_data.columns else None\n",
    "            for idx, row_data in subtask_data.iterrows():\n",
    "                if isinstance(row_data, pd.Series) and all(m in row_data.index for m in models_in_subtasks_for_ranking):\n",
    "                        scores = row_data[models_in_subtasks_for_ranking]; ranked_models = scores.sort_values(ascending=False, na_position='last').index.tolist()\n",
    "                        ranked_short_names = [clean_plot_name(short_names_map.get(m, m)) for m in ranked_models]; row = {'Subtask': row_data[subtask_name_col]} # Apply clean_plot_name\n",
    "                        if group_col and group_col in row_data.index: row['Group'] = row_data[group_col]\n",
    "                        for i, name in enumerate(ranked_short_names): row[f'Rank {i+1}'] = name\n",
    "                        subtask_rankings.append(row)\n",
    "            if subtask_rankings:\n",
    "                rank_df_sub = pd.DataFrame(subtask_rankings)\n",
    "                base_cols = ['Subtask'] + (['Group'] if group_col and 'Group' in rank_df_sub.columns else []); rank_cols = [f'Rank {i+1}' for i in range(len(models_in_subtasks_for_ranking))]\n",
    "                cols_order = base_cols + rank_cols\n",
    "                for c in cols_order:\n",
    "                    if c not in rank_df_sub.columns: rank_df_sub[c] = np.nan\n",
    "                rank_df_sub = rank_df_sub[cols_order]\n",
    "                if group_col and 'Group' in rank_df_sub.columns: rank_df_sub = rank_df_sub.sort_values(by=['Group', 'Subtask']).set_index(['Group', 'Subtask'])\n",
    "                else: rank_df_sub = rank_df_sub.set_index('Subtask')\n",
    "                ranking_results['subtasks'] = rank_df_sub\n",
    "            else: ranking_results['subtasks'] = pd.DataFrame()\n",
    "\n",
    "\n",
    "            if 'subtasks' in ranking_results and group_col and models_in_subtasks_for_ranking and 'group' in subtask_data.columns:\n",
    "                try:\n",
    "                    avg_scores_group = subtask_data.groupby('group')[models_in_subtasks_for_ranking].mean(numeric_only=True); group_rankings = []\n",
    "                    for group_name_iter in avg_scores_group.index:\n",
    "                        scores = avg_scores_group.loc[group_name_iter]; ranked_models = scores.sort_values(ascending=False, na_position='last').index.tolist()\n",
    "                        ranked_short_names = [clean_plot_name(short_names_map.get(m, m)) for m in ranked_models]; row = {'Group': group_name_iter} # Apply clean_plot_name\n",
    "                        for i, name in enumerate(ranked_short_names): row[f'Rank {i+1}'] = name\n",
    "                        group_rankings.append(row)\n",
    "                    if group_rankings: ranking_results['group_avg'] = pd.DataFrame(group_rankings).set_index('Group')\n",
    "                    else: ranking_results['group_avg'] = pd.DataFrame()\n",
    "                except Exception as e: print(f\"Could not calculate group average rankings: {e}\")\n",
    "    return ranking_results\n",
    "\n",
    "rankings = generate_rankings(summary_comp_df, subtasks_comp_df, comparison_models, short_names)\n",
    "print(\"\\n\" + \"=\"*20 + \" MODEL RANKINGS \" + \"=\"*20)\n",
    "if 'main_tasks' in rankings and not rankings['main_tasks'].empty: print(\"\\n--- Main Task Rankings ---\"); print(rankings['main_tasks'])\n",
    "if 'subtasks' in rankings and not rankings['subtasks'].empty: print(\"\\n--- Subtask Rankings ---\"); print(rankings['subtasks'].head(10))\n",
    "if 'group_avg' in rankings and not rankings['group_avg'].empty: print(\"\\n--- Group Average Rankings ---\"); print(rankings['group_avg'])\n",
    "output_dir_harness = \"rankings_output_harness\"\n",
    "os.makedirs(output_dir_harness, exist_ok=True); print(f\"\\n--- Saving Rankings to CSV in '{output_dir_harness}/' ---\")\n",
    "for name, df_rank in rankings.items():\n",
    "    if isinstance(df_rank, pd.DataFrame) and not df_rank.empty:\n",
    "        try: csv_filename = os.path.join(output_dir_harness, f\"{name}_rankings_harness.csv\"); df_rank.to_csv(csv_filename, index=True); print(f\"Saved {name} rankings to {csv_filename}\")\n",
    "        except Exception as e: print(f\"Error saving {name} rankings: {e}\")\n",
    "print(\"=\"*58)\n",
    "\n",
    "\n",
    "# --- 5. Merged Model Performance Categorization & Task Scenario Table ---\n",
    "\n",
    "def generate_performance_analysis(df_analyze, df_name_suffix, task_id_col, instruct_m, coder_m, merged_m_list, short_names_map, out_dir):\n",
    "    print(f\"\\n--- Merged Model Performance Categorization for: {df_name_suffix} ---\")\n",
    "    analysis_results = []\n",
    "    task_scenario_data = []\n",
    "\n",
    "    if not (instruct_m and coder_m and instruct_m in df_analyze.columns and coder_m in df_analyze.columns):\n",
    "        print(f\"Skipping analysis for {df_name_suffix}: Instruct or Coder model data missing from DataFrame.\")\n",
    "        return\n",
    "\n",
    "    if task_id_col is None: # For summary_df, index is the task\n",
    "        task_iterable = df_analyze.index\n",
    "        get_task_name_from_val = lambda task_val: task_val\n",
    "        get_task_data_row_from_val = lambda task_val: df_analyze.loc[task_val]\n",
    "    elif task_id_col in df_analyze.columns: # For subtasks_df\n",
    "        task_iterable = df_analyze[task_id_col].unique()\n",
    "        get_task_name_from_val = lambda task_val: task_val\n",
    "        get_task_data_row_from_val = lambda task_val: df_analyze[df_analyze[task_id_col] == task_val].iloc[0] if not df_analyze[df_analyze[task_id_col] == task_val].empty else None\n",
    "    else:\n",
    "        print(f\"Error: Task identifier '{task_id_col}' not found for {df_name_suffix}.\")\n",
    "        return\n",
    "\n",
    "    task_categorization_for_table = {get_task_name_from_val(task_val): {\"Better_than_both\": [], \"Worse_than_both\": [], \"Between_Equal\": []}\n",
    "                                         for task_val in task_iterable}\n",
    "\n",
    "    for merged_model_full_name in merged_m_list:\n",
    "        if merged_model_full_name in df_analyze.columns:\n",
    "            better_count, worse_count, between_count = 0, 0, 0\n",
    "            merged_model_short_name = short_names_map.get(merged_model_full_name, merged_model_full_name)\n",
    "            cleaned_merged_model_short_name_for_plot = clean_plot_name(merged_model_short_name)\n",
    "\n",
    "            for task_value in task_iterable:\n",
    "                task_data_row = get_task_data_row_from_val(task_value)\n",
    "                if task_data_row is None: continue\n",
    "\n",
    "                merged_score = task_data_row[merged_model_full_name]\n",
    "                instruct_score = task_data_row[instruct_m]\n",
    "                coder_score = task_data_row[coder_m]\n",
    "\n",
    "                if pd.isna(merged_score) or pd.isna(instruct_score) or pd.isna(coder_score):\n",
    "                    continue\n",
    "\n",
    "                min_im = min(instruct_score, coder_score)\n",
    "                max_im = max(instruct_score, coder_score)\n",
    "                current_task_name = get_task_name_from_val(task_value)\n",
    "\n",
    "                if merged_score > max_im:\n",
    "                    better_count += 1\n",
    "                    task_categorization_for_table[current_task_name][\"Better_than_both\"].append(cleaned_merged_model_short_name_for_plot)\n",
    "                elif merged_score < min_im:\n",
    "                    worse_count += 1\n",
    "                    task_categorization_for_table[current_task_name][\"Worse_than_both\"].append(cleaned_merged_model_short_name_for_plot)\n",
    "                elif min_im <= merged_score <= max_im:\n",
    "                    between_count += 1\n",
    "                    task_categorization_for_table[current_task_name][\"Between_Equal\"].append(cleaned_merged_model_short_name_for_plot)\n",
    "\n",
    "            analysis_results.append({\n",
    "                \"Merged Model\": cleaned_merged_model_short_name_for_plot,\n",
    "                \"Better than Instruct & Coder\": better_count,\n",
    "                \"Worse than Instruct & Coder\": worse_count,\n",
    "                \"Between/Equal to Instruct & Coder\": between_count,\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Skipping categorization for {merged_model_full_name} ({df_name_suffix}) as it's not in the DataFrame.\")\n",
    "\n",
    "    for task_name_iter, categories in task_categorization_for_table.items():\n",
    "        task_scenario_data.append({\n",
    "            \"Task\": task_name_iter,\n",
    "            \"Better_Count\": len(categories[\"Better_than_both\"]),\n",
    "            \"Better_Models\": \", \".join(sorted(list(set(categories[\"Better_than_both\"])))),\n",
    "            \"Worse_Count\": len(categories[\"Worse_than_both\"]),\n",
    "            \"Worse_Models\": \", \".join(sorted(list(set(categories[\"Worse_than_both\"])))),\n",
    "            \"Between_Equal_Count\": len(categories[\"Between_Equal\"]),\n",
    "            \"Between_Equal_Models\": \", \".join(sorted(list(set(categories[\"Between_Equal\"])))),\n",
    "        })\n",
    "    task_scenario_df = pd.DataFrame(task_scenario_data)\n",
    "    print(f\"\\n--- Task Scenario Ranking Table ({df_name_suffix}) ---\")\n",
    "    print(task_scenario_df.head())\n",
    "    try:\n",
    "        task_scenario_csv_filename = os.path.join(out_dir, f\"task_scenario_rankings_{df_name_suffix.lower().replace(' ','_')}.csv\")\n",
    "        task_scenario_df.to_csv(task_scenario_csv_filename, index=False)\n",
    "        print(f\"Saved task scenario rankings for {df_name_suffix} to {task_scenario_csv_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving task scenario rankings for {df_name_suffix} to CSV: {e}\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "    if analysis_results:\n",
    "        counts_df = pd.DataFrame(analysis_results)\n",
    "        print(f\"\\n--- Counts of Merged Model Performance Categories ({df_name_suffix} Overall) ---\")\n",
    "        print(counts_df)\n",
    "        counts_df_melted = counts_df.melt(id_vars=\"Merged Model\",\n",
    "                                          value_vars=[\"Better than Instruct & Coder\", \"Worse than Instruct & Coder\", \"Between/Equal to Instruct & Coder\"],\n",
    "                                          var_name=\"Category\", value_name=\"Number of Tasks\")\n",
    "        fig_counts_title = f\"Merged Model Performance vs. Instruct & Coder ({df_name_suffix} Overall)\"\n",
    "        fig_counts = px.bar(counts_df_melted, x=\"Merged Model\", y=\"Number of Tasks\", color=\"Category\",\n",
    "                            title=fig_counts_title,\n",
    "                            barmode='stack',\n",
    "                            labels={\"Number of Tasks\": f\"Number of {df_name_suffix.replace('Subtasks', 'Subtasks')}\"})\n",
    "        fig_counts.update_xaxes(categoryorder=\"array\", categoryarray=counts_df[\"Merged Model\"].tolist())\n",
    "        fig_counts.update_layout(\n",
    "            height=default_plot_height + 50, \n",
    "            width=default_plot_width, \n",
    "            legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.3, xanchor=\"center\", x=0.5),\n",
    "            **font_config\n",
    "        )\n",
    "        fig_counts.show()\n",
    "        print(f\"Generated plot: {fig_counts_title}\")\n",
    "\n",
    "        if len(counts_df) > 1:\n",
    "            pivot_counts_df = counts_df.set_index(\"Merged Model\")\n",
    "            categories_to_correlate = [\"Better than Instruct & Coder\", \"Worse than Instruct & Coder\", \"Between/Equal to Instruct & Coder\"]\n",
    "            pivot_counts_df_filtered = pivot_counts_df[[cat for cat in categories_to_correlate if cat in pivot_counts_df.columns]]\n",
    "            if len(pivot_counts_df_filtered.columns) > 0 and len(pivot_counts_df_filtered) >1:\n",
    "                correlation_matrix = pivot_counts_df_filtered.T.corr()\n",
    "                print(f\"\\n--- Correlation Matrix of Performance Categories Between Merged Models ({df_name_suffix} Overall) ---\")\n",
    "                print(correlation_matrix)\n",
    "                fig_corr_heatmap_title = f\"Correlation of Perf. Categories Between Merged Models ({df_name_suffix})\"\n",
    "                fig_corr_heatmap = px.imshow(correlation_matrix, text_auto=True, aspect=\"auto\",\n",
    "                                             color_continuous_scale='RdBu_r', range_color=[-1,1],\n",
    "                                             title=fig_corr_heatmap_title)\n",
    "                fig_corr_heatmap.update_layout(width=default_plot_width, height=600, **font_config)\n",
    "                fig_corr_heatmap.show()\n",
    "                print(f\"Generated plot: {fig_corr_heatmap_title}\")\n",
    "            else:\n",
    "                print(f\"Not enough data or categories to calculate category correlation for {df_name_suffix}.\")\n",
    "        else:\n",
    "            print(f\"Not enough merged models with data to calculate category correlation for {df_name_suffix}.\")\n",
    "    else:\n",
    "        print(f\"No merged model comparison counts generated for {df_name_suffix}.\")\n",
    "    print(\"=\"*58)\n",
    "\n",
    "if not summary_comp_df.empty:\n",
    "    generate_performance_analysis(\n",
    "        summary_comp_df,\n",
    "        df_name_suffix=\"MainTasks\",\n",
    "        task_id_col=None, # Indicates using DataFrame index for tasks\n",
    "        instruct_m=instruct_model,\n",
    "        coder_m=coder_model,\n",
    "        merged_m_list=merged_models,\n",
    "        short_names_map=short_names,\n",
    "        out_dir=output_dir_harness\n",
    "    )\n",
    "\n",
    "# This block will be skipped.\n",
    "if not subtasks_comp_df.empty and 'subtask_cleaned' in subtasks_comp_df.columns:\n",
    "    generate_performance_analysis(\n",
    "        subtasks_comp_df,\n",
    "        df_name_suffix=\"LeaderboardSubtasks\",\n",
    "        task_id_col='subtask_cleaned', # Column name for subtask identifiers\n",
    "        instruct_m=instruct_model,\n",
    "        coder_m=coder_model,\n",
    "        merged_m_list=merged_models,\n",
    "        short_names_map=short_names,\n",
    "        out_dir=output_dir_harness\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping performance categorization for Leaderboard Subtasks: DataFrame is empty.\")\n",
    "print(\"=\"*58)\n",
    "\n",
    "\n",
    "# --- 6. Original Plotting Section ---\n",
    "print(\"\\n--- Generating Original Plots (Harness Data) ---\")\n",
    "instruct_short_label = clean_plot_name(short_names.get(instruct_model, \"Instruct\")) # Will be \"Qwen2.5 Instruct\"\n",
    "coder_short_label = clean_plot_name(short_names.get(coder_model, \"Coder\")) # Will be \"Qwen2.5 Coder\"\n",
    "\n",
    "# Plot: Difference Trends on Main Tasks (vs Instruct)\n",
    "if not summary_comp_df.empty and 'd_coder' in summary_comp_df.columns :\n",
    "    fig1 = go.Figure()\n",
    "    if 'd_coder' in summary_comp_df.columns and not summary_comp_df['d_coder'].isna().all():\n",
    "        fig1.add_trace(go.Scatter(x=summary_comp_df.index, y=summary_comp_df['d_coder'], mode='lines+markers', name=f'{instruct_short_label}–{coder_short_label}', marker=dict(symbol='circle', size=8), line=dict(dash='dash'), hovertemplate='Task: %{x}<br>Difference: %{y:.2f}%<extra></extra>'))\n",
    "\n",
    "    colors = px.colors.qualitative.Plotly; merged_plot_idx = 0\n",
    "    for diff_col in diff_cols_main:\n",
    "        if diff_col.startswith('d_merged_') and diff_col in summary_comp_df.columns and not summary_comp_df[diff_col].isna().all():\n",
    "            merged_short_name_plot = diff_col.replace('d_merged_', '')\n",
    "            cleaned_merged_name_plot = clean_plot_name(merged_short_name_plot) # Ensures _XX is removed\n",
    "            fig1.add_trace(go.Scatter(x=summary_comp_df.index, y=summary_comp_df[diff_col], mode='lines+markers', name=f'{instruct_short_label}–{cleaned_merged_name_plot}', marker=dict(symbol='square', size=8, color=colors[merged_plot_idx % len(colors)]), hovertemplate='Task: %{x}<br>Difference: %{y:.2f}%<extra></extra>')); merged_plot_idx +=1\n",
    "        \n",
    "    if fig1.data:\n",
    "        fig1_title = 'Difference Trends on Main Tasks (vs Instruct) - All Merged'\n",
    "        fig1.update_layout(\n",
    "            title=fig1_title,\n",
    "            xaxis_title='Task', yaxis_title='Performance Difference (%)',\n",
    "            legend_title_text='Difference Type', hovermode='x unified',\n",
    "            width=default_plot_width, \n",
    "            **font_config\n",
    "        )\n",
    "        fig1.show()\n",
    "        print(f\"Generated plot: {fig1_title}\")\n",
    "    else: print(\"Skipping plot 'Difference Trends on Main Tasks': No data to plot.\")\n",
    "else: print(\"Skipping plot 'Difference Trends on Main Tasks': summary_comp_df is empty or 'd_coder' column missing.\")\n",
    "\n",
    "\n",
    "# Plot: Absolute Performance on Main Tasks (Line Chart)\n",
    "if not summary_comp_df.empty:\n",
    "    fig1_abs = go.Figure(); colors_line = px.colors.qualitative.Plotly\n",
    "    models_to_plot_abs = [m for m in comparison_models if m in summary_comp_df.columns and not summary_comp_df[m].isna().all()]\n",
    "\n",
    "    for plot_idx, model_name_abs in enumerate(models_to_plot_abs):\n",
    "        short_name_abs = short_names.get(model_name_abs, model_name_abs)\n",
    "        cleaned_short_name_abs = clean_plot_name(short_name_abs) # Apply cleaning\n",
    "        \n",
    "        current_symbol = 'circle'\n",
    "        current_line_style = 'solid'\n",
    "\n",
    "        if model_name_abs == base_model:\n",
    "            current_symbol = 'star'\n",
    "            current_line_style = 'dashdot'\n",
    "        elif model_name_abs == instruct_model:\n",
    "            current_symbol = 'circle'\n",
    "            current_line_style = 'solid'\n",
    "        elif model_name_abs == coder_model:\n",
    "            current_symbol = 'diamond'\n",
    "            current_line_style = 'dash'\n",
    "        elif model_name_abs in merged_models:\n",
    "            current_symbol = 'square'\n",
    "            current_line_style = 'dot'\n",
    "                \n",
    "        fig1_abs.add_trace(go.Scatter(x=summary_comp_df.index, y=summary_comp_df[model_name_abs],\n",
    "                                      mode='lines+markers', name=cleaned_short_name_abs,\n",
    "                                      marker=dict(symbol=current_symbol, size=8, color=colors_line[plot_idx % len(colors_line)]),\n",
    "                                      line=dict(dash=current_line_style),\n",
    "                                      hovertemplate='Task: %{x}<br>Score: %{y:.2f}%<extra></extra>'))\n",
    "    if fig1_abs.data:\n",
    "        fig1_abs_title = 'Absolute Performance on Main Tasks (Line Chart)'\n",
    "        fig1_abs.update_layout(\n",
    "            title=fig1_abs_title,\n",
    "            xaxis_title='Task', yaxis_title='Performance Score (%)',\n",
    "            legend_title_text='Model', hovermode='x unified',\n",
    "            width=default_plot_width, \n",
    "            **font_config\n",
    "        )\n",
    "        fig1_abs.show()\n",
    "        print(f\"Generated plot: {fig1_abs_title}\")\n",
    "    else: print(\"Skipping plot 'Absolute Performance on Main Tasks (Line Chart)': No data to plot.\")\n",
    "else: print(\"Skipping plot 'Absolute Performance on Main Tasks (Line Chart)': summary_comp_df is empty.\")\n",
    "\n",
    "# New Plot: Absolute Performance Comparison on Main Tasks (Faceted Horizontal Bar Chart with Shaded Area)\n",
    "if not summary_comp_df.empty:\n",
    "    models_to_plot_bar = [m for m in comparison_models if m in summary_comp_df.columns and not summary_comp_df[m].isna().all()]\n",
    "    if models_to_plot_bar:\n",
    "        \n",
    "        # Define color map based on NEW short names for legend/coloring\n",
    "        color_map_specific = {}\n",
    "        if base_model: color_map_specific[\"Qwen2.5 Base\"] = 'rgb(100, 149, 237)'\n",
    "        if instruct_model: color_map_specific[\"Qwen2.5 Instruct\"] = 'rgb(50, 205, 50)'\n",
    "        if coder_model: color_map_specific[\"Qwen2.5 Coder\"] = 'rgb(255, 165, 0)'\n",
    "        color_map_specific['Merged'] = 'rgb(192, 192, 192)' # For all merged types\n",
    "\n",
    "        plot_data_list = []\n",
    "        for task_idx, task_name in enumerate(tasks):\n",
    "            if task_name in summary_comp_df.index:\n",
    "                for model_idx, model_full_name in enumerate(comparison_models):\n",
    "                    if model_full_name in models_to_plot_bar:\n",
    "                        score = summary_comp_df.loc[task_name, model_full_name]\n",
    "                        if not pd.isna(score):\n",
    "                            model_short_clean = clean_plot_name(short_names.get(model_full_name, model_full_name))\n",
    "                            \n",
    "                            # Determine Model Type for coloring based on new short names\n",
    "                            current_model_type = 'Merged' # Default\n",
    "                            s_name = short_names.get(model_full_name)\n",
    "                            if s_name == \"Qwen2.5 Base\": current_model_type = \"Qwen2.5 Base\"\n",
    "                            elif s_name == \"Qwen2.5 Instruct\": current_model_type = \"Qwen2.5 Instruct\"\n",
    "                            elif s_name == \"Qwen2.5 Coder\": current_model_type = \"Qwen2.5 Coder\"\n",
    "                            \n",
    "                            plot_data_list.append({\n",
    "                                'Task': task_name,\n",
    "                                'Model Short Name': model_short_clean, # For Y-axis\n",
    "                                'Score': score,\n",
    "                                'Model Type': current_model_type, # For color\n",
    "                                'Task Index': task_idx,\n",
    "                                'Model Index': model_idx\n",
    "                            })\n",
    "        \n",
    "        if not plot_data_list:\n",
    "            print(\"Skipping 'Absolute Performance Comparison on Main Tasks (Faceted Bar Chart)': No data to plot after filtering.\")\n",
    "        else:\n",
    "            horizontal_bar_df = pd.DataFrame(plot_data_list)\n",
    "            horizontal_bar_df.sort_values(by=['Task Index', 'Model Index'], ascending=[True, True], inplace=True)\n",
    "\n",
    "            fig_main_bar_faceted_title = 'Absolute Performance Comparison by Task'\n",
    "            fig_main_bar_faceted = px.bar(\n",
    "                horizontal_bar_df,\n",
    "                x='Score',\n",
    "                y='Model Short Name',\n",
    "                color='Model Type', # Use the determined model type for color\n",
    "                color_discrete_map=color_map_specific,\n",
    "                orientation='h',\n",
    "                title=fig_main_bar_faceted_title,\n",
    "                labels={'Score': 'Performance Score (%)', 'Model Short Name': 'Model', 'Model Type': 'Model Category'},\n",
    "                text='Score',\n",
    "                facet_row='Task',\n",
    "                category_orders={\"Task\": tasks}\n",
    "            )\n",
    "            \n",
    "            fig_main_bar_faceted.update_traces(\n",
    "                texttemplate='%{text:.2f}%',\n",
    "                textposition='outside'\n",
    "            )\n",
    "            \n",
    "            model_order_for_y = [clean_plot_name(short_names.get(m,m)) for m in comparison_models if m in models_to_plot_bar]\n",
    "            fig_main_bar_faceted.update_yaxes(categoryorder='array', categoryarray=model_order_for_y, title=None)\n",
    "            fig_main_bar_faceted.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "\n",
    "            num_y_categories = len(model_order_for_y)\n",
    "            \n",
    "            tasks_actually_plotted = [t for t in tasks if t in horizontal_bar_df['Task'].unique()]\n",
    "            num_total_facets = len(tasks_actually_plotted)\n",
    "\n",
    "            for facet_idx, task_name in enumerate(tasks_actually_plotted):\n",
    "\n",
    "                instruct_score_val = summary_comp_df.loc[task_name, instruct_model] if instruct_model in summary_comp_df.columns and task_name in summary_comp_df.index and not pd.isna(summary_comp_df.loc[task_name, instruct_model]) else np.nan\n",
    "                coder_score_val = summary_comp_df.loc[task_name, coder_model] if coder_model in summary_comp_df.columns and task_name in summary_comp_df.index and not pd.isna(summary_comp_df.loc[task_name, coder_model]) else np.nan\n",
    "                \n",
    "                if pd.isna(instruct_score_val) or pd.isna(coder_score_val):\n",
    "                    print(f\"Skipping lines/area for task '{task_name}' due to missing Instruct/Coder scores.\")\n",
    "                    continue\n",
    "\n",
    "                axis_num_suffix = num_total_facets - facet_idx\n",
    "                \n",
    "                current_xaxis_ref = f'x{axis_num_suffix}' if axis_num_suffix > 1 else 'x'\n",
    "                current_yaxis_ref = f'y{axis_num_suffix}' if axis_num_suffix > 1 else 'y'\n",
    "                \n",
    "                fig_main_bar_faceted.add_shape(\n",
    "                    type=\"rect\",\n",
    "                    xref=current_xaxis_ref, yref=current_yaxis_ref,\n",
    "                    x0=min(instruct_score_val, coder_score_val),\n",
    "                    x1=max(instruct_score_val, coder_score_val),\n",
    "                    y0=-0.5, y1=num_y_categories - 0.5,\n",
    "                    fillcolor=\"rgba(255, 128, 128, 0.2)\", # Light red shade\n",
    "                    line_width=0,\n",
    "                    layer=\"below\"\n",
    "                )\n",
    "                fig_main_bar_faceted.add_shape(\n",
    "                    type=\"line\",\n",
    "                    xref=current_xaxis_ref, yref=current_yaxis_ref,\n",
    "                    x0=instruct_score_val, y0=-0.5,\n",
    "                    x1=instruct_score_val, y1=num_y_categories - 0.5,\n",
    "                    line=dict(color=color_map_specific.get('Qwen2.5 Instruct', 'green'), dash=\"dash\", width=2),\n",
    "                    layer=\"above\" \n",
    "                )\n",
    "                fig_main_bar_faceted.add_shape(\n",
    "                    type=\"line\",\n",
    "                    xref=current_xaxis_ref, yref=current_yaxis_ref,\n",
    "                    x0=coder_score_val, y0=-0.5,\n",
    "                    x1=coder_score_val, y1=num_y_categories - 0.5,\n",
    "                    line=dict(color=color_map_specific.get('Qwen2.5 Coder', 'orange'), dash=\"dash\", width=2),\n",
    "                    layer=\"above\"\n",
    "                )\n",
    "\n",
    "            fig_main_bar_faceted.update_layout(\n",
    "                xaxis_showgrid=True,\n",
    "                yaxis_showgrid=False,\n",
    "                showlegend=False, # Removed legend\n",
    "                **font_config \n",
    "            )\n",
    "            \n",
    "            plot_height = max(400, num_total_facets * (num_y_categories * 25 + 70))\n",
    "            fig_main_bar_faceted.update_layout(height=plot_height, width=default_plot_width, margin=dict(l=150, r=50, t=50, b=50))\n",
    "\n",
    "\n",
    "            fig_main_bar_faceted.show()\n",
    "            print(f\"Generated plot: {fig_main_bar_faceted_title} (Faceted Horizontal with Colors & Shaded Area)\")\n",
    "    else:\n",
    "        print(\"Skipping 'Absolute Performance Comparison on Main Tasks (Faceted Bar Chart)': No models with data to plot.\")\n",
    "else:\n",
    "    print(\"Skipping 'Absolute Performance Comparison on Main Tasks (Faceted Bar Chart)': summary_comp_df is empty.\")\n",
    "\n",
    "\n",
    "# REMOVED: All subsequent plots that relied on the granular subtasks_comp_df have been removed\n",
    "# as the analysis is now focused on the main leaderboard tasks. This includes:\n",
    "# - Leaderboard Subtasks Performance Distribution by Model (Box Plot)\n",
    "# - Absolute Performance Comparison for Leaderboard Subtasks (Faceted Horizontal Bar Chart Grid)\n",
    "# - Subtask Difference Boxplot(s)\n",
    "# - Absolute Score Boxplot(s) for Subtasks\n",
    "# - Jointplot(s) for Subtask Differences\n",
    "# - Scatter Plot(s) for Subtask Differences\n",
    "# - Top/Bottom Subtask Impact Plot(s)\n",
    "# - Clustermap/Dendrogram(s) for Subtasks\n",
    "print(\"\\n--- Granular subtask plots are skipped as per request to focus on main leaderboard tasks. ---\")\n",
    "\n",
    "\n",
    "# Plot: Main Task Dendrogram\n",
    "plot_dcoder_col_main = 'd_coder' if 'd_coder' in summary_comp_df.columns and not summary_comp_df['d_coder'].isna().all() else None\n",
    "plot_merged_diff_cols_main = [c for c in diff_cols_main if c.startswith('d_merged_') and c in summary_comp_df.columns and not summary_comp_df[c].isna().all()]\n",
    "\n",
    "if plot_dcoder_col_main and plot_merged_diff_cols_main:\n",
    "    print(\"Generating Main Task Dendrogram(s) (I-M vs I-MergedX)...\")\n",
    "    for merged_diff_col_main in plot_merged_diff_cols_main:\n",
    "        merged_short_name = merged_diff_col_main.replace('d_merged_', '')\n",
    "        cleaned_merged_short_name = clean_plot_name(merged_short_name)\n",
    "        cols_main_single = [plot_dcoder_col_main, merged_diff_col_main]\n",
    "        \n",
    "        main_matrix_data_single = summary_comp_df[cols_main_single].dropna(how='any')\n",
    "        if len(main_matrix_data_single) >= 2:\n",
    "            try:\n",
    "                fig_dendro_main_s_title = f'Dendrogram: Main Tasks based on Profile for {cleaned_merged_short_name}'\n",
    "                fig_dendro_main_s = ff.create_dendrogram(main_matrix_data_single.values, labels=main_matrix_data_single.index.tolist(), linkagefun=lambda x: linkage(x, method='ward'))\n",
    "                dynamic_width_dendro_main = max(default_plot_width, 30 * len(main_matrix_data_single.index))\n",
    "                fig_dendro_main_s.update_layout(title=fig_dendro_main_s_title, yaxis_title='Distance', xaxis_title='Task', width=dynamic_width_dendro_main, height=default_plot_height, **font_config); fig_dendro_main_s.show(); print(f\"  - Generated plot: {fig_dendro_main_s_title}\") \n",
    "            except Exception as e: print(f\"Could not generate Main Tasks Dendrogram for {cleaned_merged_short_name}: {e}\")\n",
    "else: print(\"Skipping Main Task Dendrogram(s): 'd_coder' on main tasks missing/all_NaN or no valid merged diff columns for main tasks.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
