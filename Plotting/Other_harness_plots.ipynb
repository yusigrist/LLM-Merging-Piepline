{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb2cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import re # Added for cleaning plot names\n",
    "\n",
    "# Set default plotly template for better aesthetics if needed\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# --- Helper function for cleaning model names for plots ---\n",
    "def clean_plot_name(name):\n",
    "    if name is None: # Handle potential None input\n",
    "        return \"Unknown\"\n",
    "    name_str = str(name)\n",
    "\n",
    "    # For merged models (which won't start with \"Qwen2.5\" after short_names mapping)\n",
    "    # remove the trailing _XX\n",
    "    # Also handles cases where \"Merged_\" might still be there if short_names wasn't fully applied before this function\n",
    "    if not name_str.startswith(\"Qwen2.5\") or \"Merged_\" in name_str : # Handles names like \"Linear_24\" -> \"Linear\" or \"Merged_Linear_24\" -> \"Merged_Linear\"\n",
    "        name_str = re.sub(r'_\\d+$', '', name_str)\n",
    "    return name_str\n",
    "\n",
    "# --- Font configuration for plots ---\n",
    "font_config = {\n",
    "    \"title_font_size\": 30,\n",
    "    \"font_size\": 20,\n",
    "    \"xaxis_title_font_size\": 20,\n",
    "    \"yaxis_title_font_size\": 20,\n",
    "    \"xaxis_tickfont_size\": 20, # Adjusted for potentially dense plots\n",
    "    \"yaxis_tickfont_size\": 20, # Adjusted for potentially dense plots\n",
    "    \"legend_title_font_size\": 20, # Kept for other plots that might use legends\n",
    "    \"legend_font_size\": 15,       # Kept for other plots\n",
    "}\n",
    "# --- Default Plot Dimensions ---\n",
    "default_plot_height = 400\n",
    "default_plot_width = 1400 # Added for wider plots\n",
    "\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "\n",
    "# --- Paths and Model Definitions ---\n",
    "models = [\n",
    "    \"Qwen2.5-7B\",\n",
    "    \"Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen2.5-Coder-7B\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-task_arithmetic-29\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-dare_ties-29\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-ties-29\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-slerp-29\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-linear-29\"\n",
    "]\n",
    "\n",
    "# Updated short_names for harness script\n",
    "# Della and DARE_Ties entries have been removed\n",
    "short_names = {\n",
    "    \"Qwen2.5-7B\": \"Qwen2.5 Base\",\n",
    "    \"Qwen2.5-7B-Instruct\": \"Qwen2.5 Instruct\",\n",
    "    \"Qwen2.5-Coder-7B\": \"Qwen2.5 Coder\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-task_arithmetic-29\": \"Task Arithmetic\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-dare_ties-29\": \"DARE Ties\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-ties-29\": \"Ties\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-slerp-29\": \"Slerp\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-linear-29\": \"Linear \"\n",
    "}\n",
    "\n",
    "tasks = [\"gsm8k\", \"mmlu\", \"leaderboard\"] # This list defines the order of tasks\n",
    "paths = {m: {t: f\"organized_results/{t}/{m}/result.json\" for t in tasks} for m in models}\n",
    "\n",
    "# Updated model categorization logic for harness script\n",
    "instruct_model = None; coder_model = None; merged_models = []; base_model = None\n",
    "for m_full_name in models: # Iterate through the original full model names\n",
    "    m_short = short_names.get(m_full_name, \"\") # Get the NEW short name\n",
    "\n",
    "    is_instruct = (m_short == \"Qwen2.5 Instruct\")\n",
    "    is_coder = (m_short == \"Qwen2.5 Coder\")\n",
    "    is_base = (m_short == \"Qwen2.5 Base\")\n",
    "    \n",
    "    # A model is considered merged if its short name does not match Base, Instruct, or Coder\n",
    "    is_merged = not (is_instruct or is_coder or is_base)\n",
    "\n",
    "    if is_instruct:\n",
    "        instruct_model = m_full_name\n",
    "    elif is_coder:\n",
    "        coder_model = m_full_name\n",
    "    elif is_base:\n",
    "        base_model = m_full_name\n",
    "    elif is_merged:\n",
    "         # This ensures we only add models from our initial `models` list\n",
    "         # that are intended to be merged.\n",
    "        if m_full_name in models:\n",
    "            merged_models.append(m_full_name)\n",
    "\n",
    "\n",
    "if not instruct_model: print(\"CRITICAL ERROR: Instruct model not identified.\"); exit()\n",
    "if not coder_model: print(\"CRITICAL ERROR: Coder model not identified.\"); exit()\n",
    "if not merged_models: print(\"WARNING: No merged models identified.\")\n",
    "if not base_model: print(\"WARNING: Base model not identified in harness script setup.\")\n",
    "\n",
    "\n",
    "print(\"--- Model Categorization (Harness) ---\")\n",
    "if base_model: print(f\"Base Model: {base_model} ({short_names.get(base_model, 'N/A')})\")\n",
    "print(f\"Instruct Model: {instruct_model} ({short_names.get(instruct_model, 'N/A')})\")\n",
    "print(f\"Coder Model: {coder_model} ({short_names.get(coder_model, 'N/A')})\")\n",
    "print(f\"Merged Models ({len(merged_models)}):\")\n",
    "for m in merged_models: print(f\"  - {m} ({short_names.get(m, 'N/A')})\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Define the order and inclusion for comparison_models\n",
    "comparison_models_ordered = []\n",
    "if base_model:\n",
    "    comparison_models_ordered.append(base_model)\n",
    "if instruct_model:\n",
    "    comparison_models_ordered.append(instruct_model)\n",
    "if coder_model:\n",
    "    comparison_models_ordered.append(coder_model)\n",
    "comparison_models_ordered.extend([m for m in merged_models if m])\n",
    "comparison_models = list(dict.fromkeys(m for m in comparison_models_ordered if m)) # Filter out None and duplicates\n",
    "\n",
    "print(f\"Models for comparison (in order): {[clean_plot_name(short_names.get(m, m)) for m in comparison_models]}\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# --- 2. Data Loading ---\n",
    "def load_summary(paths_dict, model_list):\n",
    "    df = pd.DataFrame(index=tasks, columns=model_list, dtype=float)\n",
    "    key_map = {\"gsm8k\": \"exact_match,strict-match\", \"mmlu\": \"acc,none\", \"leaderboard\": \"acc_norm,none\"}\n",
    "    for m in model_list:\n",
    "        for t in tasks:\n",
    "            fp = paths_dict.get(m, {}).get(t)\n",
    "            if not fp: df.at[t, m] = np.nan; continue\n",
    "            if os.path.isfile(fp):\n",
    "                try:\n",
    "                    with open(fp, 'r') as f: data = json.load(f)\n",
    "                    results_for_task = data.get('results', {}).get(t, {})\n",
    "                    if not results_for_task and t == 'leaderboard': results_for_task = data.get('results', {}).get('leaderboard', {})\n",
    "                    val = results_for_task.get(key_map[t], np.nan)\n",
    "                    df.at[t, m] = val * 100 if val is not None and not pd.isna(val) else np.nan\n",
    "                except Exception as e: print(f\"Error loading file {fp}: {e}\"); df.at[t, m] = np.nan\n",
    "            else: print(f\"Warning: File not found {fp}\"); df.at[t, m] = np.nan\n",
    "    return df.reindex(tasks).dropna(how='all', axis=1).dropna(how='all', axis=0)\n",
    "\n",
    "def load_leaderboard_with_groups(paths_dict, model_list):\n",
    "    agg = defaultdict(dict); inv_group = {}\n",
    "    leaderboard_paths = {m: paths_dict.get(m, {}).get('leaderboard') for m in model_list}\n",
    "    leaderboard_paths = {m: p for m, p in leaderboard_paths.items() if p}\n",
    "    if not any(os.path.isfile(fp) for fp in leaderboard_paths.values() if fp):\n",
    "        return pd.DataFrame(columns=['subtask'] + model_list + ['group'])\n",
    "    first_valid_file_checked_for_groups = False\n",
    "    for m, fp in leaderboard_paths.items():\n",
    "        if not fp or not os.path.isfile(fp): continue\n",
    "        try:\n",
    "            with open(fp, 'r') as f: data = json.load(f)\n",
    "            if 'group_subtasks' in data and not inv_group and not first_valid_file_checked_for_groups:\n",
    "                for grp, subs in data['group_subtasks'].items():\n",
    "                    clean_grp_name = grp.replace('leaderboard_', '') if isinstance(grp, str) else grp\n",
    "                    for sub in subs: inv_group[sub] = clean_grp_name\n",
    "                first_valid_file_checked_for_groups = True\n",
    "            for key, metrics in data.get('results', {}).items():\n",
    "                 if isinstance(key, str) and key.startswith('leaderboard_') and key != 'leaderboard':\n",
    "                    score = metrics.get('acc_norm,none', metrics.get('acc,none', metrics.get('exact_match,none', np.nan)))\n",
    "                    if not pd.isna(score): agg[key][m] = score * 100\n",
    "        except Exception as e: print(f\"Error processing file {fp} for model {m}: {e}\")\n",
    "    if not agg: return pd.DataFrame(columns=['subtask'] + model_list + ['group'])\n",
    "    df = pd.DataFrame.from_dict(agg, orient='index')\n",
    "    for m_col in model_list:\n",
    "        if m_col not in df.columns: df[m_col] = np.nan\n",
    "    present_models_in_agg = [m for m in model_list if m in df.columns]; df = df[present_models_in_agg]\n",
    "    df = df.dropna(subset=present_models_in_agg, how='all')\n",
    "    if df.empty: return pd.DataFrame(columns=['subtask'] + model_list + ['group', 'subtask_cleaned'])\n",
    "    df['group'] = df.index.map(lambda x: inv_group.get(x, 'Unknown'))\n",
    "    df['subtask_cleaned'] = df.index.str.replace('leaderboard_', '', regex=False)\n",
    "    final_cols = ['group', 'subtask_cleaned'] + present_models_in_agg\n",
    "    return df.reset_index().rename(columns={'index': 'subtask'})[final_cols + ['subtask']]\n",
    "\n",
    "summary_df = load_summary(paths, models)\n",
    "subtasks_df = load_leaderboard_with_groups(paths, models)\n",
    "\n",
    "models_in_summary_data = [m for m in comparison_models if m in summary_df.columns]\n",
    "summary_comp_df = summary_df.loc[:, models_in_summary_data].copy() if models_in_summary_data else pd.DataFrame()\n",
    "\n",
    "\n",
    "models_in_subtasks_data = [m for m in comparison_models if m in subtasks_df.columns]\n",
    "if not subtasks_df.empty and models_in_subtasks_data:\n",
    "    present_base_cols = [c for c in ['subtask', 'subtask_cleaned', 'group'] if c in subtasks_df.columns]\n",
    "    subtasks_comp_df = subtasks_df[present_base_cols + models_in_subtasks_data].copy()\n",
    "else:\n",
    "    subtasks_comp_df = pd.DataFrame(columns=['subtask', 'subtask_cleaned', 'group'] + models_in_subtasks_data)\n",
    "\n",
    "\n",
    "print(\"\\n--- Summary DataFrame (Comparison Models) ---\"); print(summary_comp_df); print(\"-\" * 50)\n",
    "if not subtasks_comp_df.empty: print(\"\\n--- Subtasks DataFrame (Comparison Models Head) ---\"); print(subtasks_comp_df.head()); print(\"-\" * 50)\n",
    "else: print(\"\\n--- Subtasks DataFrame is empty or could not be loaded/filtered for comparison models ---\")\n",
    "\n",
    "# --- 3. Calculate Differences ---\n",
    "can_calc_diffs = True\n",
    "if instruct_model not in summary_comp_df.columns or coder_model not in summary_comp_df.columns:\n",
    "     print(\"Warning: Instruct or Coder model data missing from summary_comp_df. Difference calculations involving them will be skipped or result in NaN.\"); can_calc_diffs = False\n",
    "diff_cols_main = []; diff_cols_subtasks = []\n",
    "\n",
    "if can_calc_diffs:\n",
    "    if instruct_model in summary_comp_df.columns and coder_model in summary_comp_df.columns:\n",
    "        summary_comp_df['d_coder'] = summary_comp_df[instruct_model] - summary_comp_df[coder_model]; diff_cols_main.append('d_coder')\n",
    "    else:\n",
    "        summary_comp_df['d_coder'] = np.nan\n",
    "\n",
    "    for merged_m in merged_models:\n",
    "        if merged_m in summary_comp_df.columns and instruct_model in summary_comp_df.columns:\n",
    "            merged_short_name = clean_plot_name(short_names.get(merged_m, merged_m)) # Apply clean_plot_name\n",
    "            col_name = f\"d_merged_{merged_short_name}\"\n",
    "            summary_comp_df[col_name] = summary_comp_df[instruct_model] - summary_comp_df[merged_m]; diff_cols_main.append(col_name)\n",
    "        \n",
    "    print(\"\\n--- Summary DataFrame with Differences ---\");\n",
    "    diff_cols_main_present = [col for col in diff_cols_main if col in summary_comp_df.columns]\n",
    "    if diff_cols_main_present: print(summary_comp_df[diff_cols_main_present])\n",
    "    else: print(\"No difference columns to show for main tasks.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    if not subtasks_comp_df.empty:\n",
    "        if instruct_model in subtasks_comp_df.columns and coder_model in subtasks_comp_df.columns:\n",
    "            subtasks_comp_df['d_coder'] = subtasks_comp_df[instruct_model] - subtasks_comp_df[coder_model]; diff_cols_subtasks.append('d_coder')\n",
    "        else: subtasks_comp_df['d_coder'] = np.nan\n",
    "\n",
    "        for merged_m in merged_models:\n",
    "            merged_short_name = clean_plot_name(short_names.get(merged_m, merged_m)) # Apply clean_plot_name\n",
    "            col_name = f\"d_merged_{merged_short_name}\"\n",
    "            if instruct_model in subtasks_comp_df.columns and merged_m in subtasks_comp_df.columns:\n",
    "                subtasks_comp_df[col_name] = subtasks_comp_df[instruct_model] - subtasks_comp_df[merged_m]; diff_cols_subtasks.append(col_name)\n",
    "            \n",
    "        print(\"\\n--- Subtasks DataFrame with Differences (Head) ---\")\n",
    "        diff_cols_sub_present = [col for col in diff_cols_subtasks if col in subtasks_comp_df.columns]\n",
    "        cols_to_show_sub_diff = [c for c in ['subtask_cleaned', 'group'] + models_in_subtasks_data + diff_cols_sub_present if c in subtasks_comp_df.columns]\n",
    "        if cols_to_show_sub_diff: print(subtasks_comp_df[cols_to_show_sub_diff].head())\n",
    "        else: print(\"No difference columns to show for subtasks or base columns missing.\")\n",
    "        print(\"-\" * 50)\n",
    "else: print(\"Skipping difference calculations as instruct or coder model data is critically missing from summary_comp_df.\")\n",
    "\n",
    "\n",
    "# --- 4. Ranking Generation ---\n",
    "def generate_rankings(summary_data, subtask_data, model_names_for_ranking, short_names_map):\n",
    "    ranking_results = {};\n",
    "    models_in_summary_for_ranking = [m for m in model_names_for_ranking if m in summary_data.columns]\n",
    "\n",
    "    if not models_in_summary_for_ranking:\n",
    "        print(\"No models available in summary_data for ranking.\")\n",
    "    else:\n",
    "        main_rankings = []\n",
    "        for task_item in summary_data.index:\n",
    "            scores = summary_data.loc[task_item, models_in_summary_for_ranking]; ranked_models = scores.sort_values(ascending=False, na_position='last').index.tolist()\n",
    "            ranked_short_names = [clean_plot_name(short_names_map.get(m, m)) for m in ranked_models]; row = {'Task': task_item} # Apply clean_plot_name\n",
    "            for i, name in enumerate(ranked_short_names): row[f'Rank {i+1}'] = name\n",
    "            main_rankings.append(row)\n",
    "        if main_rankings: ranking_results['main_tasks'] = pd.DataFrame(main_rankings).set_index('Task')\n",
    "        else: ranking_results['main_tasks'] = pd.DataFrame()\n",
    "\n",
    "\n",
    "    if not subtask_data.empty:\n",
    "        models_in_subtasks_for_ranking = [m for m in model_names_for_ranking if m in subtask_data.columns]\n",
    "        if not models_in_subtasks_for_ranking:\n",
    "            print(\"No models available in subtask_data for ranking.\")\n",
    "        else:\n",
    "            subtask_rankings = []; subtask_name_col = 'subtask_cleaned' if 'subtask_cleaned' in subtask_data.columns else 'subtask'; group_col = 'group' if 'group' in subtask_data.columns else None\n",
    "            for idx, row_data in subtask_data.iterrows():\n",
    "                if isinstance(row_data, pd.Series) and all(m in row_data.index for m in models_in_subtasks_for_ranking):\n",
    "                     scores = row_data[models_in_subtasks_for_ranking]; ranked_models = scores.sort_values(ascending=False, na_position='last').index.tolist()\n",
    "                     ranked_short_names = [clean_plot_name(short_names_map.get(m, m)) for m in ranked_models]; row = {'Subtask': row_data[subtask_name_col]} # Apply clean_plot_name\n",
    "                     if group_col and group_col in row_data.index: row['Group'] = row_data[group_col]\n",
    "                     for i, name in enumerate(ranked_short_names): row[f'Rank {i+1}'] = name\n",
    "                     subtask_rankings.append(row)\n",
    "            if subtask_rankings:\n",
    "                 rank_df_sub = pd.DataFrame(subtask_rankings)\n",
    "                 base_cols = ['Subtask'] + (['Group'] if group_col and 'Group' in rank_df_sub.columns else []); rank_cols = [f'Rank {i+1}' for i in range(len(models_in_subtasks_for_ranking))]\n",
    "                 cols_order = base_cols + rank_cols\n",
    "                 for c in cols_order:\n",
    "                     if c not in rank_df_sub.columns: rank_df_sub[c] = np.nan\n",
    "                 rank_df_sub = rank_df_sub[cols_order]\n",
    "                 if group_col and 'Group' in rank_df_sub.columns: rank_df_sub = rank_df_sub.sort_values(by=['Group', 'Subtask']).set_index(['Group', 'Subtask'])\n",
    "                 else: rank_df_sub = rank_df_sub.set_index('Subtask')\n",
    "                 ranking_results['subtasks'] = rank_df_sub\n",
    "            else: ranking_results['subtasks'] = pd.DataFrame()\n",
    "\n",
    "\n",
    "            if 'subtasks' in ranking_results and group_col and models_in_subtasks_for_ranking and 'group' in subtask_data.columns:\n",
    "                 try:\n",
    "                     avg_scores_group = subtask_data.groupby('group')[models_in_subtasks_for_ranking].mean(numeric_only=True); group_rankings = []\n",
    "                     for group_name_iter in avg_scores_group.index:\n",
    "                         scores = avg_scores_group.loc[group_name_iter]; ranked_models = scores.sort_values(ascending=False, na_position='last').index.tolist()\n",
    "                         ranked_short_names = [clean_plot_name(short_names_map.get(m, m)) for m in ranked_models]; row = {'Group': group_name_iter} # Apply clean_plot_name\n",
    "                         for i, name in enumerate(ranked_short_names): row[f'Rank {i+1}'] = name\n",
    "                         group_rankings.append(row)\n",
    "                     if group_rankings: ranking_results['group_avg'] = pd.DataFrame(group_rankings).set_index('Group')\n",
    "                     else: ranking_results['group_avg'] = pd.DataFrame()\n",
    "                 except Exception as e: print(f\"Could not calculate group average rankings: {e}\")\n",
    "    return ranking_results\n",
    "\n",
    "rankings = generate_rankings(summary_comp_df, subtasks_comp_df, comparison_models, short_names)\n",
    "print(\"\\n\" + \"=\"*20 + \" MODEL RANKINGS \" + \"=\"*20)\n",
    "if 'main_tasks' in rankings and not rankings['main_tasks'].empty: print(\"\\n--- Main Task Rankings ---\"); print(rankings['main_tasks'])\n",
    "if 'subtasks' in rankings and not rankings['subtasks'].empty: print(\"\\n--- Subtask Rankings ---\"); print(rankings['subtasks'].head(10))\n",
    "if 'group_avg' in rankings and not rankings['group_avg'].empty: print(\"\\n--- Group Average Rankings ---\"); print(rankings['group_avg'])\n",
    "output_dir_harness = \"rankings_output_harness\"\n",
    "os.makedirs(output_dir_harness, exist_ok=True); print(f\"\\n--- Saving Rankings to CSV in '{output_dir_harness}/' ---\")\n",
    "for name, df_rank in rankings.items():\n",
    "    if isinstance(df_rank, pd.DataFrame) and not df_rank.empty:\n",
    "        try: csv_filename = os.path.join(output_dir_harness, f\"{name}_rankings_harness.csv\"); df_rank.to_csv(csv_filename, index=True); print(f\"Saved {name} rankings to {csv_filename}\")\n",
    "        except Exception as e: print(f\"Error saving {name} rankings: {e}\")\n",
    "print(\"=\"*58)\n",
    "\n",
    "\n",
    "# --- 5. Merged Model Performance Categorization & Task Scenario Table ---\n",
    "\n",
    "def generate_performance_analysis(df_analyze, df_name_suffix, task_id_col, instruct_m, coder_m, merged_m_list, short_names_map, out_dir):\n",
    "    print(f\"\\n--- Merged Model Performance Categorization for: {df_name_suffix} ---\")\n",
    "    analysis_results = []\n",
    "    task_scenario_data = []\n",
    "\n",
    "    if not (instruct_m and coder_m and instruct_m in df_analyze.columns and coder_m in df_analyze.columns):\n",
    "        print(f\"Skipping analysis for {df_name_suffix}: Instruct or Coder model data missing from DataFrame.\")\n",
    "        return\n",
    "\n",
    "    if task_id_col is None: # For summary_df, index is the task\n",
    "        task_iterable = df_analyze.index\n",
    "        get_task_name_from_val = lambda task_val: task_val\n",
    "        get_task_data_row_from_val = lambda task_val: df_analyze.loc[task_val]\n",
    "    elif task_id_col in df_analyze.columns: # For subtasks_df\n",
    "        task_iterable = df_analyze[task_id_col].unique()\n",
    "        get_task_name_from_val = lambda task_val: task_val\n",
    "        get_task_data_row_from_val = lambda task_val: df_analyze[df_analyze[task_id_col] == task_val].iloc[0] if not df_analyze[df_analyze[task_id_col] == task_val].empty else None\n",
    "    else:\n",
    "        print(f\"Error: Task identifier '{task_id_col}' not found for {df_name_suffix}.\")\n",
    "        return\n",
    "\n",
    "    task_categorization_for_table = {get_task_name_from_val(task_val): {\"Better_than_both\": [], \"Worse_than_both\": [], \"Between_Equal\": []}\n",
    "                                         for task_val in task_iterable}\n",
    "\n",
    "    for merged_model_full_name in merged_m_list:\n",
    "        if merged_model_full_name in df_analyze.columns:\n",
    "            better_count, worse_count, between_count = 0, 0, 0\n",
    "            merged_model_short_name = short_names_map.get(merged_model_full_name, merged_model_full_name)\n",
    "            cleaned_merged_model_short_name_for_plot = clean_plot_name(merged_model_short_name)\n",
    "\n",
    "            for task_value in task_iterable:\n",
    "                task_data_row = get_task_data_row_from_val(task_value)\n",
    "                if task_data_row is None: continue\n",
    "\n",
    "                merged_score = task_data_row[merged_model_full_name]\n",
    "                instruct_score = task_data_row[instruct_m]\n",
    "                coder_score = task_data_row[coder_m]\n",
    "\n",
    "                if pd.isna(merged_score) or pd.isna(instruct_score) or pd.isna(coder_score):\n",
    "                    continue\n",
    "\n",
    "                min_im = min(instruct_score, coder_score)\n",
    "                max_im = max(instruct_score, coder_score)\n",
    "                current_task_name = get_task_name_from_val(task_value)\n",
    "\n",
    "                if merged_score > max_im:\n",
    "                    better_count += 1\n",
    "                    task_categorization_for_table[current_task_name][\"Better_than_both\"].append(cleaned_merged_model_short_name_for_plot)\n",
    "                elif merged_score < min_im:\n",
    "                    worse_count += 1\n",
    "                    task_categorization_for_table[current_task_name][\"Worse_than_both\"].append(cleaned_merged_model_short_name_for_plot)\n",
    "                elif min_im <= merged_score <= max_im:\n",
    "                    between_count += 1\n",
    "                    task_categorization_for_table[current_task_name][\"Between_Equal\"].append(cleaned_merged_model_short_name_for_plot)\n",
    "\n",
    "            analysis_results.append({\n",
    "                \"Merged Model\": cleaned_merged_model_short_name_for_plot,\n",
    "                \"Better than Instruct & Coder\": better_count,\n",
    "                \"Worse than Instruct & Coder\": worse_count,\n",
    "                \"Between/Equal to Instruct & Coder\": between_count,\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Skipping categorization for {merged_model_full_name} ({df_name_suffix}) as it's not in the DataFrame.\")\n",
    "\n",
    "    for task_name_iter, categories in task_categorization_for_table.items():\n",
    "        task_scenario_data.append({\n",
    "            \"Task\": task_name_iter,\n",
    "            \"Better_Count\": len(categories[\"Better_than_both\"]),\n",
    "            \"Better_Models\": \", \".join(sorted(list(set(categories[\"Better_than_both\"])))),\n",
    "            \"Worse_Count\": len(categories[\"Worse_than_both\"]),\n",
    "            \"Worse_Models\": \", \".join(sorted(list(set(categories[\"Worse_than_both\"])))),\n",
    "            \"Between_Equal_Count\": len(categories[\"Between_Equal\"]),\n",
    "            \"Between_Equal_Models\": \", \".join(sorted(list(set(categories[\"Between_Equal\"])))),\n",
    "        })\n",
    "    task_scenario_df = pd.DataFrame(task_scenario_data)\n",
    "    print(f\"\\n--- Task Scenario Ranking Table ({df_name_suffix}) ---\")\n",
    "    print(task_scenario_df.head())\n",
    "    try:\n",
    "        task_scenario_csv_filename = os.path.join(out_dir, f\"task_scenario_rankings_{df_name_suffix.lower().replace(' ','_')}.csv\")\n",
    "        task_scenario_df.to_csv(task_scenario_csv_filename, index=False)\n",
    "        print(f\"Saved task scenario rankings for {df_name_suffix} to {task_scenario_csv_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving task scenario rankings for {df_name_suffix} to CSV: {e}\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "    if analysis_results:\n",
    "        counts_df = pd.DataFrame(analysis_results)\n",
    "        print(f\"\\n--- Counts of Merged Model Performance Categories ({df_name_suffix} Overall) ---\")\n",
    "        print(counts_df)\n",
    "        counts_df_melted = counts_df.melt(id_vars=\"Merged Model\",\n",
    "                                          value_vars=[\"Better than Instruct & Coder\", \"Worse than Instruct & Coder\", \"Between/Equal to Instruct & Coder\"],\n",
    "                                          var_name=\"Category\", value_name=\"Number of Tasks\")\n",
    "        fig_counts_title = f\"Merged Model Performance vs. Instruct & Coder ({df_name_suffix} Overall)\"\n",
    "        fig_counts = px.bar(counts_df_melted, x=\"Merged Model\", y=\"Number of Tasks\", color=\"Category\",\n",
    "                            title=fig_counts_title,\n",
    "                            barmode='stack',\n",
    "                            labels={\"Number of Tasks\": f\"Number of {df_name_suffix.replace('Subtasks', 'Subtasks')}\"})\n",
    "        fig_counts.update_xaxes(categoryorder=\"array\", categoryarray=counts_df[\"Merged Model\"].tolist())\n",
    "        fig_counts.update_layout(\n",
    "            height=default_plot_height + 50, \n",
    "            width=default_plot_width, \n",
    "            legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.3, xanchor=\"center\", x=0.5),\n",
    "            **font_config\n",
    "        )\n",
    "        fig_counts.show()\n",
    "        print(f\"Generated plot: {fig_counts_title}\")\n",
    "\n",
    "        if len(counts_df) > 1:\n",
    "            pivot_counts_df = counts_df.set_index(\"Merged Model\")\n",
    "            categories_to_correlate = [\"Better than Instruct & Coder\", \"Worse than Instruct & Coder\", \"Between/Equal to Instruct & Coder\"]\n",
    "            pivot_counts_df_filtered = pivot_counts_df[[cat for cat in categories_to_correlate if cat in pivot_counts_df.columns]]\n",
    "            if len(pivot_counts_df_filtered.columns) > 0 and len(pivot_counts_df_filtered) >1:\n",
    "                correlation_matrix = pivot_counts_df_filtered.T.corr()\n",
    "                print(f\"\\n--- Correlation Matrix of Performance Categories Between Merged Models ({df_name_suffix} Overall) ---\")\n",
    "                print(correlation_matrix)\n",
    "                fig_corr_heatmap_title = f\"Correlation of Perf. Categories Between Merged Models ({df_name_suffix})\"\n",
    "                fig_corr_heatmap = px.imshow(correlation_matrix, text_auto=True, aspect=\"auto\",\n",
    "                                             color_continuous_scale='RdBu_r', range_color=[-1,1],\n",
    "                                             title=fig_corr_heatmap_title)\n",
    "                fig_corr_heatmap.update_layout(width=default_plot_width, height=600, **font_config)\n",
    "                fig_corr_heatmap.show()\n",
    "                print(f\"Generated plot: {fig_corr_heatmap_title}\")\n",
    "            else:\n",
    "                print(f\"Not enough data or categories to calculate category correlation for {df_name_suffix}.\")\n",
    "        else:\n",
    "            print(f\"Not enough merged models with data to calculate category correlation for {df_name_suffix}.\")\n",
    "    else:\n",
    "        print(f\"No merged model comparison counts generated for {df_name_suffix}.\")\n",
    "    print(\"=\"*58)\n",
    "\n",
    "if not summary_comp_df.empty:\n",
    "    generate_performance_analysis(\n",
    "        summary_comp_df,\n",
    "        df_name_suffix=\"MainTasks\",\n",
    "        task_id_col=None, # Indicates using DataFrame index for tasks\n",
    "        instruct_m=instruct_model,\n",
    "        coder_m=coder_model,\n",
    "        merged_m_list=merged_models,\n",
    "        short_names_map=short_names,\n",
    "        out_dir=output_dir_harness\n",
    "    )\n",
    "\n",
    "if not subtasks_comp_df.empty and 'subtask_cleaned' in subtasks_comp_df.columns:\n",
    "    generate_performance_analysis(\n",
    "        subtasks_comp_df,\n",
    "        df_name_suffix=\"LeaderboardSubtasks\",\n",
    "        task_id_col='subtask_cleaned', # Column name for subtask identifiers\n",
    "        instruct_m=instruct_model,\n",
    "        coder_m=coder_model,\n",
    "        merged_m_list=merged_models,\n",
    "        short_names_map=short_names,\n",
    "        out_dir=output_dir_harness\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping performance categorization for Leaderboard Subtasks: DataFrame is empty or 'subtask_cleaned' column is missing.\")\n",
    "print(\"=\"*58)\n",
    "\n",
    "\n",
    "# --- 6. Original Plotting Section ---\n",
    "print(\"\\n--- Generating Original Plots (Harness Data) ---\")\n",
    "instruct_short_label = clean_plot_name(short_names.get(instruct_model, \"Instruct\")) # Will be \"Qwen2.5 Instruct\"\n",
    "coder_short_label = clean_plot_name(short_names.get(coder_model, \"Coder\")) # Will be \"Qwen2.5 Coder\"\n",
    "\n",
    "# Plot: Difference Trends on Main Tasks (vs Instruct)\n",
    "if not summary_comp_df.empty and 'd_coder' in summary_comp_df.columns :\n",
    "    fig1 = go.Figure()\n",
    "    if 'd_coder' in summary_comp_df.columns and not summary_comp_df['d_coder'].isna().all():\n",
    "        fig1.add_trace(go.Scatter(x=summary_comp_df.index, y=summary_comp_df['d_coder'], mode='lines+markers', name=f'{instruct_short_label}–{coder_short_label}', marker=dict(symbol='circle', size=8), line=dict(dash='dash'), hovertemplate='Task: %{x}<br>Difference: %{y:.2f}%<extra></extra>'))\n",
    "\n",
    "    colors = px.colors.qualitative.Plotly; merged_plot_idx = 0\n",
    "    for diff_col in diff_cols_main:\n",
    "        if diff_col.startswith('d_merged_') and diff_col in summary_comp_df.columns and not summary_comp_df[diff_col].isna().all():\n",
    "            merged_short_name_plot = diff_col.replace('d_merged_', '')\n",
    "            cleaned_merged_name_plot = clean_plot_name(merged_short_name_plot) # Ensures _XX is removed\n",
    "            fig1.add_trace(go.Scatter(x=summary_comp_df.index, y=summary_comp_df[diff_col], mode='lines+markers', name=f'{instruct_short_label}–{cleaned_merged_name_plot}', marker=dict(symbol='square', size=8, color=colors[merged_plot_idx % len(colors)]), hovertemplate='Task: %{x}<br>Difference: %{y:.2f}%<extra></extra>')); merged_plot_idx +=1\n",
    "        \n",
    "    if fig1.data:\n",
    "        fig1_title = 'Difference Trends on Main Tasks (vs Instruct) - All Merged'\n",
    "        fig1.update_layout(\n",
    "            title=fig1_title,\n",
    "            xaxis_title='Task', yaxis_title='Performance Difference (%)',\n",
    "            legend_title_text='Difference Type', hovermode='x unified',\n",
    "            width=default_plot_width, \n",
    "            **font_config\n",
    "        )\n",
    "        fig1.show()\n",
    "        print(f\"Generated plot: {fig1_title}\")\n",
    "    else: print(\"Skipping plot 'Difference Trends on Main Tasks': No data to plot.\")\n",
    "else: print(\"Skipping plot 'Difference Trends on Main Tasks': summary_comp_df is empty or 'd_coder' column missing.\")\n",
    "\n",
    "\n",
    "# Plot: Absolute Performance on Main Tasks (Line Chart)\n",
    "if not summary_comp_df.empty:\n",
    "    fig1_abs = go.Figure(); colors_line = px.colors.qualitative.Plotly\n",
    "    models_to_plot_abs = [m for m in comparison_models if m in summary_comp_df.columns and not summary_comp_df[m].isna().all()]\n",
    "\n",
    "    for plot_idx, model_name_abs in enumerate(models_to_plot_abs):\n",
    "        short_name_abs = short_names.get(model_name_abs, model_name_abs)\n",
    "        cleaned_short_name_abs = clean_plot_name(short_name_abs) # Apply cleaning\n",
    "        \n",
    "        current_symbol = 'circle'\n",
    "        current_line_style = 'solid'\n",
    "\n",
    "        if model_name_abs == base_model:\n",
    "            current_symbol = 'star'\n",
    "            current_line_style = 'dashdot'\n",
    "        elif model_name_abs == instruct_model:\n",
    "            current_symbol = 'circle'\n",
    "            current_line_style = 'solid'\n",
    "        elif model_name_abs == coder_model:\n",
    "            current_symbol = 'diamond'\n",
    "            current_line_style = 'dash'\n",
    "        elif model_name_abs in merged_models:\n",
    "            current_symbol = 'square'\n",
    "            current_line_style = 'dot'\n",
    "                \n",
    "        fig1_abs.add_trace(go.Scatter(x=summary_comp_df.index, y=summary_comp_df[model_name_abs],\n",
    "                                      mode='lines+markers', name=cleaned_short_name_abs,\n",
    "                                      marker=dict(symbol=current_symbol, size=8, color=colors_line[plot_idx % len(colors_line)]),\n",
    "                                      line=dict(dash=current_line_style),\n",
    "                                      hovertemplate='Task: %{x}<br>Score: %{y:.2f}%<extra></extra>'))\n",
    "    if fig1_abs.data:\n",
    "        fig1_abs_title = 'Absolute Performance on Main Tasks (Line Chart)'\n",
    "        fig1_abs.update_layout(\n",
    "            title=fig1_abs_title,\n",
    "            xaxis_title='Task', yaxis_title='Performance Score (%)',\n",
    "            legend_title_text='Model', hovermode='x unified',\n",
    "            width=default_plot_width, \n",
    "            **font_config\n",
    "        )\n",
    "        fig1_abs.show()\n",
    "        print(f\"Generated plot: {fig1_abs_title}\")\n",
    "    else: print(\"Skipping plot 'Absolute Performance on Main Tasks (Line Chart)': No data to plot.\")\n",
    "else: print(\"Skipping plot 'Absolute Performance on Main Tasks (Line Chart)': summary_comp_df is empty.\")\n",
    "\n",
    "# New Plot: Absolute Performance Comparison on Main Tasks (Faceted Horizontal Bar Chart with Shaded Area)\n",
    "if not summary_comp_df.empty:\n",
    "    models_to_plot_bar = [m for m in comparison_models if m in summary_comp_df.columns and not summary_comp_df[m].isna().all()]\n",
    "    if models_to_plot_bar:\n",
    "        \n",
    "        # Define color map based on NEW short names for legend/coloring\n",
    "        color_map_specific = {}\n",
    "        if base_model: color_map_specific[\"Qwen2.5 Base\"] = 'rgb(100, 149, 237)'\n",
    "        if instruct_model: color_map_specific[\"Qwen2.5 Instruct\"] = 'rgb(50, 205, 50)'\n",
    "        if coder_model: color_map_specific[\"Qwen2.5 Coder\"] = 'rgb(255, 165, 0)'\n",
    "        color_map_specific['Merged'] = 'rgb(192, 192, 192)' # For all merged types\n",
    "\n",
    "        plot_data_list = []\n",
    "        for task_idx, task_name in enumerate(tasks):\n",
    "            if task_name in summary_comp_df.index:\n",
    "                for model_idx, model_full_name in enumerate(comparison_models):\n",
    "                    if model_full_name in models_to_plot_bar:\n",
    "                        score = summary_comp_df.loc[task_name, model_full_name]\n",
    "                        if not pd.isna(score):\n",
    "                            model_short_clean = clean_plot_name(short_names.get(model_full_name, model_full_name))\n",
    "                            \n",
    "                            # Determine Model Type for coloring based on new short names\n",
    "                            current_model_type = 'Merged' # Default\n",
    "                            s_name = short_names.get(model_full_name)\n",
    "                            if s_name == \"Qwen2.5 Base\": current_model_type = \"Qwen2.5 Base\"\n",
    "                            elif s_name == \"Qwen2.5 Instruct\": current_model_type = \"Qwen2.5 Instruct\"\n",
    "                            elif s_name == \"Qwen2.5 Coder\": current_model_type = \"Qwen2.5 Coder\"\n",
    "                            \n",
    "                            plot_data_list.append({\n",
    "                                'Task': task_name,\n",
    "                                'Model Short Name': model_short_clean, # For Y-axis\n",
    "                                'Score': score,\n",
    "                                'Model Type': current_model_type, # For color\n",
    "                                'Task Index': task_idx,\n",
    "                                'Model Index': model_idx\n",
    "                            })\n",
    "        \n",
    "        if not plot_data_list:\n",
    "            print(\"Skipping 'Absolute Performance Comparison on Main Tasks (Faceted Bar Chart)': No data to plot after filtering.\")\n",
    "        else:\n",
    "            horizontal_bar_df = pd.DataFrame(plot_data_list)\n",
    "            horizontal_bar_df.sort_values(by=['Task Index', 'Model Index'], ascending=[True, True], inplace=True)\n",
    "\n",
    "            fig_main_bar_faceted_title = 'Absolute Performance Comparison by Task'\n",
    "            fig_main_bar_faceted = px.bar(\n",
    "                horizontal_bar_df,\n",
    "                x='Score',\n",
    "                y='Model Short Name',\n",
    "                color='Model Type', # Use the determined model type for color\n",
    "                color_discrete_map=color_map_specific,\n",
    "                orientation='h',\n",
    "                title=fig_main_bar_faceted_title,\n",
    "                labels={'Score': 'Performance Score (%)', 'Model Short Name': 'Model', 'Model Type': 'Model Category'},\n",
    "                text='Score',\n",
    "                facet_row='Task',\n",
    "                category_orders={\"Task\": tasks}\n",
    "            )\n",
    "            \n",
    "            fig_main_bar_faceted.update_traces(\n",
    "                texttemplate='%{text:.2f}%',\n",
    "                textposition='outside'\n",
    "            )\n",
    "            \n",
    "            model_order_for_y = [clean_plot_name(short_names.get(m,m)) for m in comparison_models if m in models_to_plot_bar]\n",
    "            fig_main_bar_faceted.update_yaxes(categoryorder='array', categoryarray=model_order_for_y, title=None)\n",
    "            fig_main_bar_faceted.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "\n",
    "            num_y_categories = len(model_order_for_y)\n",
    "            \n",
    "            tasks_actually_plotted = [t for t in tasks if t in horizontal_bar_df['Task'].unique()]\n",
    "            num_total_facets = len(tasks_actually_plotted)\n",
    "\n",
    "            for facet_idx, task_name in enumerate(tasks_actually_plotted):\n",
    "\n",
    "                instruct_score_val = summary_comp_df.loc[task_name, instruct_model] if instruct_model in summary_comp_df.columns and task_name in summary_comp_df.index and not pd.isna(summary_comp_df.loc[task_name, instruct_model]) else np.nan\n",
    "                coder_score_val = summary_comp_df.loc[task_name, coder_model] if coder_model in summary_comp_df.columns and task_name in summary_comp_df.index and not pd.isna(summary_comp_df.loc[task_name, coder_model]) else np.nan\n",
    "                \n",
    "                if pd.isna(instruct_score_val) or pd.isna(coder_score_val):\n",
    "                    print(f\"Skipping lines/area for task '{task_name}' due to missing Instruct/Coder scores.\")\n",
    "                    continue\n",
    "\n",
    "                axis_num_suffix = num_total_facets - facet_idx\n",
    "                \n",
    "                current_xaxis_ref = f'x{axis_num_suffix}' if axis_num_suffix > 1 else 'x'\n",
    "                current_yaxis_ref = f'y{axis_num_suffix}' if axis_num_suffix > 1 else 'y'\n",
    "                \n",
    "                fig_main_bar_faceted.add_shape(\n",
    "                    type=\"rect\",\n",
    "                    xref=current_xaxis_ref, yref=current_yaxis_ref,\n",
    "                    x0=min(instruct_score_val, coder_score_val),\n",
    "                    x1=max(instruct_score_val, coder_score_val),\n",
    "                    y0=-0.5, y1=num_y_categories - 0.5,\n",
    "                    fillcolor=\"rgba(255, 128, 128, 0.2)\", # Light red shade\n",
    "                    line_width=0,\n",
    "                    layer=\"below\"\n",
    "                )\n",
    "                fig_main_bar_faceted.add_shape(\n",
    "                    type=\"line\",\n",
    "                    xref=current_xaxis_ref, yref=current_yaxis_ref,\n",
    "                    x0=instruct_score_val, y0=-0.5,\n",
    "                    x1=instruct_score_val, y1=num_y_categories - 0.5,\n",
    "                    line=dict(color=color_map_specific.get('Qwen2.5 Instruct', 'green'), dash=\"dash\", width=2),\n",
    "                    layer=\"above\" \n",
    "                )\n",
    "                fig_main_bar_faceted.add_shape(\n",
    "                    type=\"line\",\n",
    "                    xref=current_xaxis_ref, yref=current_yaxis_ref,\n",
    "                    x0=coder_score_val, y0=-0.5,\n",
    "                    x1=coder_score_val, y1=num_y_categories - 0.5,\n",
    "                    line=dict(color=color_map_specific.get('Qwen2.5 Coder', 'orange'), dash=\"dash\", width=2),\n",
    "                    layer=\"above\"\n",
    "                )\n",
    "\n",
    "            fig_main_bar_faceted.update_layout(\n",
    "                xaxis_showgrid=True,\n",
    "                yaxis_showgrid=False,\n",
    "                showlegend=False, # Removed legend\n",
    "                **font_config \n",
    "            )\n",
    "            \n",
    "            plot_height = max(400, num_total_facets * (num_y_categories * 25 + 70))\n",
    "            fig_main_bar_faceted.update_layout(height=plot_height, width=default_plot_width, margin=dict(l=150, r=50, t=50, b=50))\n",
    "\n",
    "\n",
    "            fig_main_bar_faceted.show()\n",
    "            print(f\"Generated plot: {fig_main_bar_faceted_title} (Faceted Horizontal with Colors & Shaded Area)\")\n",
    "    else:\n",
    "        print(\"Skipping 'Absolute Performance Comparison on Main Tasks (Faceted Bar Chart)': No models with data to plot.\")\n",
    "else:\n",
    "    print(\"Skipping 'Absolute Performance Comparison on Main Tasks (Faceted Bar Chart)': summary_comp_df is empty.\")\n",
    "\n",
    "# Modified Plot: Leaderboard Subtasks Performance Distribution by Model (Box Plot)\n",
    "print(\"\\n--- Generating Leaderboard Subtasks Performance Distribution Box Plot ---\")\n",
    "if not subtasks_comp_df.empty and 'subtask_cleaned' in subtasks_comp_df.columns and instruct_model and coder_model:\n",
    "    models_for_plot_full_names = [\n",
    "        m for m in comparison_models if m in subtasks_comp_df.columns\n",
    "    ]\n",
    "\n",
    "    if models_for_plot_full_names:\n",
    "        id_vars_melt = [col for col in ['subtask_cleaned', 'group', 'subtask'] if col in subtasks_comp_df.columns]\n",
    "        value_vars_melt = [m for m in models_for_plot_full_names if m in subtasks_comp_df.columns]\n",
    "\n",
    "        if not value_vars_melt:\n",
    "            print(\"Skipping 'Leaderboard Subtasks Performance Distribution Box Plot': No valid model columns found in subtasks_comp_df for melting.\")\n",
    "        else:\n",
    "            leaderboard_subtasks_melted_df = subtasks_comp_df.melt(\n",
    "                id_vars=id_vars_melt,\n",
    "                value_vars=value_vars_melt,\n",
    "                var_name='Model_Full_Name',\n",
    "                value_name='Score'\n",
    "            )\n",
    "            leaderboard_subtasks_melted_df.dropna(subset=['Score'], inplace=True)\n",
    "\n",
    "            if not leaderboard_subtasks_melted_df.empty:\n",
    "                leaderboard_subtasks_melted_df['Model'] = leaderboard_subtasks_melted_df['Model_Full_Name'].map(\n",
    "                    lambda x: clean_plot_name(short_names.get(x, x))\n",
    "                )\n",
    "                model_order_for_plot = [\n",
    "                    clean_plot_name(short_names.get(m, m)) for m in comparison_models if m in value_vars_melt\n",
    "                ]\n",
    "                # Ensure order only contains models present in the melted data\n",
    "                model_order_for_plot = [m for m in model_order_for_plot if m in leaderboard_subtasks_melted_df['Model'].unique()]\n",
    "\n",
    "\n",
    "                fig_leaderboard_subtasks_box_title = 'Leaderboard Subtasks Performance Distribution by Model'\n",
    "                fig_leaderboard_subtasks_box = px.box(\n",
    "                    leaderboard_subtasks_melted_df,\n",
    "                    x='Model',\n",
    "                    y='Score',\n",
    "                    color='Model', # Color by model, legend will be removed\n",
    "                    points=False, \n",
    "                    title=fig_leaderboard_subtasks_box_title,\n",
    "                    labels={'Score': 'Performance Score (%) on Subtasks', 'Model': 'Model'},\n",
    "                    category_orders={\"Model\": model_order_for_plot},\n",
    "                    hover_data=['subtask_cleaned'] if 'subtask_cleaned' in leaderboard_subtasks_melted_df.columns else None\n",
    "                )\n",
    "                \n",
    "                # Calculate medians for Instruct and Coder models\n",
    "                instruct_median_subtasks = np.nan\n",
    "                coder_median_subtasks = np.nan\n",
    "\n",
    "                if instruct_model in value_vars_melt:\n",
    "                    instruct_scores = leaderboard_subtasks_melted_df[leaderboard_subtasks_melted_df['Model_Full_Name'] == instruct_model]['Score']\n",
    "                    if not instruct_scores.empty:\n",
    "                        instruct_median_subtasks = instruct_scores.median()\n",
    "                \n",
    "                if coder_model in value_vars_melt:\n",
    "                    coder_scores = leaderboard_subtasks_melted_df[leaderboard_subtasks_melted_df['Model_Full_Name'] == coder_model]['Score']\n",
    "                    if not coder_scores.empty:\n",
    "                        coder_median_subtasks = coder_scores.median()\n",
    "\n",
    "                # Add shaded area and lines if medians are valid\n",
    "                if pd.notna(instruct_median_subtasks) and pd.notna(coder_median_subtasks):\n",
    "                    fig_leaderboard_subtasks_box.add_shape(\n",
    "                        type=\"rect\", xref=\"paper\", yref=\"y\", x0=0, x1=1,\n",
    "                        y0=min(instruct_median_subtasks, coder_median_subtasks),\n",
    "                        y1=max(instruct_median_subtasks, coder_median_subtasks),\n",
    "                        fillcolor=\"rgba(128, 128, 128, 0.2)\", line_width=0, layer=\"below\"\n",
    "                    )\n",
    "                    fig_leaderboard_subtasks_box.add_hline(\n",
    "                        y=instruct_median_subtasks, line_dash=\"dash\", line_color=\"green\",\n",
    "                        annotation_text=f\"Instruct Median: {instruct_median_subtasks:.2f}\",\n",
    "                        annotation_position=\"bottom right\", layer=\"above\"\n",
    "                    )\n",
    "                    fig_leaderboard_subtasks_box.add_hline(\n",
    "                        y=coder_median_subtasks, line_dash=\"dash\", line_color=\"orange\",\n",
    "                        annotation_text=f\"Coder Median: {coder_median_subtasks:.2f}\",\n",
    "                        annotation_position=\"top right\", layer=\"above\"\n",
    "                    )\n",
    "                elif pd.notna(instruct_median_subtasks):\n",
    "                     fig_leaderboard_subtasks_box.add_hline(\n",
    "                        y=instruct_median_subtasks, line_dash=\"dash\", line_color=\"green\",\n",
    "                        annotation_text=f\"Instruct Median: {instruct_median_subtasks:.2f}\",\n",
    "                        annotation_position=\"bottom right\", layer=\"above\"\n",
    "                    )\n",
    "                elif pd.notna(coder_median_subtasks):\n",
    "                    fig_leaderboard_subtasks_box.add_hline(\n",
    "                        y=coder_median_subtasks, line_dash=\"dash\", line_color=\"orange\",\n",
    "                        annotation_text=f\"Coder Median: {coder_median_subtasks:.2f}\",\n",
    "                        annotation_position=\"top right\", layer=\"above\"\n",
    "                    )\n",
    "                \n",
    "                # Calculate and add mean points for each model\n",
    "                mean_scores_data = leaderboard_subtasks_melted_df.groupby('Model')['Score'].mean().reset_index()\n",
    "                \n",
    "                model_order_df = pd.DataFrame({'Model': model_order_for_plot})\n",
    "                mean_scores_to_plot = pd.merge(model_order_df, mean_scores_data, on='Model', how='left')\n",
    "\n",
    "\n",
    "                fig_leaderboard_subtasks_box.add_trace(go.Scatter(\n",
    "                    x=mean_scores_to_plot['Model'],\n",
    "                    y=mean_scores_to_plot['Score'],\n",
    "                    mode='markers',\n",
    "                    marker=dict(symbol='diamond', size=8, color='black'),\n",
    "                    name='Mean Score',\n",
    "                    showlegend=True # Explicitly show this trace in legend if others are hidden by main showlegend=False\n",
    "                ))\n",
    "\n",
    "\n",
    "                fig_leaderboard_subtasks_box.update_layout(\n",
    "                    height=600, width=default_plot_width, \n",
    "                    showlegend=False, # Remove main legend for model colors\n",
    "                    **font_config\n",
    "                ) \n",
    "                fig_leaderboard_subtasks_box.update_xaxes(tickangle=45)\n",
    "                fig_leaderboard_subtasks_box.show()\n",
    "                print(f\"Generated plot: {fig_leaderboard_subtasks_box_title}\")\n",
    "            else:\n",
    "                print(\"Skipping 'Leaderboard Subtasks Performance Distribution Box Plot': No data to plot after melting and NaN removal.\")\n",
    "    else:\n",
    "        print(\"Skipping 'Leaderboard Subtasks Performance Distribution Box Plot': No relevant models found in subtasks_comp_df columns.\")\n",
    "else:\n",
    "    print(\"Skipping 'Leaderboard Subtasks Performance Distribution Box Plot': subtasks_comp_df is empty, 'subtask_cleaned' column is missing, or Instruct/Coder models not defined.\")\n",
    "\n",
    "\n",
    "# New Plot: Absolute Performance Comparison for Leaderboard Subtasks (Faceted Horizontal Bar Chart Grid)\n",
    "if not subtasks_comp_df.empty and 'group' in subtasks_comp_df.columns and 'subtask_cleaned' in subtasks_comp_df.columns:\n",
    "    print(\"\\n--- Generating Subtask Performance Comparison Chart ---\")\n",
    "    \n",
    "    subtask_models_to_plot = [m for m in comparison_models if m in subtasks_comp_df.columns]\n",
    "    \n",
    "    valid_subtask_models = []\n",
    "    for m in subtask_models_to_plot:\n",
    "        if m in subtasks_comp_df.columns and not subtasks_comp_df[m].isna().all():\n",
    "            valid_subtask_models.append(m)\n",
    "    subtask_models_to_plot = valid_subtask_models\n",
    "    \n",
    "    if not subtask_models_to_plot:\n",
    "        print(\"Skipping Subtask Performance Comparison Chart: No models with non-NaN data in subtasks_comp_df.\")\n",
    "    else:\n",
    "        subtask_plot_data_list = []\n",
    "        \n",
    "        subtask_model_order_y = [clean_plot_name(short_names.get(m, m)) for m in comparison_models if m in subtask_models_to_plot]\n",
    "        \n",
    "        filtered_subtasks_df_for_plot = subtasks_comp_df[['group', 'subtask_cleaned'] + subtask_models_to_plot].copy()\n",
    "        filtered_subtasks_df_for_plot.dropna(subset=subtask_models_to_plot, how='all', inplace=True)\n",
    "\n",
    "        sorted_groups_from_data = sorted(filtered_subtasks_df_for_plot['group'].unique())\n",
    "        all_subtasks_sorted_from_data = sorted(filtered_subtasks_df_for_plot['subtask_cleaned'].unique())\n",
    "\n",
    "\n",
    "        for group_name in sorted_groups_from_data:\n",
    "            group_data = filtered_subtasks_df_for_plot[filtered_subtasks_df_for_plot['group'] == group_name]\n",
    "            for subtask_name in sorted(group_data['subtask_cleaned'].unique()):\n",
    "                subtask_data_for_model = group_data[group_data['subtask_cleaned'] == subtask_name]\n",
    "                for model_full_name in comparison_models:\n",
    "                    if model_full_name in subtask_models_to_plot and model_full_name in subtask_data_for_model.columns:\n",
    "                        score_series = subtask_data_for_model[model_full_name]\n",
    "                        if not score_series.empty:\n",
    "                            score = score_series.iloc[0]\n",
    "                            if not pd.isna(score):\n",
    "                                model_short_clean = clean_plot_name(short_names.get(model_full_name, model_full_name))\n",
    "                                \n",
    "                                # Determine Model Type for coloring\n",
    "                                current_model_type = 'Merged' # Default\n",
    "                                s_name = short_names.get(model_full_name)\n",
    "                                if s_name == \"Qwen2.5 Base\": current_model_type = \"Qwen2.5 Base\"\n",
    "                                elif s_name == \"Qwen2.5 Instruct\": current_model_type = \"Qwen2.5 Instruct\"\n",
    "                                elif s_name == \"Qwen2.5 Coder\": current_model_type = \"Qwen2.5 Coder\"\n",
    "                                \n",
    "                                subtask_plot_data_list.append({\n",
    "                                    'Group': group_name,\n",
    "                                    'Subtask': subtask_name,\n",
    "                                    'Model Short Name': model_short_clean,\n",
    "                                    'Score': score,\n",
    "                                    'Model Type': current_model_type # For color\n",
    "                                })\n",
    "\n",
    "        if not subtask_plot_data_list:\n",
    "            print(\"Skipping Subtask Performance Comparison Chart: No data to plot after detailed filtering.\")\n",
    "        else:\n",
    "            subtask_horizontal_bar_df = pd.DataFrame(subtask_plot_data_list)\n",
    "\n",
    "            model_type_color_map = {\n",
    "                \"Qwen2.5 Base\": 'rgb(100, 149, 237)', \"Qwen2.5 Instruct\": 'rgb(50, 205, 50)',\n",
    "                \"Qwen2.5 Coder\": 'rgb(255, 165, 0)', 'Merged': 'rgb(192, 192, 192)'\n",
    "            }\n",
    "            \n",
    "            facet_col_wrap_value = 4 \n",
    "\n",
    "            fig_subtasks_faceted_title = 'Subtask Performance Comparison by Group and Subtask'\n",
    "            fig_subtasks_faceted = px.bar(\n",
    "                subtask_horizontal_bar_df,\n",
    "                x='Score', y='Model Short Name', color='Model Type',\n",
    "                color_discrete_map=model_type_color_map, orientation='h',\n",
    "                title=fig_subtasks_faceted_title,\n",
    "                labels={'Score': 'Perf. Score (%)', 'Model Short Name': 'Model', 'Model Type': 'Category'},\n",
    "                text='Score', facet_row='Group', facet_col='Subtask',\n",
    "                category_orders={\n",
    "                    \"Group\": sorted_groups_from_data,\n",
    "                    \"Subtask\": all_subtasks_sorted_from_data,\n",
    "                    \"Model Short Name\": subtask_model_order_y\n",
    "                },\n",
    "                facet_col_wrap=facet_col_wrap_value\n",
    "            )\n",
    "            \n",
    "            fig_subtasks_faceted.update_traces(texttemplate='%{text:.2f}%', textposition='outside')\n",
    "            fig_subtasks_faceted.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "            \n",
    "            fig_subtasks_faceted.update_yaxes(matches=None, showticklabels=True, title=None, categoryorder='array', categoryarray=subtask_model_order_y)\n",
    "            fig_subtasks_faceted.update_xaxes(matches=None, showticklabels=True, title=None, tickangle=0)\n",
    "\n",
    "\n",
    "            num_y_cats_sub = len(subtask_model_order_y)\n",
    "            \n",
    "            for r_idx, group_val in enumerate(sorted_groups_from_data):\n",
    "                for c_idx, subtask_val in enumerate(all_subtasks_sorted_from_data):\n",
    "                    cell_df = subtask_horizontal_bar_df[\n",
    "                        (subtask_horizontal_bar_df['Group'] == group_val) &\n",
    "                        (subtask_horizontal_bar_df['Subtask'] == subtask_val)\n",
    "                    ]\n",
    "                    if cell_df.empty:\n",
    "                        continue\n",
    "\n",
    "                    instruct_s = np.nan\n",
    "                    coder_s = np.nan\n",
    "                    \n",
    "                    original_data_point = subtasks_comp_df[\n",
    "                        (subtasks_comp_df['group'] == group_val) &\n",
    "                        (subtasks_comp_df['subtask_cleaned'] == subtask_val)\n",
    "                    ]\n",
    "\n",
    "                    if not original_data_point.empty:\n",
    "                        if instruct_model in original_data_point.columns:\n",
    "                            instruct_s = original_data_point[instruct_model].iloc[0]\n",
    "                        if coder_model in original_data_point.columns:\n",
    "                            coder_s = original_data_point[coder_model].iloc[0]\n",
    "\n",
    "                    if pd.isna(instruct_s) or pd.isna(coder_s):\n",
    "                        continue\n",
    "                    \n",
    "                    plot_row_num_for_shape = r_idx + 1\n",
    "                    plot_col_num_for_shape = c_idx + 1\n",
    "                    \n",
    "                    fig_subtasks_faceted.add_shape(\n",
    "                        type=\"rect\", x0=min(instruct_s, coder_s), x1=max(instruct_s, coder_s),\n",
    "                        y0=-0.5, y1=num_y_cats_sub - 0.5,\n",
    "                        fillcolor=\"rgba(128, 128, 128, 0.2)\", line_width=0, layer=\"below\",\n",
    "                        row=plot_row_num_for_shape, col=plot_col_num_for_shape\n",
    "                    )\n",
    "                    fig_subtasks_faceted.add_shape(\n",
    "                        type=\"line\", x0=instruct_s, y0=-0.5, x1=instruct_s, y1=num_y_cats_sub - 0.5,\n",
    "                        line=dict(color=model_type_color_map.get('Qwen2.5 Instruct'), dash=\"dash\", width=1.5), layer=\"above\",\n",
    "                        row=plot_row_num_for_shape, col=plot_col_num_for_shape\n",
    "                    )\n",
    "                    fig_subtasks_faceted.add_shape(\n",
    "                        type=\"line\", x0=coder_s, y0=-0.5, x1=coder_s, y1=num_y_cats_sub - 0.5,\n",
    "                        line=dict(color=model_type_color_map.get('Qwen2.5 Coder'), dash=\"dash\", width=1.5), layer=\"above\",\n",
    "                        row=plot_row_num_for_shape, col=plot_col_num_for_shape\n",
    "                    )\n",
    "            \n",
    "            fig_subtasks_faceted.update_layout(showlegend=False, **font_config) # Removed legend\n",
    "            \n",
    "            num_facet_r = len(sorted_groups_from_data)\n",
    "            num_facet_c = len(all_subtasks_sorted_from_data)\n",
    "            \n",
    "            row_height_per_facet = (len(subtask_model_order_y) * 20 + 60) \n",
    "            \n",
    "            wrap = facet_col_wrap_value if facet_col_wrap_value > 0 and facet_col_wrap_value <= num_facet_c else num_facet_c\n",
    "            actual_cols_on_screen = min(num_facet_c, wrap)\n",
    "            actual_rows_on_screen = -(-num_facet_c // wrap) * num_facet_r \n",
    "\n",
    "            plot_h_wrapped = max(400, actual_rows_on_screen * row_height_per_facet + 100) \n",
    "            \n",
    "            col_width_per_facet = 250 \n",
    "            plot_w_wrapped = max(default_plot_width, actual_cols_on_screen * col_width_per_facet + 150) \n",
    "\n",
    "\n",
    "            fig_subtasks_faceted.update_layout(\n",
    "                height=plot_h_wrapped, \n",
    "                width=plot_w_wrapped, \n",
    "                margin=dict(l=100, t=80, b=50, r=50) \n",
    "            )\n",
    "            \n",
    "            if actual_cols_on_screen > 3: \n",
    "                 fig_subtasks_faceted.for_each_annotation(lambda a: a.update(font=dict(size=10)) if \"Subtask=\" in a.text else a)\n",
    "            if actual_rows_on_screen > 3: \n",
    "                 fig_subtasks_faceted.for_each_annotation(lambda a: a.update(font=dict(size=10)) if \"Group=\" in a.text else a)\n",
    "\n",
    "\n",
    "            fig_subtasks_faceted.show()\n",
    "            print(f\"Generated plot: {fig_subtasks_faceted_title} (Grid with Colors & Shaded Area)\")\n",
    "else:\n",
    "    print(\"Skipping Subtask Performance Comparison Chart: `subtasks_comp_df` is empty or critical columns missing.\")\n",
    "\n",
    "\n",
    "# Plot: Subtask Difference Boxplot(s)\n",
    "plot_dcoder_col = 'd_coder' if 'd_coder' in subtasks_comp_df.columns else None\n",
    "if not subtasks_comp_df.empty and plot_dcoder_col and 'group' in subtasks_comp_df.columns and not subtasks_comp_df[plot_dcoder_col].isna().all():\n",
    "    print(\"Generating Subtask Difference Boxplot(s)...\")\n",
    "    merged_diff_cols_sub_present = [col for col in diff_cols_subtasks if col.startswith('d_merged_') and col in subtasks_comp_df.columns and not subtasks_comp_df[col].isna().all()]\n",
    "\n",
    "    if not merged_diff_cols_sub_present:\n",
    "        plot_data_box_im_only = subtasks_comp_df.melt(id_vars=['group', 'subtask_cleaned'], value_vars=[plot_dcoder_col], var_name='difference_type', value_name='difference_value'); plot_data_box_im_only.dropna(subset=['difference_value'], inplace=True)\n",
    "        if not plot_data_box_im_only.empty:\n",
    "            label_map_im_only = {plot_dcoder_col: f'{instruct_short_label}–{coder_short_label}'}; plot_data_box_im_only['difference_label'] = plot_data_box_im_only['difference_type'].map(label_map_im_only)\n",
    "            fig2_im_only_title = f'Subtask Differences: {instruct_short_label}–{coder_short_label}'\n",
    "            fig2_im_only = px.box(plot_data_box_im_only, x='group', y='difference_value', color='difference_label', hover_data=['subtask_cleaned'], labels={'group': 'Group', 'difference_value': 'Difference (%)', 'difference_label': 'Difference Type', 'subtask_cleaned': 'Subtask'}, title=fig2_im_only_title, category_orders={\"group\": sorted(plot_data_box_im_only['group'].unique())})\n",
    "            fig2_im_only.update_xaxes(tickangle=45)\n",
    "            fig2_im_only.update_layout(width=default_plot_width, **font_config) \n",
    "            fig2_im_only.show(); print(f\"  - Generated plot: {fig2_im_only_title}\")\n",
    "\n",
    "    for merged_diff_col in merged_diff_cols_sub_present:\n",
    "        merged_short_name = merged_diff_col.replace('d_merged_', '')\n",
    "        cleaned_merged_short_name = clean_plot_name(merged_short_name)\n",
    "        cols_for_this_plot = [plot_dcoder_col, merged_diff_col]\n",
    "        \n",
    "        plot_data_box_single = subtasks_comp_df.melt(id_vars=['group', 'subtask_cleaned'], value_vars=cols_for_this_plot, var_name='difference_type', value_name='difference_value'); plot_data_box_single.dropna(subset=['difference_value'], inplace=True)\n",
    "        if not plot_data_box_single.empty and 'group' in plot_data_box_single.columns:\n",
    "            label_map = {plot_dcoder_col: f'{instruct_short_label}–{coder_short_label}', merged_diff_col: f'{instruct_short_label}–{cleaned_merged_short_name}'}; plot_data_box_single['difference_label'] = plot_data_box_single['difference_type'].map(label_map)\n",
    "            fig2_single_title = f'Subtask Diffs: {instruct_short_label}–{coder_short_label} vs {instruct_short_label}–{cleaned_merged_short_name}'\n",
    "            fig2_single = px.box(plot_data_box_single, x='group', y='difference_value', color='difference_label', hover_data=['subtask_cleaned'], labels={'group': 'Group', 'difference_value': 'Difference (%)', 'difference_label': 'Difference Type', 'subtask_cleaned': 'Subtask'}, title=fig2_single_title, category_orders={\"group\": sorted(plot_data_box_single['group'].unique())})\n",
    "            fig2_single.update_xaxes(tickangle=45); fig2_single.update_layout(boxmode='group', width=default_plot_width, **font_config); fig2_single.show(); print(f\"  - Generated plot: {fig2_single_title}\") \n",
    "else: print(\"Skipping Subtask Difference Boxplot(s): subtasks_comp_df empty, 'd_coder' or 'group' column missing, or 'd_coder' has no data.\")\n",
    "\n",
    "base_comparison_models_plot3 = []\n",
    "if base_model and base_model in subtasks_df.columns: base_comparison_models_plot3.append(base_model)\n",
    "if instruct_model and instruct_model in subtasks_df.columns: base_comparison_models_plot3.append(instruct_model)\n",
    "if coder_model and coder_model in subtasks_df.columns: base_comparison_models_plot3.append(coder_model)\n",
    "base_comparison_models_plot3 = list(dict.fromkeys(base_comparison_models_plot3))\n",
    "\n",
    "if not subtasks_df.empty and 'group' in subtasks_df.columns and base_comparison_models_plot3:\n",
    "    print(\"Generating Absolute Score Boxplot(s) for Subtasks...\")\n",
    "    present_merged_models_in_subtasks = [m for m in merged_models if m in subtasks_df.columns]\n",
    "    \n",
    "    if not present_merged_models_in_subtasks: print(\"  - Info: No merged models found in subtask data to pair with base/instruct/coder for boxplots.\")\n",
    "\n",
    "    if not present_merged_models_in_subtasks and base_comparison_models_plot3:\n",
    "        plot_data_base_only = subtasks_df.melt(id_vars=['group', 'subtask_cleaned'], value_vars=base_comparison_models_plot3, var_name='model_full_name', value_name='score'); plot_data_base_only.dropna(subset=['score'], inplace=True)\n",
    "        if not plot_data_base_only.empty and 'group' in plot_data_base_only.columns:\n",
    "            plot_data_base_only['model_short_name'] = plot_data_base_only['model_full_name'].map(lambda x: clean_plot_name(short_names.get(x,x)))\n",
    "            fig_abs_base_title = f'Absolute Subtask Perf: Base/Instruct/Coder Models'\n",
    "            fig_abs_base = px.box(plot_data_base_only, x='group', y='score', color='model_short_name', hover_data=['subtask_cleaned'], labels={'group': 'Group', 'score': 'Absolute Score (%)', 'model_short_name': 'Model', 'subtask_cleaned': 'Subtask'}, title=fig_abs_base_title, category_orders={\"group\": sorted(plot_data_base_only['group'].unique())})\n",
    "            fig_abs_base.update_xaxes(tickangle=45); fig_abs_base.update_layout(boxmode='group', width=default_plot_width, **font_config); fig_abs_base.show(); print(f\"  - Generated plot: {fig_abs_base_title}\") \n",
    "\n",
    "    for merged_m in present_merged_models_in_subtasks:\n",
    "        merged_short_name = short_names.get(merged_m, merged_m)\n",
    "        cleaned_merged_short_name = clean_plot_name(merged_short_name)\n",
    "        \n",
    "        models_for_this_plot = base_comparison_models_plot3 + [merged_m]\n",
    "        models_for_this_plot = [m for m in models_for_this_plot if m in subtasks_df.columns]\n",
    "        models_for_this_plot = list(dict.fromkeys(models_for_this_plot))\n",
    "\n",
    "        if len(models_for_this_plot) < 1:\n",
    "            print(f\"  - Skipping boxplot for {cleaned_merged_short_name}: No valid models to plot from {models_for_this_plot}\")\n",
    "            continue\n",
    "\n",
    "        plot_data_abs_single = subtasks_df.melt(id_vars=['group', 'subtask_cleaned'], value_vars=models_for_this_plot, var_name='model_full_name', value_name='score'); plot_data_abs_single.dropna(subset=['score'], inplace=True)\n",
    "        \n",
    "        if not plot_data_abs_single.empty and 'group' in plot_data_abs_single.columns:\n",
    "            plot_data_abs_single['model_short_name'] = plot_data_abs_single['model_full_name'].map(lambda x: clean_plot_name(short_names.get(x,x)))\n",
    "            fig_abs_single_title = f'Absolute Subtask Perf: Incl. {cleaned_merged_short_name}'\n",
    "            fig_abs_single = px.box(plot_data_abs_single, x='group', y='score', color='model_short_name', hover_data=['subtask_cleaned'], labels={'group': 'Group', 'score': 'Absolute Score (%)', 'model_short_name': 'Model', 'subtask_cleaned': 'Subtask'}, title=fig_abs_single_title, category_orders={\"group\": sorted(plot_data_abs_single['group'].unique())})\n",
    "            fig_abs_single.update_xaxes(tickangle=45); fig_abs_single.update_layout(boxmode='group', width=default_plot_width, **font_config); fig_abs_single.show(); print(f\"  - Generated plot: {fig_abs_single_title}\") \n",
    "else: print(f\"Skipping Absolute Score Boxplot(s) for Subtasks: subtasks_df empty, 'group' column missing, or no base/instruct/coder models with data.\")\n",
    "\n",
    "\n",
    "plot_dcoder_col = 'd_coder' if 'd_coder' in subtasks_comp_df.columns else None\n",
    "plot_merged_diff_cols_sub = [c for c in diff_cols_subtasks if c.startswith('d_merged_') and c in subtasks_comp_df.columns and not subtasks_comp_df[c].isna().all()]\n",
    "if not subtasks_comp_df.empty and plot_dcoder_col and not subtasks_comp_df[plot_dcoder_col].isna().all() and plot_merged_diff_cols_sub:\n",
    "    print(\"Generating Jointplot(s) for Subtask Differences (I-M vs I-MergedX)...\")\n",
    "    for i, merged_diff_col in enumerate(plot_merged_diff_cols_sub):\n",
    "        merged_short_name = merged_diff_col.replace('d_merged_', '')\n",
    "        cleaned_merged_short_name = clean_plot_name(merged_short_name)\n",
    "        required_cols_joint = [plot_dcoder_col, merged_diff_col, 'subtask_cleaned', 'group']\n",
    "        if all(c in subtasks_comp_df.columns for c in required_cols_joint):\n",
    "            plot_data_joint = subtasks_comp_df[required_cols_joint].dropna(subset=[plot_dcoder_col, merged_diff_col])\n",
    "            if not plot_data_joint.empty:\n",
    "                plot_data_joint['hover_name_joint'] = plot_data_joint['subtask_cleaned'] + \" (Group: \" + plot_data_joint['group'].astype(str) + \")\"\n",
    "                fig_joint_title = f'Joint Dist: (I-M) vs (I-{cleaned_merged_short_name}) Diffs'\n",
    "                fig_joint = px.scatter(plot_data_joint, x=plot_dcoder_col, y=merged_diff_col,\n",
    "                                       marginal_x=\"histogram\", marginal_y=\"histogram\", trendline=\"ols\",\n",
    "                                       hover_name='hover_name_joint',\n",
    "                                       hover_data={plot_dcoder_col: ':.2f', merged_diff_col: ':.2f', 'group': True, 'subtask_cleaned': True, 'hover_name_joint': False},\n",
    "                                       labels={plot_dcoder_col: f'{instruct_short_label}–{coder_short_label} Diff (%)', merged_diff_col: f'{instruct_short_label}–{cleaned_merged_short_name} Diff (%)', 'subtask_cleaned': 'Subtask', 'group': 'Group'},\n",
    "                                       title=fig_joint_title)\n",
    "                fig_joint.update_layout(width=default_plot_width, **font_config) \n",
    "                fig_joint.show(); print(f\"  - Generated plot: {fig_joint_title}\")\n",
    "else: print(\"Skipping Jointplot(s): subtasks_comp_df empty, 'd_coder' missing/all_NaN, or no valid merged diff columns.\")\n",
    "\n",
    "if not subtasks_comp_df.empty and plot_dcoder_col and not subtasks_comp_df[plot_dcoder_col].isna().all() and plot_merged_diff_cols_sub:\n",
    "    print(\"Generating Scatter Plot(s) for Subtask Differences (I-M vs I-MergedX)...\")\n",
    "    for i, merged_diff_col in enumerate(plot_merged_diff_cols_sub):\n",
    "        merged_short_name = merged_diff_col.replace('d_merged_', '')\n",
    "        cleaned_merged_short_name = clean_plot_name(merged_short_name)\n",
    "        required_cols_scatter = [plot_dcoder_col, merged_diff_col, 'subtask_cleaned', 'group']\n",
    "        if all(c in subtasks_comp_df.columns for c in required_cols_scatter):\n",
    "             plot_data_scatter = subtasks_comp_df[required_cols_scatter].dropna(subset=[plot_dcoder_col, merged_diff_col])\n",
    "             if not plot_data_scatter.empty and 'group' in plot_data_scatter.columns:\n",
    "                fig_scatter_comp_title = f'Subtask Diffs Comp: (I-M) vs (I-{cleaned_merged_short_name})'\n",
    "                fig_scatter_comp = px.scatter(plot_data_scatter, x=plot_dcoder_col, y=merged_diff_col, color='group',\n",
    "                                              trendline=\"ols\", hover_name='subtask_cleaned',\n",
    "                                              hover_data={'group':True, plot_dcoder_col:':.2f', merged_diff_col:':.2f'},\n",
    "                                              labels={plot_dcoder_col: f'{instruct_short_label}–{coder_short_label} Diff (%)', merged_diff_col: f'{instruct_short_label}–{cleaned_merged_short_name} Diff (%)', 'subtask_cleaned': 'Subtask', 'group': 'Group'},\n",
    "                                              title=fig_scatter_comp_title)\n",
    "                fig_scatter_comp.update_layout(width=default_plot_width, **font_config) \n",
    "                fig_scatter_comp.show(); print(f\"  - Generated plot: {fig_scatter_comp_title}\")\n",
    "else: print(f\"Skipping Scatter Plot(s): subtasks_comp_df empty, 'd_coder' missing/all_NaN, or no valid merged diff columns.\")\n",
    "\n",
    "if not subtasks_comp_df.empty and plot_dcoder_col and not subtasks_comp_df[plot_dcoder_col].isna().all() and plot_merged_diff_cols_sub:\n",
    "    print(\"Generating Top/Bottom Subtask Impact Plot(s)...\"); N = 5\n",
    "    for i, merged_diff_col in enumerate(plot_merged_diff_cols_sub):\n",
    "        merged_short_name = merged_diff_col.replace('d_merged_', '')\n",
    "        cleaned_merged_short_name = clean_plot_name(merged_short_name)\n",
    "        impact_col = f'impact_{cleaned_merged_short_name}'\n",
    "        subtasks_comp_df[impact_col] = subtasks_comp_df[plot_dcoder_col] - subtasks_comp_df[merged_diff_col]\n",
    "        subtasks_sorted = subtasks_comp_df.dropna(subset=[impact_col]).sort_values(impact_col)\n",
    "        \n",
    "        if len(subtasks_sorted) >= N * 2:\n",
    "            top_n = subtasks_sorted.nlargest(N, impact_col); bottom_n = subtasks_sorted.nsmallest(N, impact_col); plot_data_bar = pd.concat([top_n, bottom_n]).drop_duplicates(subset=['subtask_cleaned'])\n",
    "            if not plot_data_bar.empty:\n",
    "                hover_cols = {'subtask_cleaned': False, 'group': True}; labels_dict = {'subtask_cleaned': 'Subtask', 'group':'Group'}\n",
    "                if impact_col in plot_data_bar.columns: hover_cols[impact_col] = ':.2f'; labels_dict[impact_col] = f'Impact: ({coder_short_label} - {cleaned_merged_short_name}) Rel. to {instruct_short_label} (%)'\n",
    "                if plot_dcoder_col in plot_data_bar.columns: hover_cols[plot_dcoder_col] = ':.2f'; labels_dict[plot_dcoder_col] = f'I-M Diff (%)'\n",
    "                if merged_diff_col in plot_data_bar.columns: hover_cols[merged_diff_col] = ':.2f'; labels_dict[merged_diff_col] = f'I-{cleaned_merged_short_name} Diff (%)'\n",
    "                \n",
    "                fig_bar_title = f'Top/Bottom {N} Subtasks: Rel. Impact of {cleaned_merged_short_name} vs {coder_short_label}'\n",
    "                fig_bar = px.bar(plot_data_bar, x=impact_col, y='subtask_cleaned', orientation='h',\n",
    "                                 color=impact_col, color_continuous_scale=px.colors.diverging.RdBu, color_continuous_midpoint=0,\n",
    "                                 hover_data=hover_cols, labels=labels_dict, title=fig_bar_title)\n",
    "                fig_bar.update_layout(yaxis={'categoryorder':'total ascending'}, width=default_plot_width, **font_config); fig_bar.show(); print(f\"  - Generated plot: {fig_bar_title}\") \n",
    "else: print(\"Skipping Top/Bottom Subtask Impact Plot(s): subtasks_comp_df empty, 'd_coder' missing/all_NaN, or no valid merged diff columns.\")\n",
    "\n",
    "if not subtasks_comp_df.empty and plot_dcoder_col and not subtasks_comp_df[plot_dcoder_col].isna().all() and plot_merged_diff_cols_sub:\n",
    "    print(\"Generating Clustermap/Dendrogram(s) for Subtasks (I-M vs I-MergedX)...\")\n",
    "    for merged_diff_col in plot_merged_diff_cols_sub:\n",
    "        merged_short_name = merged_diff_col.replace('d_merged_', '')\n",
    "        cleaned_merged_short_name = clean_plot_name(merged_short_name)\n",
    "        cluster_cols_single = [plot_dcoder_col, merged_diff_col]\n",
    "        if 'subtask_cleaned' in subtasks_comp_df.columns:\n",
    "            diff_matrix_single = subtasks_comp_df.set_index('subtask_cleaned')[cluster_cols_single].dropna(how='any')\n",
    "            if not diff_matrix_single.empty and len(diff_matrix_single) > 1:\n",
    "                scaler = StandardScaler(); scaled_data_single = scaler.fit_transform(diff_matrix_single.values)\n",
    "                try:\n",
    "                    row_linkage_single = linkage(pdist(scaled_data_single), method='average', metric='euclidean'); ordered_row_indices_single = leaves_list(row_linkage_single)\n",
    "                    heatmap_data_ordered_single = scaled_data_single[ordered_row_indices_single]; ordered_row_labels_single = diff_matrix_single.index[ordered_row_indices_single].tolist()\n",
    "                    heatmap_col_labels_single = [f'I-M', f'I-{cleaned_merged_short_name}']\n",
    "                    \n",
    "                    fig_heatmap_single_title = f'Clustered Heatmap: Profile for {cleaned_merged_short_name} (Scaled)'\n",
    "                    fig_heatmap_single = px.imshow(heatmap_data_ordered_single, labels=dict(x=\"Difference Type (vs Instruct)\", y=\"Subtask\", color=\"Scaled Value\"), x=heatmap_col_labels_single, y=ordered_row_labels_single, aspect=\"auto\", color_continuous_scale='RdBu_r', title=fig_heatmap_single_title)\n",
    "                    fig_heatmap_single.update_xaxes(side=\"top\"); fig_heatmap_single.update_layout(height=max(600, 20 * len(ordered_row_labels_single)), width=default_plot_width, **font_config); fig_heatmap_single.show(); print(f\"  - Generated plot: {fig_heatmap_single_title}\") \n",
    "                    \n",
    "                    if len(scaled_data_single) > 1:\n",
    "                        fig_dendro_row_single_title = f'Row Dendrogram: Profile for {cleaned_merged_short_name} (Scaled)'\n",
    "                        fig_dendro_row_single = ff.create_dendrogram(scaled_data_single, orientation='right', labels=diff_matrix_single.index.tolist(), linkagefun=lambda x: linkage(x, method='average', metric='euclidean'))\n",
    "                        fig_dendro_row_single.update_layout(title=fig_dendro_row_single_title, height=max(600, 20 * len(diff_matrix_single)), width=default_plot_width, **font_config); fig_dendro_row_single.show(); print(f\"  - Generated plot: {fig_dendro_row_single_title}\") \n",
    "                except Exception as e: print(f\"Error during clustering or plotting for {cleaned_merged_short_name}: {e}\")\n",
    "else: print(\"Skipping Clustermap/Dendrogram(s) for Subtasks: subtasks_comp_df empty, 'd_coder' missing/all_NaN, or no valid merged diff columns.\")\n",
    "\n",
    "\n",
    "plot_dcoder_col_main = 'd_coder' if 'd_coder' in summary_comp_df.columns and not summary_comp_df['d_coder'].isna().all() else None\n",
    "plot_merged_diff_cols_main = [c for c in diff_cols_main if c.startswith('d_merged_') and c in summary_comp_df.columns and not summary_comp_df[c].isna().all()]\n",
    "\n",
    "if plot_dcoder_col_main and plot_merged_diff_cols_main:\n",
    "     print(\"Generating Main Task Dendrogram(s) (I-M vs I-MergedX)...\")\n",
    "     for merged_diff_col_main in plot_merged_diff_cols_main:\n",
    "        merged_short_name = merged_diff_col_main.replace('d_merged_', '')\n",
    "        cleaned_merged_short_name = clean_plot_name(merged_short_name)\n",
    "        cols_main_single = [plot_dcoder_col_main, merged_diff_col_main]\n",
    "        \n",
    "        main_matrix_data_single = summary_comp_df[cols_main_single].dropna(how='any')\n",
    "        if len(main_matrix_data_single) >= 2:\n",
    "            try:\n",
    "                fig_dendro_main_s_title = f'Dendrogram: Main Tasks based on Profile for {cleaned_merged_short_name}'\n",
    "                fig_dendro_main_s = ff.create_dendrogram(main_matrix_data_single.values, labels=main_matrix_data_single.index.tolist(), linkagefun=lambda x: linkage(x, method='ward'))\n",
    "                dynamic_width_dendro_main = max(default_plot_width, 30 * len(main_matrix_data_single.index))\n",
    "                fig_dendro_main_s.update_layout(title=fig_dendro_main_s_title, yaxis_title='Distance', xaxis_title='Task', width=dynamic_width_dendro_main, height=default_plot_height, **font_config); fig_dendro_main_s.show(); print(f\"  - Generated plot: {fig_dendro_main_s_title}\") \n",
    "            except Exception as e: print(f\"Could not generate Main Tasks Dendrogram for {cleaned_merged_short_name}: {e}\")\n",
    "else: print(\"Skipping Main Task Dendrogram(s): 'd_coder' on main tasks missing/all_NaN or no valid merged diff columns for main tasks.\")\n",
    "\n",
    "plot_dcoder_col_sub = 'd_coder' if 'd_coder' in subtasks_comp_df.columns and not subtasks_comp_df['d_coder'].isna().all() else None\n",
    "plot_merged_diff_cols_sub = [c for c in diff_cols_subtasks if c.startswith('d_merged_') and c in subtasks_comp_df.columns and not subtasks_comp_df[c].isna().all()]\n",
    "\n",
    "if not subtasks_comp_df.empty and plot_dcoder_col_sub and plot_merged_diff_cols_sub:\n",
    "    print(\"Generating Subtask Dendrogram(s) (I-M vs I-MergedX)...\")\n",
    "    for merged_diff_col_sub_dendro in plot_merged_diff_cols_sub:\n",
    "       merged_short_name = merged_diff_col_sub_dendro.replace('d_merged_', '')\n",
    "       cleaned_merged_short_name = clean_plot_name(merged_short_name)\n",
    "       cols_sub_single = [plot_dcoder_col_sub, merged_diff_col_sub_dendro]\n",
    "       if 'subtask_cleaned' in subtasks_comp_df.columns:\n",
    "           sub_matrix_df_dendro_s = subtasks_comp_df.set_index('subtask_cleaned')[cols_sub_single].dropna(how='any')\n",
    "           if len(sub_matrix_df_dendro_s) >= 2:\n",
    "               sub_values_s = sub_matrix_df_dendro_s.values; sub_labels_s = sub_matrix_df_dendro_s.index.tolist()\n",
    "               try:\n",
    "                   fig_dendro_sub_s_title = f'Dendrogram: Subtasks based on Profile for {cleaned_merged_short_name}'\n",
    "                   fig_dendro_sub_s = ff.create_dendrogram(sub_values_s, labels=sub_labels_s, linkagefun=lambda x: linkage(x, method='ward'))\n",
    "                   dynamic_width_dendro_sub = max(default_plot_width, 30 * len(sub_labels_s)) # Dynamic width\n",
    "                   fig_dendro_sub_s.update_layout(title=fig_dendro_sub_s_title, yaxis_title='Distance', xaxis_title='Subtask', height=max(600, 15 * len(sub_labels_s)), width=dynamic_width_dendro_sub, xaxis=dict(tickangle=-90), **font_config); fig_dendro_sub_s.show(); print(f\"  - Generated plot: {fig_dendro_sub_s_title}\") \n",
    "               except Exception as e: print(f\"Could not generate Subtasks Dendrogram for {cleaned_merged_short_name}: {e}\")\n",
    "else: print(\"Skipping Subtask Dendrogram(s): subtasks_comp_df empty, 'd_coder' for subtasks missing/all_NaN, or no valid merged diff columns for subtasks.\")\n",
    "print(\"\\n--- Script Finished ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
