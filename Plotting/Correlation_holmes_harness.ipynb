{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08482cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from scipy.stats import kendalltau # Replaced by pandas.corr\n",
    "from sklearn.preprocessing import StandardScaler # Only if needed by ranking/helpers, not directly for kendall\n",
    "from collections import defaultdict\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "# import plotly.figure_factory as ff # For dendrograms if kept, not primary for heatmaps\n",
    "# from plotly.subplots import make_subplots # Not used in current version\n",
    "import plotly.io as pio\n",
    "import re\n",
    "\n",
    "# Set default plotly template\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# --- 1. Global Configurations ---\n",
    "\n",
    "# Please define the models and their short/holmes/harness names here, which you want to analyze.\n",
    "MODEL_CONFIG = [\n",
    "    # Core Models\n",
    "    {\"short_name\": \"Qwen2.5 Base\", \"holmes_name\": \"Qwen__Qwen2.5-7B\", \"harness_name\": \"Qwen2.5-7B\"},\n",
    "    {\"short_name\": \"Qwen2.5 Instruct\", \"holmes_name\": \"Qwen__Qwen2.5-7B-Instruct\", \"harness_name\": \"Qwen2.5-7B-Instruct\"},\n",
    "    {\"short_name\": \"Qwen2.5 Coder\", \"holmes_name\": \"Qwen__Qwen2.5-Coder-7B\", \"harness_name\": \"Qwen2.5-Coder-7B\"},\n",
    "    {\"short_name\": \"Qwen2.5 Math\", \"holmes_name\": \"Qwen__Qwen2.5-Math-7B\", \"harness_name\": \"Qwen2.5-Math-7B\"},\n",
    "    # Coder Merged Models\n",
    "    {\"short_name\": \"Linear (Coder)\", \"holmes_name\": \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-linear-29\", \"harness_name\": \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-linear-29\"},\n",
    "    {\"short_name\": \"Task Arithmetic (Coder)\", \"holmes_name\": \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-task_arithmetic-29\", \"harness_name\": \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-task_arithmetic-29\"},\n",
    "    {\"short_name\": \"DARE Ties (Coder)\", \"holmes_name\": \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-dare_ties-29\", \"harness_name\": \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-dare_ties-29\"},\n",
    "    {\"short_name\": \"Ties (Coder)\", \"holmes_name\": \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-ties-29\", \"harness_name\": \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-ties-29\"},\n",
    "    {\"short_name\": \"Slerp (Coder)\", \"holmes_name\": \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-slerp-29\", \"harness_name\": \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-slerp-29\"},\n",
    "    # Math Merged Models\n",
    "    {\"short_name\": \"Linear (Math)\", \"holmes_name\": \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-linear-24\", \"harness_name\": \"Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-linear-24\"},\n",
    "    {\"short_name\": \"Slerp (Math)\", \"holmes_name\": \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-slerp-24\", \"harness_name\": \"Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-slerp-24\"},\n",
    "    {\"short_name\": \"Task Arithmetic (Math)\", \"holmes_name\": \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-task_arithmetic-26\", \"harness_name\": \"Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-task_arithmetic-26\"},\n",
    "    {\"short_name\": \"Ties (Math)\", \"holmes_name\": \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-ties-25\", \"harness_name\": \"Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-ties-25\"},\n",
    "    {\"short_name\": \"DARE Ties (Math)\", \"holmes_name\": \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-dare_ties-27\", \"harness_name\": \"Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-dare_ties-27\"},\n",
    "]\n",
    "\n",
    "# --- Generate lists and mappings from the new config ---\n",
    "holmes_model_list = [m['holmes_name'] for m in MODEL_CONFIG if 'holmes_name' in m]\n",
    "harness_model_list = [m['harness_name'] for m in MODEL_CONFIG if 'harness_name' in m]\n",
    "\n",
    "MASTER_SHORT_NAMES = {}\n",
    "for model in MODEL_CONFIG:\n",
    "    if 'holmes_name' in model:\n",
    "        MASTER_SHORT_NAMES[model['holmes_name']] = model['short_name']\n",
    "    if 'harness_name' in model:\n",
    "        MASTER_SHORT_NAMES[model['harness_name']] = model['short_name']\n",
    "\n",
    "COMPARABLE_MODEL_SHORT_NAMES = sorted(list(set(m['short_name'] for m in MODEL_CONFIG)))\n",
    "\n",
    "# MODIFICATION: Define the specific Harness leaderboard groups to analyze\n",
    "TARGET_LEADERBOARD_GROUPS = ['mmlu_pro', 'bbh', 'gpqa', 'math_hard', 'ifeval', 'musr']\n",
    "HARNESS_PATHS = {m: {f\"leaderboard_{t}\": f\"organized_results/leaderboard/{m}/result.json\" for t in TARGET_LEADERBOARD_GROUPS} for m in harness_model_list}\n",
    "\n",
    "\n",
    "font_config = {\n",
    "    \"title_font_size\": 35,\n",
    "    \"font_size\": 20,\n",
    "    \"xaxis_title_font_size\": 32,\n",
    "    \"yaxis_title_font_size\": 32,\n",
    "    \"xaxis_tickfont_size\": 26,\n",
    "    \"yaxis_tickfont_size\": 26,\n",
    "    \"legend_title_font_size\": 18,\n",
    "    \"legend_font_size\": 16,\n",
    "}\n",
    "\n",
    "default_plot_height = 800\n",
    "heatmap_color_scale = 'RdBu_r'\n",
    "OUTPUT_DIR = \"rankings_output\"\n",
    "\n",
    "# --- Helper function for cleaning model names for plots ---\n",
    "def clean_plot_name(name):\n",
    "    if isinstance(name, str) and \"Merged_\" in name:\n",
    "        return re.sub(r'_\\d+$', '', name)\n",
    "    return name\n",
    "\n",
    "def clean_axis_label(label):\n",
    "    label = label.replace(\"HolmesM_\", \"H-Main: \")\n",
    "    label = label.replace(\"HarnessM_\", \"Ha-Main: \")\n",
    "    label = label.replace(\"HarnessS_\", \"Ha-Sub: \")\n",
    "    label = re.sub(r'Ha-Sub: ([^_]+)_', r'Ha-Sub: \\1 - ', label)\n",
    "    return label\n",
    "\n",
    "\n",
    "# --- 2. Holmes Data Section ---\n",
    "HOLMES_ABS_DATA_FILE = \"results_flash-holmes.csv\"\n",
    "HOLMES_GROUP_DATA_FILE = \"transformed_results_2.csv\"\n",
    "HOLMES_MAIN_TASK_COL_FROM_GROUP_FILE = \"probing dataset\"\n",
    "HOLMES_SUBTASK_COL_FROM_GROUP_FILE = \"probe\"\n",
    "HOLMES_LINGUISTIC_COMPETENCY_COL = \"linguistic competencies\"\n",
    "HOLMES_SUBTASK_PHENOMENA_COL = \"linguistic phenomena\" # Retained for potential future use\n",
    "\n",
    "def process_frame_holmes(frame):\n",
    "    if \"Unnamed: 0\" in frame.columns:\n",
    "        del frame[\"Unnamed: 0\"]\n",
    "    if \"linguistic subfield\" in frame.columns and HOLMES_LINGUISTIC_COMPETENCY_COL not in frame.columns:\n",
    "        frame[HOLMES_LINGUISTIC_COMPETENCY_COL] = frame[\"linguistic subfield\"]\n",
    "    return frame\n",
    "\n",
    "def load_data_holmes(abs_filepath, group_filepath, model_list_in_file,\n",
    "                     main_task_col_in_group_file, sub_task_col_in_group_file,\n",
    "                     linguistic_competency_col_in_group_file, phenomena_col_in_group_file): # phenomena_col retained\n",
    "    try:\n",
    "        raw_abs_df = pd.read_csv(abs_filepath)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Holmes absolute scores file not found at {abs_filepath}\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    if \"Unnamed: 0\" in raw_abs_df.columns: del raw_abs_df[\"Unnamed: 0\"]\n",
    "    raw_abs_df.rename(columns={'probing_dataset': 'holmes_subtask_id', 'model_name': 'model'}, inplace=True)\n",
    "\n",
    "    if 'encoding' in raw_abs_df.columns:\n",
    "        raw_abs_df = raw_abs_df[raw_abs_df['encoding'] == 'full'].copy()\n",
    "    \n",
    "    raw_abs_df['score'] = pd.to_numeric(raw_abs_df['score'], errors='coerce')\n",
    "    abs_df_grouped = raw_abs_df.groupby(['holmes_subtask_id', 'model'])['score'].mean().reset_index()\n",
    "    abs_pivot_df = abs_df_grouped.pivot_table(index='holmes_subtask_id', columns='model', values='score').reset_index()\n",
    "\n",
    "    for model_col in model_list_in_file: \n",
    "        if model_col not in abs_pivot_df.columns:\n",
    "            abs_pivot_df[model_col] = np.nan\n",
    "    \n",
    "    cols_to_keep_abs = ['holmes_subtask_id'] + [m for m in model_list_in_file if m in abs_pivot_df.columns]\n",
    "    abs_scores_per_subtask_df = abs_pivot_df[cols_to_keep_abs].copy()\n",
    "\n",
    "    try:\n",
    "        raw_group_df = pd.read_csv(group_filepath)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Holmes group info file not found at {group_filepath}.\")\n",
    "        for model_col in model_list_in_file:\n",
    "            if model_col in abs_scores_per_subtask_df.columns: abs_scores_per_subtask_df[model_col] *= 100\n",
    "        return pd.DataFrame(columns=model_list_in_file), abs_scores_per_subtask_df \n",
    "\n",
    "    group_df_processed = process_frame_holmes(raw_group_df.copy())\n",
    "    rename_map_group = {\n",
    "        sub_task_col_in_group_file: 'holmes_subtask_id',\n",
    "        linguistic_competency_col_in_group_file: 'holmes_linguistic_competency',\n",
    "        main_task_col_in_group_file: 'holmes_main_task_category'\n",
    "    }\n",
    "    if phenomena_col_in_group_file not in rename_map_group:\n",
    "        rename_map_group[phenomena_col_in_group_file] = 'holmes_linguistic_phenomena'\n",
    "\n",
    "\n",
    "    group_df_processed.rename(columns=rename_map_group, inplace=True)\n",
    "\n",
    "    id_cols_group = ['holmes_main_task_category', 'holmes_subtask_id', 'holmes_linguistic_competency']\n",
    "    if 'holmes_linguistic_phenomena' in group_df_processed.columns:\n",
    "         id_cols_group.append('holmes_linguistic_phenomena')\n",
    "    \n",
    "    if 'probe type' in group_df_processed.columns: id_cols_group.append('probe type')\n",
    "    \n",
    "    id_cols_group_present = [col for col in id_cols_group if col in group_df_processed.columns]\n",
    "\n",
    "    if 'holmes_subtask_id' not in group_df_processed.columns:\n",
    "        print(f\"Critical Error: Subtask ID column ('holmes_subtask_id' from '{sub_task_col_in_group_file}') not found in Holmes group file.\")\n",
    "        for model_col in model_list_in_file:\n",
    "            if model_col in abs_scores_per_subtask_df.columns: abs_scores_per_subtask_df[model_col] *= 100\n",
    "        return pd.DataFrame(columns=model_list_in_file), abs_scores_per_subtask_df\n",
    "\n",
    "    group_info_to_merge = group_df_processed[id_cols_group_present].drop_duplicates(subset=['holmes_subtask_id'])\n",
    "    subtasks_df_with_groups = pd.merge(abs_scores_per_subtask_df, group_info_to_merge, on='holmes_subtask_id', how='left')\n",
    "\n",
    "    models_in_merged_df = [m for m in model_list_in_file if m in subtasks_df_with_groups.columns]\n",
    "    for model_col in models_in_merged_df:\n",
    "        subtasks_df_with_groups[model_col] = pd.to_numeric(subtasks_df_with_groups[model_col], errors='coerce') * 100\n",
    "\n",
    "    summary_df = pd.DataFrame()\n",
    "    if 'holmes_main_task_category' in subtasks_df_with_groups.columns and models_in_merged_df:\n",
    "        summary_df = subtasks_df_with_groups.groupby('holmes_main_task_category')[models_in_merged_df].mean()\n",
    "    else:\n",
    "        summary_df = pd.DataFrame(columns=models_in_merged_df)\n",
    "        print(\"Warning: 'holmes_main_task_category' not found or no models. Holmes summary will be empty.\")\n",
    "        \n",
    "    if not summary_df.empty:\n",
    "        summary_df.index = summary_df.index.fillna('Unknown_Holmes_MainTask').astype(str)\n",
    "        summary_df = summary_df[~summary_df.index.str.lower().isin(['nan', 'none', ''])]\n",
    "\n",
    "    if 'holmes_linguistic_competency' in subtasks_df_with_groups.columns:\n",
    "        subtasks_df_with_groups['holmes_linguistic_competency'] = subtasks_df_with_groups['holmes_linguistic_competency'].fillna('Unknown_Ling_Comp').astype(str)\n",
    "        subtasks_df_with_groups = subtasks_df_with_groups[~subtasks_df_with_groups['holmes_linguistic_competency'].str.lower().isin(['nan', 'none', ''])]\n",
    "    else:\n",
    "        print(\"Warning: 'holmes_linguistic_competency' column not found in Holmes subtask data.\")\n",
    "        subtasks_df_with_groups['holmes_linguistic_competency'] = 'Unknown_Ling_Comp'\n",
    "\n",
    "    return summary_df, subtasks_df_with_groups\n",
    "\n",
    "# --- 3. Harness Data Section ---\n",
    "\n",
    "def load_summary_harness(paths_dict, model_list_in_file, tasks_list):\n",
    "    df = pd.DataFrame(index=tasks_list, columns=model_list_in_file, dtype=float)\n",
    "    for m in model_list_in_file:\n",
    "        fp = f\"organized_results/leaderboard/{m}/result.json\"\n",
    "        if not os.path.isfile(fp):\n",
    "            for t in tasks_list:\n",
    "                df.at[t, m] = np.nan\n",
    "            print(f\"Warning: Harness file not found {fp}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            with open(fp, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            for t in tasks_list:\n",
    "                results_for_task = data.get('results', {}).get(f'leaderboard_{t}', {})\n",
    "                val = results_for_task.get('acc,none', results_for_task.get('acc_norm,none', np.nan))\n",
    "                df.at[t, m] = val * 100 if val is not None and not pd.isna(val) else np.nan\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Harness file {fp}: {e}\")\n",
    "            for t in tasks_list:\n",
    "                df.at[t, m] = np.nan\n",
    "                \n",
    "    return df.dropna(how='all', axis=1).dropna(how='all', axis=0)\n",
    "\n",
    "def load_leaderboard_with_groups_harness(model_list_in_file, target_groups):\n",
    "    agg = defaultdict(dict)\n",
    "    inv_group = {}\n",
    "    \n",
    "    leaderboard_paths = {m: f\"organized_results/leaderboard/{m}/result.json\" for m in model_list_in_file}\n",
    "    leaderboard_paths = {m: p for m, p in leaderboard_paths.items() if p}\n",
    "\n",
    "    if not any(os.path.isfile(fp) for fp in leaderboard_paths.values() if fp):\n",
    "        return pd.DataFrame(columns=['subtask_cleaned', 'harness_group'] + model_list_in_file)\n",
    "\n",
    "    first_valid_file_checked_for_groups = False\n",
    "    for m, fp in leaderboard_paths.items():\n",
    "        if not fp or not os.path.isfile(fp): continue\n",
    "        try:\n",
    "            with open(fp, 'r') as f: data = json.load(f)\n",
    "            if 'group_subtasks' in data and not inv_group and not first_valid_file_checked_for_groups:\n",
    "                for grp, subs in data['group_subtasks'].items():\n",
    "                    clean_grp_name = grp.replace('leaderboard_', '') if isinstance(grp, str) else str(grp)\n",
    "                    for sub in subs: \n",
    "                        clean_sub_name = sub.replace('leaderboard_', '') if isinstance(sub, str) else str(sub)\n",
    "                        inv_group[clean_sub_name] = clean_grp_name \n",
    "                first_valid_file_checked_for_groups = True\n",
    "            \n",
    "            for key, metrics in data.get('results', {}).items():\n",
    "                if isinstance(key, str) and key.startswith('leaderboard_') and key != 'leaderboard': \n",
    "                    subtask_name_cleaned = key.replace('leaderboard_', '')\n",
    "                    score = metrics.get('acc_norm,none', metrics.get('acc,none', metrics.get('exact_match,none', np.nan)))\n",
    "                    if pd.isna(score) and 'exact_match,strict-match' in metrics:\n",
    "                        score = metrics.get('exact_match,strict-match', np.nan)\n",
    "\n",
    "                    if not pd.isna(score): agg[subtask_name_cleaned][m] = score * 100\n",
    "        except Exception as e: print(f\"Error processing Harness file {fp} for model {m}: {e}\")\n",
    "\n",
    "    if not agg: return pd.DataFrame(columns=['subtask_cleaned', 'harness_group'] + model_list_in_file)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(agg, orient='index')\n",
    "    for m_col in model_list_in_file: \n",
    "        if m_col not in df.columns: df[m_col] = np.nan\n",
    "    \n",
    "    present_models_in_agg = [m for m in model_list_in_file if m in df.columns]\n",
    "    df = df[present_models_in_agg].copy() \n",
    "    df = df.dropna(subset=present_models_in_agg, how='all')\n",
    "\n",
    "    if df.empty: return pd.DataFrame(columns=['subtask_cleaned', 'harness_group'] + model_list_in_file)\n",
    "\n",
    "    df['harness_group'] = df.index.map(lambda x: inv_group.get(x, 'Unknown_Harness_Group')) \n",
    "    \n",
    "    # --- FIX: Manually correct the group for tasks that are their own group (like mmlu_pro) ---\n",
    "    for group_name in target_groups:\n",
    "        if group_name in df.index:\n",
    "             df.loc[group_name, 'harness_group'] = group_name\n",
    "    \n",
    "    # Filter to keep only subtasks from the target groups\n",
    "    df = df[df['harness_group'].isin(target_groups)].copy()\n",
    "    \n",
    "    df.index.name = 'subtask_cleaned' \n",
    "    df.reset_index(inplace=True) \n",
    "    \n",
    "    final_cols = ['subtask_cleaned', 'harness_group'] + present_models_in_agg \n",
    "    for col in ['harness_group']: \n",
    "        if col not in df.columns:\n",
    "            df[col] = 'Unknown_Harness_Group'\n",
    "            \n",
    "    return df[final_cols]\n",
    "\n",
    "\n",
    "# --- 4. Generic Ranking Generation ---\n",
    "def generate_rankings_generic(score_data, model_names_in_data_cols, short_names_map,\n",
    "                              task_id_col=None, group_col_for_index=None):\n",
    "    rankings_list = []\n",
    "    models_to_rank = [m for m in model_names_in_data_cols if m in score_data.columns]\n",
    "    if not models_to_rank:\n",
    "        print(\"No models to rank in generate_rankings_generic.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if task_id_col: \n",
    "        if task_id_col not in score_data.columns:\n",
    "            print(f\"Error: task_id_col '{task_id_col}' not found in score_data for ranking.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        if group_col_for_index and group_col_for_index in score_data.columns:\n",
    "            iterator = score_data.groupby([group_col_for_index, task_id_col], observed=False)\n",
    "        else:\n",
    "            if group_col_for_index: \n",
    "                print(f\"Warning: group_col_for_index '{group_col_for_index}' not found. Ranking by task_id_col '{task_id_col}' only.\")\n",
    "            score_data['_dummy_grouper_for_rank'] = 0\n",
    "            iterator = score_data.groupby(['_dummy_grouper_for_rank', task_id_col], observed=False)\n",
    "\n",
    "\n",
    "        for name_tuple, group_df_iter in iterator:\n",
    "            if group_col_for_index and group_col_for_index in score_data.columns:\n",
    "                current_group_val = name_tuple[0]\n",
    "                current_task_id_val = name_tuple[1]\n",
    "            else:\n",
    "                current_group_val = None\n",
    "                current_task_id_val = name_tuple[1]\n",
    "\n",
    "\n",
    "            if not group_df_iter[models_to_rank].empty:\n",
    "                scores_series = group_df_iter[models_to_rank].mean(numeric_only=True) \n",
    "            else:\n",
    "                continue \n",
    "\n",
    "            ranked_models_full = scores_series.sort_values(ascending=False, na_position='last').index.tolist()\n",
    "            ranked_short_names = [short_names_map.get(m, m) for m in ranked_models_full]\n",
    "            \n",
    "            row = {}\n",
    "            if group_col_for_index and current_group_val is not None and group_col_for_index in score_data.columns : \n",
    "                row[group_col_for_index] = current_group_val\n",
    "            row[task_id_col] = current_task_id_val\n",
    "            \n",
    "            for i, name in enumerate(ranked_short_names):\n",
    "                row[f'Rank {i+1}'] = name\n",
    "            rankings_list.append(row)\n",
    "        \n",
    "        if '_dummy_grouper_for_rank' in score_data.columns:\n",
    "            del score_data['_dummy_grouper_for_rank']\n",
    "            \n",
    "    else: \n",
    "        if score_data.empty:\n",
    "            print(\"Score data is empty for non-task_id_col ranking.\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        for task_name_idx, row_data in score_data.iterrows():\n",
    "            scores = row_data[models_to_rank].astype(float)\n",
    "            ranked_models_full = scores.sort_values(ascending=False, na_position='last').index.tolist()\n",
    "            ranked_short_names = [short_names_map.get(m, m) for m in ranked_models_full]\n",
    "            row = {'Task': task_name_idx} \n",
    "            for i, name in enumerate(ranked_short_names):\n",
    "                row[f'Rank {i+1}'] = name\n",
    "            rankings_list.append(row)\n",
    "\n",
    "    if not rankings_list: return pd.DataFrame()\n",
    "    rankings_df = pd.DataFrame(rankings_list)\n",
    "    \n",
    "    index_cols = []\n",
    "    if group_col_for_index and group_col_for_index in rankings_df.columns: index_cols.append(group_col_for_index)\n",
    "    if task_id_col and task_id_col in rankings_df.columns: index_cols.append(task_id_col)\n",
    "    elif 'Task' in rankings_df.columns: index_cols.append('Task')\n",
    "    \n",
    "    if index_cols:\n",
    "        try:\n",
    "            rankings_df.dropna(subset=index_cols, how='all', inplace=True)\n",
    "            if not rankings_df.empty:\n",
    "                 rankings_df.set_index(index_cols, inplace=True)\n",
    "            else: \n",
    "                print(f\"Warning: Rankings_df became empty after dropping NaNs in index columns: {index_cols}\")\n",
    "        except KeyError as e:\n",
    "            print(f\"Warning: Could not set index on {index_cols} for rankings_df. Columns: {rankings_df.columns}. Error: {e}\")\n",
    "    return rankings_df\n",
    "\n",
    "\n",
    "# --- 5. Kendall's Tau Calculation using Pandas ---\n",
    "def calculate_kendall_tau_with_pandas(rank_series1, rank_series2, all_possible_models):\n",
    "    if not rank_series1 or not rank_series2:\n",
    "        return np.nan\n",
    "\n",
    "    s1_numeric = pd.Series(index=all_possible_models, dtype=float)\n",
    "    for i, model_name in enumerate(rank_series1):\n",
    "        if model_name in s1_numeric.index:\n",
    "            s1_numeric[model_name] = i \n",
    "\n",
    "    s2_numeric = pd.Series(index=all_possible_models, dtype=float)\n",
    "    for i, model_name in enumerate(rank_series2):\n",
    "        if model_name in s2_numeric.index:\n",
    "            s2_numeric[model_name] = i\n",
    "            \n",
    "    rank_df = pd.DataFrame({'rank1': s1_numeric, 'rank2': s2_numeric})\n",
    "    rank_df.dropna(inplace=True) \n",
    "\n",
    "    if len(rank_df) < 2: \n",
    "        return np.nan\n",
    "\n",
    "    corr_matrix = rank_df.corr(method='kendall')\n",
    "    \n",
    "    if corr_matrix.shape == (2,2) and not pd.isna(corr_matrix.iloc[0,1]):\n",
    "        return corr_matrix.iloc[0,1]\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# --- 6. Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "    # --- Load Holmes Data ---\n",
    "    print(\"--- Loading Holmes Data ---\")\n",
    "    summary_holmes_df, subtasks_holmes_df = load_data_holmes(\n",
    "        HOLMES_ABS_DATA_FILE, HOLMES_GROUP_DATA_FILE, holmes_model_list,\n",
    "        HOLMES_MAIN_TASK_COL_FROM_GROUP_FILE, HOLMES_SUBTASK_COL_FROM_GROUP_FILE, \n",
    "        HOLMES_LINGUISTIC_COMPETENCY_COL, HOLMES_SUBTASK_PHENOMENA_COL\n",
    "    )\n",
    "    print(f\"Holmes Summary (Main Tasks) DF loaded: {summary_holmes_df.shape}\")\n",
    "    print(f\"Holmes Subtasks (with linguistic competencies) DF loaded: {subtasks_holmes_df.shape}\")\n",
    "\n",
    "\n",
    "    # --- Load Harness Data ---\n",
    "    print(\"\\n--- Loading Harness Data (Filtered to Target Leaderboard Groups) ---\")\n",
    "    summary_harness_df = load_summary_harness(HARNESS_PATHS, harness_model_list, TARGET_LEADERBOARD_GROUPS)\n",
    "    subtasks_harness_df = load_leaderboard_with_groups_harness(harness_model_list, TARGET_LEADERBOARD_GROUPS)\n",
    "    print(f\"Harness Summary DF loaded: {summary_harness_df.shape}\")\n",
    "    if not summary_harness_df.empty:\n",
    "        print(\"Harness Summary DF head:\\n\", summary_harness_df.head())\n",
    "    print(f\"Harness Subtasks DF loaded: {subtasks_harness_df.shape}\")\n",
    "    if not subtasks_harness_df.empty:\n",
    "        print(\"Harness Subtasks DF head:\\n\", subtasks_harness_df.head())\n",
    "\n",
    "\n",
    "    # --- Generate Holmes Rankings ---\n",
    "    print(\"\\n--- Generating Holmes Rankings ---\")\n",
    "    holmes_summary_model_cols = [m for m in holmes_model_list if m in summary_holmes_df.columns]\n",
    "    holmes_main_ranks_df = generate_rankings_generic(summary_holmes_df, holmes_summary_model_cols, MASTER_SHORT_NAMES)\n",
    "    \n",
    "    holmes_subtasks_model_cols = [m for m in holmes_model_list if m in subtasks_holmes_df.columns]\n",
    "    holmes_subtask_ranks_df = generate_rankings_generic(subtasks_holmes_df, holmes_subtasks_model_cols, MASTER_SHORT_NAMES,\n",
    "                                                       task_id_col='holmes_subtask_id', group_col_for_index='holmes_linguistic_competency')\n",
    "    \n",
    "    if not holmes_main_ranks_df.empty:\n",
    "        holmes_main_ranks_df.to_csv(os.path.join(OUTPUT_DIR, \"holmes_main_task_rankings.csv\"), index=True)\n",
    "        print(f\"Saved Holmes main task rankings.\")\n",
    "    if not holmes_subtask_ranks_df.empty:\n",
    "        holmes_subtask_ranks_df.to_csv(os.path.join(OUTPUT_DIR, \"holmes_subtask_rankings_by_ling_competency.csv\"), index=True)\n",
    "        print(f\"Saved Holmes subtask rankings by linguistic competency.\")\n",
    "\n",
    "\n",
    "    # --- Generate Harness Rankings ---\n",
    "    print(\"\\n--- Generating Harness Rankings ---\")\n",
    "    harness_summary_model_cols = [m for m in harness_model_list if m in summary_harness_df.columns]\n",
    "    harness_subtasks_model_cols = [m for m in harness_model_list if m in subtasks_harness_df.columns]\n",
    "    \n",
    "    harness_main_ranks_df = generate_rankings_generic(summary_harness_df, harness_summary_model_cols, MASTER_SHORT_NAMES)\n",
    "    harness_subtask_ranks_df = generate_rankings_generic(subtasks_harness_df, harness_subtasks_model_cols, MASTER_SHORT_NAMES,\n",
    "                                                         task_id_col='subtask_cleaned', group_col_for_index='harness_group')\n",
    "\n",
    "    if not harness_main_ranks_df.empty:\n",
    "        harness_main_ranks_df.to_csv(os.path.join(OUTPUT_DIR, \"harness_main_rankings.csv\"), index=True)\n",
    "        print(f\"Saved Harness main rankings.\")\n",
    "    if not harness_subtask_ranks_df.empty:\n",
    "        harness_subtask_ranks_df.to_csv(os.path.join(OUTPUT_DIR, \"harness_subtask_rankings_by_group.csv\"), index=True)\n",
    "        print(f\"Saved Harness subtask rankings by group.\")\n",
    "\n",
    "    # --- Prepare Task/Subtask Lists for \"Huge\" Correlation Matrix ---\n",
    "    holmes_main_task_keys_labels_for_huge_corr = []\n",
    "    if not holmes_main_ranks_df.empty:\n",
    "        for idx in holmes_main_ranks_df.index.tolist():\n",
    "            holmes_main_task_keys_labels_for_huge_corr.append((f\"HolmesM_{idx}\", clean_axis_label(f\"HolmesM_{idx}\")))\n",
    "    \n",
    "    harness_all_task_keys_labels_for_huge_corr = []\n",
    "    if not harness_main_ranks_df.empty:\n",
    "        for idx in harness_main_ranks_df.index.tolist():\n",
    "            harness_all_task_keys_labels_for_huge_corr.append((f\"HarnessM_{idx}\", clean_axis_label(f\"HarnessM_{idx}\")))\n",
    "    if not harness_subtask_ranks_df.empty and isinstance(harness_subtask_ranks_df.index, pd.MultiIndex):\n",
    "        for group_idx, subtask_idx in harness_subtask_ranks_df.index: \n",
    "            internal_key = f\"HarnessS_{group_idx}_{subtask_idx}\"\n",
    "            harness_all_task_keys_labels_for_huge_corr.append((internal_key, clean_axis_label(internal_key)))\n",
    "    elif not harness_subtask_ranks_df.empty:\n",
    "         for idx in harness_subtask_ranks_df.index:\n",
    "            internal_key = f\"HarnessS_{idx}\"\n",
    "            harness_all_task_keys_labels_for_huge_corr.append((internal_key, clean_axis_label(internal_key)))\n",
    "\n",
    "    \n",
    "    # --- Huge Correlation Map (Holmes Main Tasks vs Harness All Tasks/Subtasks) ---\n",
    "    print(\"\\n--- Calculating Huge Correlation Matrix (Holmes Main Tasks vs Harness All Tasks/Subtasks) ---\")\n",
    "    if holmes_main_task_keys_labels_for_huge_corr and harness_all_task_keys_labels_for_huge_corr:\n",
    "        holmes_labels_for_index = [item[1] for item in holmes_main_task_keys_labels_for_huge_corr]\n",
    "        harness_labels_for_columns = [item[1] for item in harness_all_task_keys_labels_for_huge_corr]\n",
    "        \n",
    "        huge_corr_matrix = pd.DataFrame(index=holmes_labels_for_index, columns=harness_labels_for_columns, dtype=float)\n",
    "\n",
    "        for holmes_key, holmes_display_label in holmes_main_task_keys_labels_for_huge_corr:\n",
    "            task_idx_holmes = holmes_key.replace(\"HolmesM_\", \"\")\n",
    "            if task_idx_holmes not in holmes_main_ranks_df.index: continue\n",
    "            holmes_rank_series = holmes_main_ranks_df.loc[task_idx_holmes].dropna().tolist()\n",
    "            if not holmes_rank_series: continue\n",
    "\n",
    "            for harness_key, harness_display_label in harness_all_task_keys_labels_for_huge_corr:\n",
    "                harness_rank_series = None\n",
    "                if harness_key.startswith(\"HarnessM_\"):\n",
    "                    task_idx_harness = harness_key.replace(\"HarnessM_\", \"\")\n",
    "                    if task_idx_harness in harness_main_ranks_df.index:\n",
    "                        harness_rank_series = harness_main_ranks_df.loc[task_idx_harness].dropna().tolist()\n",
    "                elif harness_key.startswith(\"HarnessS_\"):\n",
    "                    parts = harness_key.replace(\"HarnessS_\", \"\").split('_', 1)\n",
    "                    if len(parts) == 2:\n",
    "                        group_name, subtask_name = parts[0], parts[1]\n",
    "                        target_index_tuple = (group_name, subtask_name)\n",
    "                        if target_index_tuple in harness_subtask_ranks_df.index:\n",
    "                            harness_rank_series = harness_subtask_ranks_df.loc[target_index_tuple].dropna().tolist()\n",
    "                    elif harness_key.replace(\"HarnessS_\", \"\") in harness_subtask_ranks_df.index and not isinstance(harness_subtask_ranks_df.index, pd.MultiIndex):\n",
    "                        harness_rank_series = harness_subtask_ranks_df.loc[harness_key.replace(\"HarnessS_\", \"\")].dropna().tolist()\n",
    "\n",
    "                if not harness_rank_series: continue\n",
    "                \n",
    "                tau = calculate_kendall_tau_with_pandas(holmes_rank_series, harness_rank_series, COMPARABLE_MODEL_SHORT_NAMES)\n",
    "                huge_corr_matrix.at[holmes_display_label, harness_display_label] = tau\n",
    "        \n",
    "        huge_corr_matrix.dropna(how='all', axis=0, inplace=True)\n",
    "        huge_corr_matrix.dropna(how='all', axis=1, inplace=True)\n",
    "        if not huge_corr_matrix.empty:\n",
    "            huge_corr_csv_path = os.path.join(OUTPUT_DIR, \"correlation_holmes_main_vs_harness_all.csv\")\n",
    "            huge_corr_matrix.to_csv(huge_corr_csv_path, index=True)\n",
    "            print(f\"Saved huge correlation matrix to {huge_corr_csv_path}\")\n",
    "\n",
    "            fig_huge_corr = px.imshow(huge_corr_matrix.astype(float).sort_index(axis=0).sort_index(axis=1), text_auto=\".2f\", aspect=\"auto\",\n",
    "                                      color_continuous_scale=heatmap_color_scale, range_color=[-1,1],\n",
    "                                      title=\"Kendall's Tau: Holmes Main Tasks vs. Harness Tasks/Subtasks\")\n",
    "            fig_huge_corr.update_xaxes(tickangle=45, automargin=True)\n",
    "            fig_huge_corr.update_yaxes(automargin=True)\n",
    "            fig_huge_corr.update_layout(height=max(800, 20 * len(huge_corr_matrix.index)), \n",
    "                                        width=max(1000, 20 * len(huge_corr_matrix.columns)),\n",
    "                                        **font_config)\n",
    "            fig_huge_corr.show()\n",
    "        else: print(\"Huge correlation matrix (Holmes Main vs Harness All) is empty after NaN drop.\")\n",
    "    else: print(\"Not enough task names for huge correlation matrix (Holmes Main vs Harness All).\")\n",
    "\n",
    "\n",
    "    # --- Holmes Linguistic Competencies vs Harness Leaderboard Groups Correlation Map (Group-Aggregated Taus) ---\n",
    "    print(\"\\n--- Calculating Holmes Linguistic Competencies vs Harness Leaderboard Groups Correlation Matrix (Group-Aggregated Taus) ---\")\n",
    "    \n",
    "    if not holmes_subtask_ranks_df.empty and \\\n",
    "       isinstance(holmes_subtask_ranks_df.index, pd.MultiIndex) and \\\n",
    "       'holmes_linguistic_competency' in holmes_subtask_ranks_df.index.names and \\\n",
    "       'holmes_subtask_id' in holmes_subtask_ranks_df.index.names and \\\n",
    "       not harness_subtask_ranks_df.empty and \\\n",
    "       isinstance(harness_subtask_ranks_df.index, pd.MultiIndex) and \\\n",
    "       'harness_group' in harness_subtask_ranks_df.index.names and \\\n",
    "       'subtask_cleaned' in harness_subtask_ranks_df.index.names:\n",
    "\n",
    "        unique_holmes_competencies = sorted([\n",
    "            str(c) for c in holmes_subtask_ranks_df.index.get_level_values('holmes_linguistic_competency').unique()\n",
    "            if str(c) not in ['Unknown_Ling_Comp', 'nan', 'none', '']\n",
    "        ])\n",
    "        \n",
    "        unique_harness_groups = sorted([\n",
    "            str(g) for g in harness_subtask_ranks_df.index.get_level_values('harness_group').unique()\n",
    "            if str(g) not in ['Unknown_Harness_Group', 'nan', 'none', '']\n",
    "        ])\n",
    "\n",
    "        if not unique_holmes_competencies or not unique_harness_groups:\n",
    "            print(\"No valid Holmes competencies or Harness groups found for group-aggregated correlation.\")\n",
    "        else:\n",
    "            group_vs_group_corr_matrix = pd.DataFrame(index=unique_holmes_competencies, columns=unique_harness_groups, dtype=float)\n",
    "\n",
    "            for competency in unique_holmes_competencies:\n",
    "                try:\n",
    "                    holmes_ranks_for_competency_df = holmes_subtask_ranks_df.xs(competency, level='holmes_linguistic_competency')\n",
    "                    holmes_competency_subtask_rankings_list = []\n",
    "                    for _, row_series in holmes_ranks_for_competency_df.iterrows():\n",
    "                        ranks = row_series.dropna().tolist()\n",
    "                        if ranks:\n",
    "                            holmes_competency_subtask_rankings_list.append(ranks)\n",
    "                    if not holmes_competency_subtask_rankings_list:\n",
    "                        continue\n",
    "                except KeyError:\n",
    "                    print(f\"KeyError: Holmes competency '{competency}' not found in holmes_subtask_ranks_df index.\")\n",
    "                    continue\n",
    "                \n",
    "                for harness_grp_name in unique_harness_groups:\n",
    "                    try:\n",
    "                        harness_ranks_for_group_df = harness_subtask_ranks_df.xs(harness_grp_name, level='harness_group')\n",
    "                        harness_group_subtask_rankings_list = []\n",
    "                        for _, row_series in harness_ranks_for_group_df.iterrows():\n",
    "                            ranks = row_series.dropna().tolist()\n",
    "                            if ranks:\n",
    "                                harness_group_subtask_rankings_list.append(ranks)\n",
    "                        if not harness_group_subtask_rankings_list:\n",
    "                            continue\n",
    "                    except KeyError:\n",
    "                        print(f\"KeyError: Harness group '{harness_grp_name}' not found in harness_subtask_ranks_df index.\")\n",
    "                        continue\n",
    "\n",
    "                    current_pair_taus = []\n",
    "                    for holmes_s_rank in holmes_competency_subtask_rankings_list:\n",
    "                        for harness_s_rank in harness_group_subtask_rankings_list:\n",
    "                            tau = calculate_kendall_tau_with_pandas(holmes_s_rank, harness_s_rank, COMPARABLE_MODEL_SHORT_NAMES)\n",
    "                            if not pd.isna(tau):\n",
    "                                current_pair_taus.append(tau)\n",
    "                    \n",
    "                    if current_pair_taus:\n",
    "                        group_vs_group_corr_matrix.at[competency, harness_grp_name] = np.mean(current_pair_taus)\n",
    "            \n",
    "            group_vs_group_corr_matrix.dropna(how='all', axis=0, inplace=True) \n",
    "            group_vs_group_corr_matrix.dropna(how='all', axis=1, inplace=True)  \n",
    "            \n",
    "            if not group_vs_group_corr_matrix.empty:\n",
    "                group_corr_csv_path = os.path.join(OUTPUT_DIR, \"correlation_holmes_ling_competencies_vs_harness_groups.csv\")\n",
    "                group_vs_group_corr_matrix.to_csv(group_corr_csv_path, index=True)\n",
    "                print(f\"Saved group-aggregated correlation matrix to {group_corr_csv_path}\")\n",
    "\n",
    "                fig_group_corr = px.imshow(\n",
    "                    group_vs_group_corr_matrix.astype(float).sort_index(axis=0).sort_index(axis=1), \n",
    "                    text_auto=\".2f\", aspect=\"auto\",\n",
    "                    color_continuous_scale=heatmap_color_scale, range_color=[-1,1],\n",
    "                    title=\"Kendall's Tau: Holmes Ling. Competencies vs. Harness Leaderboard Groups<br>(Avg. of Subtask-Pair Taus)\"\n",
    "                )\n",
    "                fig_group_corr.update_xaxes(tickangle=45, automargin=True, title_text=\"Harness Leaderboard Groups\")\n",
    "                fig_group_corr.update_yaxes(automargin=True, title_text=\"Holmes Linguistic Competencies\")\n",
    "                fig_group_corr.update_layout(\n",
    "                    height=max(700, 25 * len(group_vs_group_corr_matrix.index)), \n",
    "                    width=max(1500, 25 * len(group_vs_group_corr_matrix.columns)),\n",
    "                    **font_config\n",
    "                )\n",
    "                fig_group_corr.show()\n",
    "            else: print(\"Holmes Linguistic Competencies vs Harness Leaderboard Groups correlation matrix is empty after NaN drop.\")\n",
    "    else:\n",
    "        print(\"Skipping Holmes Linguistic Competencies vs Harness Leaderboard Groups Correlation: Prerequisite data or correct DataFrame structure missing.\")\n",
    "        if holmes_subtask_ranks_df.empty: print(\"- Holmes subtask ranks DF is empty.\")\n",
    "        elif not isinstance(holmes_subtask_ranks_df.index, pd.MultiIndex): print(\"- Holmes subtask ranks DF is not MultiIndexed as expected.\")\n",
    "        if harness_subtask_ranks_df.empty: print(\"- Harness subtask ranks DF is empty.\")\n",
    "        elif not isinstance(harness_subtask_ranks_df.index, pd.MultiIndex): print(\"- Harness subtask ranks DF is not MultiIndexed as expected.\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Script Finished ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
