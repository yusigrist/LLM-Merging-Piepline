{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb2cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import re # Added for cleaning plot names\n",
    "\n",
    "# Set default plotly template for better aesthetics\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# --- Helper function for cleaning model names for plots ---\n",
    "def clean_plot_name(name):\n",
    "    if name is None: # Handle potential None input\n",
    "        return \"Unknown\"\n",
    "    name_str = str(name)\n",
    "    # For merged models, remove the trailing _XX\n",
    "    if not name_str.startswith(\"Qwen2.5\") or \"Merged_\" in name_str :\n",
    "        name_str = re.sub(r'_\\d+$', '', name_str)\n",
    "    return name_str\n",
    "\n",
    "# --- Font configuration for plots ---\n",
    "font_config = {\n",
    "    \"title_font_size\": 24,\n",
    "    \"font_size\": 18,\n",
    "    \"xaxis_title_font_size\": 18,\n",
    "    \"yaxis_title_font_size\": 18,\n",
    "    \"xaxis_tickfont_size\": 16,\n",
    "    \"yaxis_tickfont_size\": 16,\n",
    "    \"legend_title_font_size\": 24,\n",
    "    \"legend_font_size\": 22,\n",
    "}\n",
    "\n",
    "# --- Default Plot Dimensions ---\n",
    "default_plot_height = 400\n",
    "default_plot_width = 1400\n",
    "\n",
    "# --- Core Analysis and Plotting Function ---\n",
    "def run_analysis_for_experiment(experiment_name, models, short_names, tasks, paths):\n",
    "    \"\"\"\n",
    "    Runs the full data loading, analysis, and plotting pipeline for a given experiment.\n",
    "    Returns the processed comparison dataframe for later use in the combined plot.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\" PROCESSING EXPERIMENT: {experiment_name.upper()} \")\n",
    "    print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "    # --- 1. Model Categorization ---\n",
    "    instruct_model = None; coder_model = None; merged_models = []; base_model = None\n",
    "    for m_full_name in models:\n",
    "        m_short = short_names.get(m_full_name, \"\")\n",
    "        is_instruct = (m_short == \"Qwen2.5 Instruct\")\n",
    "        # Use .startswith for Coder/Math to handle variations\n",
    "        is_coder = (m_short.startswith(\"Qwen2.5 Coder\") or m_short.startswith(\"Qwen2.5 Math\"))\n",
    "        is_base = (m_short == \"Qwen2.5 Base\")\n",
    "        \n",
    "        is_merged = not (is_instruct or is_coder or is_base)\n",
    "        if is_instruct:\n",
    "            instruct_model = m_full_name\n",
    "        elif is_coder:\n",
    "            coder_model = m_full_name\n",
    "        elif is_base:\n",
    "            base_model = m_full_name\n",
    "        elif is_merged:\n",
    "            if m_full_name in models:\n",
    "                merged_models.append(m_full_name)\n",
    "\n",
    "    if not instruct_model: print(\"CRITICAL ERROR: Instruct model not identified.\"); exit()\n",
    "    if not coder_model: print(f\"CRITICAL ERROR: {experiment_name} model not identified.\"); exit()\n",
    "    if not merged_models: print(\"WARNING: No merged models identified.\")\n",
    "    if not base_model: print(\"WARNING: Base model not identified in setup.\")\n",
    "\n",
    "    print(f\"--- Model Categorization ({experiment_name}) ---\")\n",
    "    if base_model: print(f\"Base Model: {base_model} ({short_names.get(base_model, 'N/A')})\")\n",
    "    print(f\"Instruct Model: {instruct_model} ({short_names.get(instruct_model, 'N/A')})\")\n",
    "    print(f\"Specialist Model: {coder_model} ({short_names.get(coder_model, 'N/A')})\")\n",
    "    print(f\"Merged Models ({len(merged_models)}):\")\n",
    "    for m in merged_models: print(f\"  - {m} ({short_names.get(m, 'N/A')})\")\n",
    "    print(\"-\" * 25)\n",
    "\n",
    "    comparison_models_ordered = []\n",
    "    if base_model: comparison_models_ordered.append(base_model)\n",
    "    if instruct_model: comparison_models_ordered.append(instruct_model)\n",
    "    if coder_model: comparison_models_ordered.append(coder_model)\n",
    "    comparison_models_ordered.extend([m for m in merged_models if m])\n",
    "    comparison_models = list(dict.fromkeys(m for m in comparison_models_ordered if m))\n",
    "    print(f\"Models for comparison: {[clean_plot_name(short_names.get(m, m)) for m in comparison_models]}\")\n",
    "    print(\"-\" * 25)\n",
    "\n",
    "    # --- 2. Data Loading ---\n",
    "    def load_leaderboard_with_groups(paths_dict, model_list):\n",
    "        agg = defaultdict(dict); inv_group = {}\n",
    "        leaderboard_paths = {m: paths_dict.get(m, {}).get('leaderboard') for m in model_list}\n",
    "        leaderboard_paths = {m: p for m, p in leaderboard_paths.items() if p}\n",
    "        if not any(os.path.isfile(fp) for fp in leaderboard_paths.values() if fp):\n",
    "            return pd.DataFrame(columns=['subtask'] + model_list + ['group'])\n",
    "        first_valid_file_checked_for_groups = False\n",
    "        for m, fp in leaderboard_paths.items():\n",
    "            if not fp or not os.path.isfile(fp): continue\n",
    "            try:\n",
    "                with open(fp, 'r') as f: data = json.load(f)\n",
    "                if 'group_subtasks' in data and not inv_group and not first_valid_file_checked_for_groups:\n",
    "                    for grp, subs in data['group_subtasks'].items():\n",
    "                        clean_grp_name = grp.replace('leaderboard_', '') if isinstance(grp, str) else grp\n",
    "                        for sub in subs: inv_group[sub] = clean_grp_name\n",
    "                    first_valid_file_checked_for_groups = True\n",
    "                for key, metrics in data.get('results', {}).items():\n",
    "                    if isinstance(key, str) and key.startswith('leaderboard_') and key != 'leaderboard':\n",
    "                        score = metrics.get('acc_norm,none', metrics.get('acc,none', metrics.get('exact_match,none', np.nan)))\n",
    "                        if not pd.isna(score): agg[key][m] = score * 100\n",
    "            except Exception as e: print(f\"Error processing file {fp} for model {m}: {e}\")\n",
    "        if not agg: return pd.DataFrame(columns=['subtask'] + model_list + ['group'])\n",
    "        df = pd.DataFrame.from_dict(agg, orient='index')\n",
    "        for m_col in model_list:\n",
    "            if m_col not in df.columns: df[m_col] = np.nan\n",
    "        present_models_in_agg = [m for m in model_list if m in df.columns]; df = df[present_models_in_agg]\n",
    "        df = df.dropna(subset=present_models_in_agg, how='all')\n",
    "        if df.empty: return pd.DataFrame(columns=['subtask'] + model_list + ['group', 'subtask_cleaned'])\n",
    "        df['group'] = df.index.map(lambda x: inv_group.get(x, 'Unknown'))\n",
    "        df['subtask_cleaned'] = df.index.str.replace('leaderboard_', '', regex=False)\n",
    "        final_cols = ['group', 'subtask_cleaned'] + present_models_in_agg\n",
    "        return df.reset_index().rename(columns={'index': 'subtask'})[final_cols + ['subtask']]\n",
    "\n",
    "    subtasks_df_all = load_leaderboard_with_groups(paths, models)\n",
    "    \n",
    "    leaderboard_main_subtasks = [\"leaderboard_mmlu_pro\", \"leaderboard_bbh\", \"leaderboard_gpqa\", \"leaderboard_math_hard\", \"leaderboard_ifeval\", \"leaderboard_musr\"]\n",
    "    if not subtasks_df_all.empty and 'subtask' in subtasks_df_all.columns:\n",
    "        subtasks_df = subtasks_df_all[subtasks_df_all['subtask'].isin(leaderboard_main_subtasks)].copy()\n",
    "    else:\n",
    "        subtasks_df = pd.DataFrame()\n",
    "\n",
    "    models_in_subtasks_data = [m for m in comparison_models if m in subtasks_df.columns]\n",
    "    if not subtasks_df.empty and models_in_subtasks_data:\n",
    "        present_base_cols = [c for c in ['subtask', 'subtask_cleaned', 'group'] if c in subtasks_df.columns]\n",
    "        subtasks_comp_df = subtasks_df[present_base_cols + models_in_subtasks_data].copy()\n",
    "    else:\n",
    "        subtasks_comp_df = pd.DataFrame(columns=['subtask', 'subtask_cleaned', 'group'] + models_in_subtasks_data)\n",
    "\n",
    "    if not subtasks_comp_df.empty: print(f\"\\n--- Main Leaderboard Tasks DataFrame ({experiment_name}) ---\"); print(subtasks_comp_df); print(\"-\" * 50)\n",
    "    else: print(f\"\\n--- Main Leaderboard Tasks DataFrame is empty for {experiment_name} ---\")\n",
    "    \n",
    "    return subtasks_comp_df, comparison_models, instruct_model, coder_model, base_model\n",
    "\n",
    "\n",
    "# --- Configuration for Each Experiment ---\n",
    "\n",
    "# --- CODER Experiment ---\n",
    "models_coder = [\n",
    "    \"Qwen2.5-7B\", \"Qwen2.5-7B-Instruct\", \"Qwen2.5-Coder-7B\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-task_arithmetic-29\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-dare_ties-29\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-ties-29\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-slerp-29\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-linear-29\"\n",
    "]\n",
    "short_names_coder = {\n",
    "    \"Qwen2.5-7B\": \"Qwen2.5 Base\", \"Qwen2.5-7B-Instruct\": \"Qwen2.5 Instruct\", \"Qwen2.5-Coder-7B\": \"Qwen2.5 Coder\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-task_arithmetic-29\": \"Task Arithmetic\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-dare_ties-29\": \"DARE Ties\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-ties-29\": \"Ties\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-slerp-29\": \"Slerp\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-linear-29\": \"Linear\"\n",
    "}\n",
    "tasks_coder = [\"leaderboard\"]\n",
    "paths_coder = {m: {t: f\"organized_results/{t}/{m}/result.json\" for t in tasks_coder} for m in models_coder}\n",
    "\n",
    "# --- MATH Experiment ---\n",
    "models_math = [\n",
    "    \"Qwen2.5-7B\", \"Qwen2.5-7B-Instruct\", \"Qwen2.5-Math-7B\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-task_arithmetic-26\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-dare_ties-27\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-ties-26\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-slerp-24\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-linear-24\"\n",
    "]\n",
    "short_names_math = {\n",
    "    \"Qwen2.5-7B\": \"Qwen2.5 Base\", \"Qwen2.5-7B-Instruct\": \"Qwen2.5 Instruct\", \"Qwen2.5-Math-7B\": \"Qwen2.5 Math\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-task_arithmetic-26\": \"Task Arithmetic\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-dare_ties-27\": \"DARE Ties\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-ties-26\": \"Ties\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-slerp-24\": \"Slerp\",\n",
    "    \"Qwen2.5-7B-Instruct-Qwen2.5-Math-7B-Merged-linear-24\": \"Linear\"\n",
    "}\n",
    "tasks_math = [\"leaderboard\"]\n",
    "paths_math = {m: {t: f\"organized_results/{t}/{m}/result.json\" for t in tasks_math} for m in models_math}\n",
    "\n",
    "\n",
    "# --- Run Analysis for Both Experiments ---\n",
    "subtasks_comp_df_coder, comparison_models_coder, instruct_model_coder, specialist_model_coder, base_model_coder = run_analysis_for_experiment(\"Coder\", models_coder, short_names_coder, tasks_coder, paths_coder)\n",
    "subtasks_comp_df_math, comparison_models_math, instruct_model_math, specialist_model_math, base_model_math = run_analysis_for_experiment(\"Math\", models_math, short_names_math, tasks_math, paths_math)\n",
    "\n",
    "\n",
    "# --- 7. Combined Plotting Section ---\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\" GENERATING COMBINED PLOT \")\n",
    "print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "def prepare_plot_data(df, comparison_models, short_names_map):\n",
    "    plot_data_list = []\n",
    "    if df.empty or 'subtask_cleaned' not in df.columns:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    models_to_plot_bar = [m for m in comparison_models if m in df.columns and not df[m].isna().all()]\n",
    "    tasks_to_plot = sorted(df['subtask_cleaned'].unique())\n",
    "\n",
    "    for task_name in tasks_to_plot:\n",
    "        task_data = df[df['subtask_cleaned'] == task_name]\n",
    "        if not task_data.empty:\n",
    "            for model_full_name in comparison_models:\n",
    "                if model_full_name in models_to_plot_bar:\n",
    "                    score = task_data.iloc[0][model_full_name]\n",
    "                    if not pd.isna(score):\n",
    "                        model_short_clean = clean_plot_name(short_names_map.get(model_full_name, model_full_name))\n",
    "                        \n",
    "                        current_model_type = 'Merged' # Default\n",
    "                        s_name = short_names_map.get(model_full_name)\n",
    "                        if s_name == \"Qwen2.5 Base\": current_model_type = \"Base\"\n",
    "                        elif s_name == \"Qwen2.5 Instruct\": current_model_type = \"Instruct\"\n",
    "                        elif s_name.startswith(\"Qwen2.5 Coder\") or s_name.startswith(\"Qwen2.5 Math\"): current_model_type = \"Specialist\"\n",
    "                        \n",
    "                        plot_data_list.append({\n",
    "                            'Task': task_name,\n",
    "                            'Model Short Name': model_short_clean,\n",
    "                            'Score': score,\n",
    "                            'Model Type': current_model_type,\n",
    "                        })\n",
    "    return pd.DataFrame(plot_data_list)\n",
    "\n",
    "# Prepare data for both experiments\n",
    "plot_df_coder = prepare_plot_data(subtasks_comp_df_coder, comparison_models_coder, short_names_coder)\n",
    "plot_df_math = prepare_plot_data(subtasks_comp_df_math, comparison_models_math, short_names_math)\n",
    "\n",
    "# Define model order with core models at the bottom\n",
    "all_tasks = sorted(list(set(plot_df_coder['Task'].unique()) | set(plot_df_math['Task'].unique())))\n",
    "\n",
    "# For Coder plot\n",
    "all_coder_models = plot_df_coder['Model Short Name'].unique()\n",
    "core_coder = ['Qwen2.5 Base', 'Qwen2.5 Instruct', 'Qwen2.5 Coder']\n",
    "core_coder_present = [m for m in core_coder if m in all_coder_models]\n",
    "merged_coder = sorted([m for m in all_coder_models if m not in core_coder_present])\n",
    "models_coder_plot = core_coder_present + merged_coder\n",
    "\n",
    "# For Math plot\n",
    "all_math_models = plot_df_math['Model Short Name'].unique()\n",
    "core_math = ['Qwen2.5 Base', 'Qwen2.5 Instruct', 'Qwen2.5 Math']\n",
    "core_math_present = [m for m in core_math if m in all_math_models]\n",
    "merged_math = sorted([m for m in all_math_models if m not in core_math_present])\n",
    "models_math_plot = core_math_present + merged_math\n",
    "\n",
    "\n",
    "if not all_tasks or (not models_coder_plot and not models_math_plot):\n",
    "    print(\"No data available to generate the combined plot. Exiting plot generation.\")\n",
    "else:\n",
    "    # Create subplot titles\n",
    "    subplot_titles = []\n",
    "    for task in all_tasks:\n",
    "        subplot_titles.append(f\"Coder - {task}\")\n",
    "        subplot_titles.append(f\"Math - {task}\")\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=len(all_tasks),\n",
    "        cols=2,\n",
    "        subplot_titles=subplot_titles,\n",
    "        horizontal_spacing=0.03,\n",
    "        vertical_spacing=0.05\n",
    "    )\n",
    "\n",
    "    color_map = {\n",
    "        \"Base\": 'rgb(100, 149, 237)',\n",
    "        \"Instruct\": 'rgb(50, 205, 50)',\n",
    "        \"Specialist\": 'rgb(255, 165, 0)',\n",
    "        \"Merged\": 'rgb(192, 192, 192)'\n",
    "    }\n",
    "    \n",
    "    # Loop through each task to create a row of subplots\n",
    "    for i, task in enumerate(all_tasks):\n",
    "        row_num = i + 1\n",
    "        \n",
    "        # --- Column 1: Coder ---\n",
    "        df_c = plot_df_coder[plot_df_coder['Task'] == task]\n",
    "        if not df_c.empty:\n",
    "            for model_type in [\"Base\", \"Instruct\", \"Specialist\", \"Merged\"]:\n",
    "                df_c_type = df_c[df_c['Model Type'] == model_type]\n",
    "                if not df_c_type.empty:\n",
    "                    text_threshold = 15\n",
    "                    text_positions = ['outside' if score < text_threshold else 'inside' for score in df_c_type['Score']]\n",
    "                    text_colors = ['black' if score < text_threshold else 'white' for score in df_c_type['Score']]\n",
    "                    \n",
    "                    fig.add_trace(go.Bar(\n",
    "                        y=df_c_type['Model Short Name'],\n",
    "                        x=df_c_type['Score'],\n",
    "                        name=model_type,\n",
    "                        marker_color=color_map.get(model_type),\n",
    "                        orientation='h',\n",
    "                        text=df_c_type.apply(lambda row: f\"<b>{row['Model Short Name']}</b>: {row['Score']:.1f}%\", axis=1),\n",
    "                        textposition=text_positions,\n",
    "                        insidetextanchor='middle',\n",
    "                        textfont=dict(size=12, color=text_colors),\n",
    "                        cliponaxis=False\n",
    "                    ), row=row_num, col=1)\n",
    "            \n",
    "            task_data_row = subtasks_comp_df_coder[subtasks_comp_df_coder['subtask_cleaned'] == task].iloc[0]\n",
    "            instruct_score = task_data_row.get(instruct_model_coder)\n",
    "            specialist_score = task_data_row.get(specialist_model_coder)\n",
    "            \n",
    "            if pd.notna(instruct_score) and pd.notna(specialist_score):\n",
    "                fig.add_shape(type=\"rect\", x0=min(instruct_score, specialist_score), x1=max(instruct_score, specialist_score), y0=-0.5, y1=len(models_coder_plot)-0.5, fillcolor=\"rgba(255, 128, 128, 0.2)\", line_width=0, layer=\"below\", row=row_num, col=1)\n",
    "                fig.add_shape(type=\"line\", x0=instruct_score, x1=instruct_score, y0=-0.5, y1=len(models_coder_plot)-0.5, line=dict(color=color_map['Instruct'], dash=\"dash\", width=2), layer=\"above\", row=row_num, col=1)\n",
    "                fig.add_shape(type=\"line\", x0=specialist_score, x1=specialist_score, y0=-0.5, y1=len(models_coder_plot)-0.5, line=dict(color=color_map['Specialist'], dash=\"dash\", width=2), layer=\"above\", row=row_num, col=1)\n",
    "\n",
    "            # --- NEW: Dynamic X-axis calculation for Coder plot ---\n",
    "            max_score_c = df_c['Score'].max()\n",
    "            fig.update_xaxes(range=[0, max_score_c * 1.20], row=row_num, col=1)\n",
    "\n",
    "\n",
    "        # --- Column 2: Math ---\n",
    "        df_m = plot_df_math[plot_df_math['Task'] == task]\n",
    "        if not df_m.empty:\n",
    "            for model_type in [\"Base\", \"Instruct\", \"Specialist\", \"Merged\"]:\n",
    "                 df_m_type = df_m[df_m['Model Type'] == model_type]\n",
    "                 if not df_m_type.empty:\n",
    "                    text_threshold = 15\n",
    "                    text_positions = ['outside' if score < text_threshold else 'inside' for score in df_m_type['Score']]\n",
    "                    text_colors = ['black' if score < text_threshold else 'white' for score in df_m_type['Score']]\n",
    "\n",
    "                    fig.add_trace(go.Bar(\n",
    "                        y=df_m_type['Model Short Name'],\n",
    "                        x=df_m_type['Score'],\n",
    "                        name=model_type,\n",
    "                        marker_color=color_map.get(model_type),\n",
    "                        orientation='h',\n",
    "                        text=df_m_type.apply(lambda row: f\"<b>{row['Model Short Name']}</b>: {row['Score']:.1f}%\", axis=1),\n",
    "                        textposition=text_positions,\n",
    "                        insidetextanchor='middle',\n",
    "                        textfont=dict(size=22, color=text_colors),\n",
    "                        cliponaxis=False\n",
    "                    ), row=row_num, col=2)\n",
    "\n",
    "            task_data_row = subtasks_comp_df_math[subtasks_comp_df_math['subtask_cleaned'] == task].iloc[0]\n",
    "            instruct_score = task_data_row.get(instruct_model_math)\n",
    "            specialist_score = task_data_row.get(specialist_model_math)\n",
    "\n",
    "            if pd.notna(instruct_score) and pd.notna(specialist_score):\n",
    "                fig.add_shape(type=\"rect\", x0=min(instruct_score, specialist_score), x1=max(instruct_score, specialist_score), y0=-0.5, y1=len(models_math_plot)-0.5, fillcolor=\"rgba(255, 128, 128, 0.2)\", line_width=0, layer=\"below\", row=row_num, col=2)\n",
    "                fig.add_shape(type=\"line\", x0=instruct_score, x1=instruct_score, y0=-0.5, y1=len(models_math_plot)-0.5, line=dict(color=color_map['Instruct'], dash=\"dash\", width=2), layer=\"above\", row=row_num, col=2)\n",
    "                fig.add_shape(type=\"line\", x0=specialist_score, x1=specialist_score, y0=-0.5, y1=len(models_math_plot)-0.5, line=dict(color=color_map['Specialist'], dash=\"dash\", width=2), layer=\"above\", row=row_num, col=2)\n",
    "\n",
    "            # --- NEW: Dynamic X-axis calculation for Math plot ---\n",
    "            max_score_m = df_m['Score'].max()\n",
    "            fig.update_xaxes(range=[0, max_score_m * 1.20], row=row_num, col=2)\n",
    "\n",
    "    # Update layout for the entire figure\n",
    "    height_per_facet = max(len(models_coder_plot), len(models_math_plot)) * 20 + 40 \n",
    "    total_height = height_per_facet * len(all_tasks)\n",
    "\n",
    "    fig.update_layout(\n",
    "        title_text='Absolute Performance Comparison by Task: Coder vs. Math Experiments',\n",
    "        barmode='stack',\n",
    "        height=total_height,\n",
    "        width=2000,\n",
    "        showlegend=False, # Hide the legend\n",
    "        margin=dict(t=100, l=10, r=10, b=50),\n",
    "        **font_config\n",
    "    )\n",
    "\n",
    "    # Update all y-axes\n",
    "    fig.update_yaxes(categoryorder='array', categoryarray=models_coder_plot, showticklabels=False, col=1)\n",
    "    fig.update_yaxes(categoryorder='array', categoryarray=models_math_plot, showticklabels=False, col=2)\n",
    "    \n",
    "    # Clean up subplot titles\n",
    "    fig.for_each_annotation(lambda a: a.update(text=a.text.split(\" - \")[-1], font=dict(size=16)))\n",
    "    \n",
    "    # Add main column titles\n",
    "    fig.add_annotation(x=0.18, y=1.02, yanchor='bottom', text=\"<b>Coder Experiment</b>\", showarrow=False, xref=\"paper\", yref=\"paper\", font=dict(size=20))\n",
    "    fig.add_annotation(x=0.80, y=1.02, yanchor='bottom', text=\"<b>Math Experiment</b>\", showarrow=False, xref=\"paper\", yref=\"paper\", font=dict(size=20))\n",
    "\n",
    "\n",
    "    fig.show()\n",
    "    print(\"Generated combined plot: Absolute Performance Comparison by Task: Coder vs. Math Experiments\")\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
