{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aec042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import defaultdict\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import re # Added for cleaning plot names\n",
    "import json # Added to handle potential JSON loading if paths were used\n",
    "\n",
    "# Set default plotly template for better aesthetics if needed\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# Create directory for CSV exports\n",
    "csv_export_dir = \"plot_data_csv_exports\"\n",
    "os.makedirs(csv_export_dir, exist_ok=True)\n",
    "\n",
    "# --- Helper function for cleaning model names for plots ---\n",
    "def clean_plot_name(name):\n",
    "    if name is None: # Handle potential None input\n",
    "        return \"Unknown\"\n",
    "    name_str = str(name)\n",
    "\n",
    "    # For merged models (which won't start with \"Qwen2.5\" after short_names mapping)\n",
    "    # remove the trailing _XX\n",
    "    if not name_str.startswith(\"Qwen2.5\"): # Handles names like \"Linear_24\" -> \"Linear\"\n",
    "        name_str = re.sub(r'_\\d+$', '', name_str)\n",
    "    return name_str\n",
    "\n",
    "# --- Font configuration for plots ---\n",
    "font_config = {\n",
    "    \"title_font_size\": 30,\n",
    "    \"font_size\": 20,\n",
    "    \"xaxis_title_font_size\": 20,\n",
    "    \"yaxis_title_font_size\": 20,\n",
    "    \"xaxis_tickfont_size\": 20, # Adjusted for potentially dense plots\n",
    "    \"yaxis_tickfont_size\": 20, # Adjusted for potentially dense plots\n",
    "    \"legend_title_font_size\": 30, # Kept for other plots that might use legends\n",
    "    \"legend_font_size\": 25,      # Kept for other plots\n",
    "}\n",
    "\n",
    "# --- Default Plot Dimensions ---\n",
    "default_plot_height = 520\n",
    "default_plot_width = 2300 # Added for wider plots\n",
    "\n",
    "# --- 0. Helper Function from process_results.py (adapted) ---\n",
    "def process_frame(frame):\n",
    "    \"\"\"\n",
    "    Processes the DataFrame by selecting relevant columns,\n",
    "    handling potential renaming and grouping data.\n",
    "    \"\"\"\n",
    "    if \"Unnamed: 0\" in frame.columns:\n",
    "        del frame[\"Unnamed: 0\"]\n",
    "\n",
    "    if \"linguistic subfield\" in frame.columns and \"linguistic competencies\" not in frame.columns:\n",
    "        frame[\"linguistic competencies\"] = frame[\"linguistic subfield\"]\n",
    "        del frame[\"linguistic subfield\"]\n",
    "    elif \"linguistic subfield\" in frame.columns and \"linguistic competencies\" in frame.columns:\n",
    "        del frame[\"linguistic subfield\"]\n",
    "    return frame\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# Please define the models and their short names here, which you want to analyze.\n",
    "models = [\n",
    "    \"Qwen__Qwen2.5-7B\",\n",
    "    \"Qwen__Qwen2.5-7B-Instruct\",\n",
    "    \"Qwen__Qwen2.5-Coder-7B\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-linear-29\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-task_arithmetic-29\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-dare_ties-29\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-ties-29\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-slerp-29\",\n",
    "]\n",
    "\n",
    "# Updated short_names for Coder models\n",
    "short_names = {\n",
    "    \"Qwen__Qwen2.5-7B\": \"Qwen2.5 Base\",\n",
    "    \"Qwen__Qwen2.5-7B-Instruct\": \"Qwen2.5 Instruct\",\n",
    "    \"Qwen__Qwen2.5-Coder-7B\": \"Qwen2.5 Coder\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-linear-29\": \"Linear_29\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-task_arithmetic-29\": \"Task_Arith_29\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-dare_ties-29\": \"DARE_Ties_29\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-ties-29\": \"Ties_29\",\n",
    "    \"_username___Qwen2.5-7B-Instruct-Qwen2.5-Coder-7B-Merged-slerp-29\": \"Slerp_29\",\n",
    "}\n",
    "\n",
    "# tasks list is used to define the order of main task categories (linguistic competencies)\n",
    "# This will be dynamically updated after data loading based on summary_comp_df.index\n",
    "tasks_initial_fallback = [\"Syntax\", \"Semantics\", \"Morphology\", \"Discourse\", \"Pragmatics\", \"Reasoning\"]\n",
    "\n",
    "paths = {m: {t: f\"organized_results/{t}/{m}/result.json\" for t in tasks_initial_fallback} for m in models} # This paths dict might not be used if data comes from CSVs\n",
    "\n",
    "abs_data_file = \"results_flash-holmes.csv\"\n",
    "group_data_file = \"transformed_results.csv\"\n",
    "\n",
    "MAIN_TASK_COL = \"probing dataset\"\n",
    "SUBTASK_COL = \"probe\"\n",
    "SUBTASK_GROUP_COL = \"linguistic competencies\"\n",
    "SUBTASK_PHENOMENA_COL = \"linguistic phenomena\"\n",
    "\n",
    "# Updated model categorization logic\n",
    "instruct_model = None; coder_model = None; merged_models = []; base_model = None\n",
    "for m_full_name in models:\n",
    "    m_short = short_names.get(m_full_name, \"\") # Get the NEW short name\n",
    "\n",
    "    is_instruct = (m_short == \"Qwen2.5 Instruct\")\n",
    "    is_coder = (m_short == \"Qwen2.5 Coder\")\n",
    "    is_base = (m_short == \"Qwen2.5 Base\")\n",
    "    \n",
    "    is_merged = not (is_instruct or is_coder or is_base)\n",
    "\n",
    "    if is_instruct:\n",
    "        instruct_model = m_full_name\n",
    "    elif is_coder:\n",
    "        coder_model = m_full_name\n",
    "    elif is_base:\n",
    "        base_model = m_full_name\n",
    "    elif is_merged:\n",
    "        if m_full_name in models:\n",
    "            merged_models.append(m_full_name)\n",
    "\n",
    "if not instruct_model: print(\"CRITICAL ERROR: Instruct model not identified.\")\n",
    "if not coder_model: print(\"CRITICAL ERROR: Coder model not identified.\")\n",
    "if not merged_models: print(f\"WARNING: No merged models identified ({merged_models}).\")\n",
    "if not base_model: print(\"WARNING: Base model not identified.\")\n",
    "\n",
    "print(\"--- Model Categorization ---\")\n",
    "if base_model: print(f\"Base Model: {base_model} ({short_names.get(base_model, 'N/A')})\")\n",
    "if instruct_model: print(f\"Instruct Model: {instruct_model} ({short_names.get(instruct_model, 'N/A')})\")\n",
    "if coder_model: print(f\"Coder Model: {coder_model} ({short_names.get(coder_model, 'N/A')})\")\n",
    "print(f\"Merged Models ({len(merged_models)}):\")\n",
    "for m_idx, m in enumerate(merged_models): print(f\"  - {m} ({short_names.get(m, 'N/A')})\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "comparison_models_ordered = []\n",
    "if base_model: comparison_models_ordered.append(base_model)\n",
    "if instruct_model: comparison_models_ordered.append(instruct_model)\n",
    "if coder_model: comparison_models_ordered.append(coder_model)\n",
    "comparison_models_ordered.extend([m for m in merged_models if m])\n",
    "comparison_models = list(dict.fromkeys(m for m in comparison_models_ordered if m))\n",
    "\n",
    "if not comparison_models: print(\"CRITICAL ERROR: No models for comparison identified. Exiting.\"); exit()\n",
    "print(f\"Models for comparison (in order): {[clean_plot_name(short_names.get(m, m)) for m in comparison_models]}\") # Apply clean_plot_name here for accurate listing\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# --- 2. Data Loading ---\n",
    "def load_data_from_csv(abs_filepath, group_filepath, model_list,\n",
    "                       main_task_col_name_in_group_file,\n",
    "                       sub_task_col_name_in_group_file,\n",
    "                       group_col_name_in_group_file,\n",
    "                       phenomena_col_name_in_group_file):\n",
    "    try:\n",
    "        raw_abs_df = pd.read_csv(abs_filepath)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Absolute scores file not found at {abs_filepath}\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    if \"Unnamed: 0\" in raw_abs_df.columns:\n",
    "        del raw_abs_df[\"Unnamed: 0\"]\n",
    "\n",
    "    raw_abs_df.rename(columns={'probing_dataset': 'subtask_cleaned', 'model_name': 'model'}, inplace=True)\n",
    "\n",
    "    if 'encoding' in raw_abs_df.columns:\n",
    "        print(f\"Filtering absolute scores for 'encoding' == 'full'. Original rows: {len(raw_abs_df)}\")\n",
    "        raw_abs_df = raw_abs_df[raw_abs_df['encoding'] == 'full'].copy()\n",
    "        print(f\"Rows after filtering for encoding == 'full': {len(raw_abs_df)}\")\n",
    "        if len(raw_abs_df) == 0:\n",
    "            print(\"Warning: No rows remaining after filtering for 'encoding' == 'full'. Subsequent data might be empty.\")\n",
    "    else:\n",
    "        print(\"Warning: 'encoding' column not found in absolute scores file. Cannot filter by encoding.\")\n",
    "\n",
    "    raw_abs_df['score'] = pd.to_numeric(raw_abs_df['score'], errors='coerce')\n",
    "    abs_df_grouped = raw_abs_df.groupby(['subtask_cleaned', 'model'])['score'].mean().reset_index()\n",
    "    abs_pivot_df = abs_df_grouped.pivot_table(index='subtask_cleaned', columns='model', values='score').reset_index()\n",
    "\n",
    "    available_models_in_abs = [m for m in model_list if m in abs_pivot_df.columns]\n",
    "    missing_models_from_abs = [m for m in model_list if m not in abs_pivot_df.columns]\n",
    "    if missing_models_from_abs:\n",
    "        print(f\"Warning: Models from model_list not found in absolute scores data (after filtering/pivoting): {missing_models_from_abs}\")\n",
    "\n",
    "    for model_col in model_list:\n",
    "        if model_col not in abs_pivot_df.columns:\n",
    "            abs_pivot_df[model_col] = np.nan\n",
    "\n",
    "    cols_to_keep_abs = ['subtask_cleaned'] + [m for m in model_list if m in abs_pivot_df.columns]\n",
    "    abs_final_df = abs_pivot_df[cols_to_keep_abs].copy()\n",
    "\n",
    "    try:\n",
    "        raw_group_df = pd.read_csv(group_filepath)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Group info file not found at {group_filepath}.\")\n",
    "        for model_col in available_models_in_abs:\n",
    "            if model_col in abs_final_df.columns: abs_final_df[model_col] = abs_final_df[model_col] * 100\n",
    "        return pd.DataFrame(columns=available_models_in_abs), abs_final_df\n",
    "\n",
    "    group_df_processed = process_frame(raw_group_df.copy())\n",
    "    rename_map_group = {\n",
    "        sub_task_col_name_in_group_file: 'subtask_cleaned',\n",
    "        group_col_name_in_group_file: 'group',\n",
    "        main_task_col_name_in_group_file: 'main_task_category'\n",
    "    }\n",
    "    group_df_processed.rename(columns=rename_map_group, inplace=True)\n",
    "\n",
    "    id_cols_group = ['main_task_category', 'subtask_cleaned', 'group']\n",
    "    original_phenomena_col_name = phenomena_col_name_in_group_file\n",
    "    phenomena_col_to_check = rename_map_group.get(original_phenomena_col_name, original_phenomena_col_name)\n",
    "    if phenomena_col_to_check in group_df_processed.columns:\n",
    "        id_cols_group.append(phenomena_col_to_check)\n",
    "    elif original_phenomena_col_name in group_df_processed.columns:\n",
    "        id_cols_group.append(original_phenomena_col_name)\n",
    "    if 'probe type' in group_df_processed.columns: id_cols_group.append('probe type')\n",
    "    id_cols_group_present = [col for col in id_cols_group if col in group_df_processed.columns]\n",
    "\n",
    "    if 'subtask_cleaned' not in group_df_processed.columns:\n",
    "        print(f\"Critical Error: Subtask column for merging ('subtask_cleaned') not found in group file after renaming. Expected from '{sub_task_col_name_in_group_file}'.\")\n",
    "        for model_col in available_models_in_abs:\n",
    "            if model_col in abs_final_df.columns: abs_final_df[model_col] = abs_final_df[model_col] * 100\n",
    "        return pd.DataFrame(columns=available_models_in_abs), abs_final_df\n",
    "\n",
    "    group_info_to_merge = group_df_processed[id_cols_group_present].drop_duplicates(subset=['subtask_cleaned'])\n",
    "    subtasks_df = pd.merge(abs_final_df, group_info_to_merge, on='subtask_cleaned', how='left')\n",
    "\n",
    "    models_in_subtasks_df = [m for m in model_list if m in subtasks_df.columns]\n",
    "    for model_col in models_in_subtasks_df:\n",
    "        subtasks_df[model_col] = pd.to_numeric(subtasks_df[model_col], errors='coerce') * 100\n",
    "\n",
    "    summary_df = pd.DataFrame()\n",
    "    if 'main_task_category' in subtasks_df.columns and models_in_subtasks_df:\n",
    "        subtasks_df['main_task_category'] = subtasks_df['main_task_category'].astype(str).fillna('Unknown_Category')\n",
    "        summary_df = subtasks_df.groupby('main_task_category')[models_in_subtasks_df].mean()\n",
    "    else:\n",
    "        cols_for_empty_summary = [m for m in model_list if m in abs_pivot_df.columns]\n",
    "        print(f\"Warning: 'main_task_category' not found or no models for summary. Summary DF might be empty or based on limited models.\")\n",
    "        summary_df = pd.DataFrame(columns=cols_for_empty_summary)\n",
    "        if not subtasks_df.empty and 'main_task_category' not in subtasks_df.columns:\n",
    "            print(\"  Specifically, 'main_task_category' column is missing in the merged subtasks_df.\")\n",
    "\n",
    "    return summary_df, subtasks_df\n",
    "\n",
    "summary_comp_df, subtasks_comp_df = load_data_from_csv(abs_data_file, group_data_file, comparison_models, MAIN_TASK_COL, SUBTASK_COL, SUBTASK_GROUP_COL, SUBTASK_PHENOMENA_COL)\n",
    "\n",
    "if not summary_comp_df.empty:\n",
    "    tasks = summary_comp_df.index.tolist()\n",
    "else:\n",
    "    print(\"Warning: summary_comp_df is empty after loading. 'tasks' list for ordering might not be accurate.\")\n",
    "    tasks = tasks_initial_fallback\n",
    "\n",
    "print(\"\\n--- Summary DataFrame (Comparison Models from Absolute Scores) ---\")\n",
    "if not summary_comp_df.empty:\n",
    "    print(summary_comp_df.head())\n",
    "else:\n",
    "    print(\"Summary DataFrame is empty.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if not subtasks_comp_df.empty:\n",
    "    print(\"\\n--- Subtasks DataFrame (Comparison Models from Absolute Scores, Head) ---\")\n",
    "    if 'group' in subtasks_comp_df.columns: print(f\"Subtasks grouped by: {SUBTASK_GROUP_COL} (column name in df: 'group')\")\n",
    "    else: print(f\"Warning: 'group' column (expected from {SUBTASK_GROUP_COL}) is missing in subtasks_comp_df.\")\n",
    "    print(subtasks_comp_df.head())\n",
    "else: print(\"\\n--- Subtasks DataFrame is empty or could not be loaded ---\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- 3. Calculate Differences ---\n",
    "can_calc_diffs = True\n",
    "if instruct_model is None:\n",
    "    print(\"Critical Error: Instruct model (instruct_model) is not defined. Cannot calculate differences.\")\n",
    "    can_calc_diffs = False\n",
    "elif instruct_model not in comparison_models :\n",
    "    print(f\"Critical Error: Instruct model '{instruct_model}' not in comparison_models. Cannot calculate differences.\")\n",
    "    can_calc_diffs = False\n",
    "\n",
    "if can_calc_diffs and (instruct_model not in summary_comp_df.columns and not summary_comp_df.empty):\n",
    "    print(f\"Warning: Instruct model '{instruct_model}' missing from summary_comp_df columns. Diff calculation for main tasks might fail.\")\n",
    "    \n",
    "diff_cols_main = []; diff_cols_subtasks = []\n",
    "\n",
    "if can_calc_diffs:\n",
    "    if not summary_comp_df.empty and instruct_model in summary_comp_df.columns:\n",
    "        if coder_model and coder_model in summary_comp_df.columns:\n",
    "            summary_comp_df['d_coder'] = summary_comp_df[instruct_model] - summary_comp_df[coder_model]; diff_cols_main.append('d_coder')\n",
    "        else: print(f\"Warning: Coder model '{coder_model}' not in summary_comp_df or not defined. Skipping 'd_coder' for main tasks.\"); summary_comp_df['d_coder'] = np.nan\n",
    "\n",
    "        for merged_m in merged_models:\n",
    "            if merged_m in summary_comp_df.columns:\n",
    "                merged_short_name = clean_plot_name(short_names.get(merged_m, merged_m.split(\"__\")[-1])) # Apply clean_plot_name here for diff col name\n",
    "                col_name = f\"d_merged_{merged_short_name}\"\n",
    "                summary_comp_df[col_name] = summary_comp_df[instruct_model] - summary_comp_df[merged_m]; diff_cols_main.append(col_name)\n",
    "            else: print(f\"Warning: Merged model {merged_m} not in summary_comp_df. Skipping diff for main tasks.\")\n",
    "\n",
    "        if diff_cols_main:\n",
    "            print(\"\\n--- Summary DataFrame with Differences (Head) ---\")\n",
    "            present_main_diff_cols = [c for c in diff_cols_main if c in summary_comp_df.columns]\n",
    "            if present_main_diff_cols: print(summary_comp_df[present_main_diff_cols].head())\n",
    "            else: print(\"No diff columns for main tasks created/present.\")\n",
    "            print(\"-\" * 50)\n",
    "    elif summary_comp_df.empty :\n",
    "        print(f\"Info: summary_comp_df is empty. Skipping difference calculation for main tasks.\")\n",
    "    else:\n",
    "        print(f\"Warning: Instruct model '{instruct_model}' not in summary_comp_df (but summary_comp_df not empty). Cannot calculate differences for main tasks.\")\n",
    "\n",
    "    if not subtasks_comp_df.empty and instruct_model in subtasks_comp_df.columns:\n",
    "        if coder_model and coder_model in subtasks_comp_df.columns:\n",
    "            subtasks_comp_df['d_coder'] = subtasks_comp_df[instruct_model] - subtasks_comp_df[coder_model]; diff_cols_subtasks.append('d_coder')\n",
    "        else: print(f\"Warning: Coder model '{coder_model}' not in subtasks_comp_df or not defined. Skipping 'd_coder' for subtasks.\"); subtasks_comp_df['d_coder'] = np.nan\n",
    "\n",
    "        for merged_m in merged_models:\n",
    "            if merged_m in subtasks_comp_df.columns:\n",
    "                merged_short_name = clean_plot_name(short_names.get(merged_m, merged_m.split(\"__\")[-1])) # Apply clean_plot_name here\n",
    "                col_name = f\"d_merged_{merged_short_name}\"\n",
    "                subtasks_comp_df[col_name] = subtasks_comp_df[instruct_model] - subtasks_comp_df[merged_m]; diff_cols_subtasks.append(col_name)\n",
    "            else: print(f\"Warning: Merged model {merged_m} not in subtasks_comp_df. Skipping diff for subtasks.\")\n",
    "\n",
    "        if diff_cols_subtasks:\n",
    "            print(\"\\n--- Subtasks DataFrame with Differences (Head) ---\")\n",
    "            present_sub_diff_cols = [col for col in diff_cols_subtasks if col in subtasks_comp_df.columns]\n",
    "            base_id_cols_sub = [c for c in ['subtask_cleaned', 'group', 'main_task_category'] if c in subtasks_comp_df.columns]\n",
    "\n",
    "            models_to_show_sub_list = []\n",
    "            if instruct_model and instruct_model in subtasks_comp_df.columns: models_to_show_sub_list.append(instruct_model)\n",
    "            if coder_model and coder_model in subtasks_comp_df.columns: models_to_show_sub_list.append(coder_model)\n",
    "            for m_model in merged_models:\n",
    "                if m_model in subtasks_comp_df.columns: models_to_show_sub_list.append(m_model)\n",
    "\n",
    "            cols_to_show_sub = base_id_cols_sub + models_to_show_sub_list + present_sub_diff_cols\n",
    "            cols_to_show_sub = [c for c in cols_to_show_sub if c in subtasks_comp_df.columns]\n",
    "            if present_sub_diff_cols and cols_to_show_sub:\n",
    "                print(subtasks_comp_df[cols_to_show_sub].head())\n",
    "            elif not present_sub_diff_cols:\n",
    "                print(\"No difference columns for subtasks created/present.\")\n",
    "            else:\n",
    "                print(\"No columns available to display for subtask differences (or base ID columns missing).\")\n",
    "        else: print(\"No diff columns for subtasks were created.\")\n",
    "        print(\"-\" * 50)\n",
    "    elif subtasks_comp_df.empty:\n",
    "        print(\"Info: Subtasks DataFrame (subtasks_comp_df) empty, skipping subtask diffs.\")\n",
    "    else:\n",
    "        print(f\"Warning: Instruct model '{instruct_model}' missing from subtasks_comp_df (but subtasks_comp_df not empty). Cannot calculate subtask diffs.\")\n",
    "else:\n",
    "    print(\"Skipping difference calculations due to earlier critical errors or missing instruct model in data.\")\n",
    "\n",
    "# --- 4. Ranking Generation ---\n",
    "def generate_rankings(summary_data, subtask_data, model_names, short_names_map,\n",
    "                      main_task_index_name='main_task_category',\n",
    "                      subtask_name_col='subtask_cleaned', group_col_in_df='group'):\n",
    "    ranking_results = {}\n",
    "\n",
    "    models_to_rank_summary = [m for m in model_names if m in summary_data.columns] if not summary_data.empty else []\n",
    "    models_to_rank_subtasks = [m for m in model_names if m in subtask_data.columns] if not subtask_data.empty else []\n",
    "\n",
    "    if not models_to_rank_summary and not models_to_rank_subtasks:\n",
    "        print(\"No models available in dataframes to generate rankings.\")\n",
    "        return ranking_results\n",
    "\n",
    "    if not summary_data.empty and models_to_rank_summary:\n",
    "        main_rankings = []\n",
    "        current_idx_name = summary_data.index.name if summary_data.index.name else main_task_index_name\n",
    "        for task_val in summary_data.index:\n",
    "            scores = summary_data.loc[task_val, models_to_rank_summary].astype(float)\n",
    "            ranked_models = scores.sort_values(ascending=False, na_position='last').index.tolist()\n",
    "            ranked_short_names_for_table = [clean_plot_name(short_names_map.get(m, m)) for m in ranked_models] # Apply clean_plot_name\n",
    "            row = {'Task': task_val}\n",
    "            for i, name in enumerate(ranked_short_names_for_table):\n",
    "                row[f'Rank {i+1}'] = name\n",
    "            main_rankings.append(row)\n",
    "        if main_rankings:\n",
    "            main_rankings_df = pd.DataFrame(main_rankings).set_index('Task')\n",
    "            main_rankings_df.index.name = current_idx_name\n",
    "            ranking_results['main_tasks'] = main_rankings_df\n",
    "\n",
    "    if not subtask_data.empty and models_to_rank_subtasks and subtask_name_col in subtask_data.columns:\n",
    "        subtask_rankings = []\n",
    "        actual_group_col_for_ranking = group_col_in_df if group_col_in_df in subtask_data.columns else None\n",
    "        for idx, row_data in subtask_data.iterrows():\n",
    "            if isinstance(row_data, pd.Series) and all(m in row_data.index for m in models_to_rank_subtasks):\n",
    "                try:\n",
    "                    scores = row_data[models_to_rank_subtasks].astype(float)\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: Scores for subtask row {idx} (name: {row_data.get(subtask_name_col, 'N/A')}) not numeric. Skipping for ranking.\")\n",
    "                    continue\n",
    "                ranked_models = scores.sort_values(ascending=False, na_position='last').index.tolist()\n",
    "                ranked_short_names_for_table = [clean_plot_name(short_names_map.get(m, m)) for m in ranked_models] # Apply clean_plot_name\n",
    "                row = {'Subtask': row_data[subtask_name_col]}\n",
    "                if actual_group_col_for_ranking:\n",
    "                    row['Group'] = row_data[actual_group_col_for_ranking]\n",
    "                for i, name in enumerate(ranked_short_names_for_table):\n",
    "                    row[f'Rank {i+1}'] = name\n",
    "                subtask_rankings.append(row)\n",
    "        if subtask_rankings:\n",
    "            rank_df_sub = pd.DataFrame(subtask_rankings)\n",
    "            base_cols_sub = ['Subtask'] + (['Group'] if actual_group_col_for_ranking and 'Group' in rank_df_sub.columns else [])\n",
    "            rank_cols_sub_list = [f'Rank {i+1}' for i in range(len(models_to_rank_subtasks))]\n",
    "            for r_col in rank_cols_sub_list:\n",
    "                if r_col not in rank_df_sub.columns: rank_df_sub[r_col] = np.nan\n",
    "            cols_order_sub = base_cols_sub + rank_cols_sub_list\n",
    "            rank_df_sub = rank_df_sub[cols_order_sub]\n",
    "            if actual_group_col_for_ranking and 'Group' in rank_df_sub.columns and 'Subtask' in rank_df_sub.columns:\n",
    "                rank_df_sub = rank_df_sub.sort_values(by=['Group', 'Subtask']).set_index(['Group', 'Subtask'])\n",
    "            elif 'Subtask' in rank_df_sub.columns:\n",
    "                rank_df_sub = rank_df_sub.set_index('Subtask')\n",
    "            ranking_results['subtasks'] = rank_df_sub\n",
    "\n",
    "    if not subtask_data.empty and group_col_in_df in subtask_data.columns and models_to_rank_subtasks:\n",
    "        subtask_rankings_defined = 'subtask_rankings' in locals() and subtask_rankings is not None\n",
    "        if 'subtasks' in ranking_results or not subtask_rankings_defined or (subtask_rankings_defined and not subtask_rankings) :\n",
    "            try:\n",
    "                temp_subtask_data = subtask_data.copy()\n",
    "                for m in models_to_rank_subtasks:\n",
    "                    if m in temp_subtask_data.columns:\n",
    "                        temp_subtask_data[m] = pd.to_numeric(temp_subtask_data[m], errors='coerce')\n",
    "\n",
    "                avg_scores_group_data = temp_subtask_data.dropna(subset=models_to_rank_subtasks, how='all')\n",
    "                if not avg_scores_group_data.empty:\n",
    "                    avg_scores_group = avg_scores_group_data.groupby(group_col_in_df)[models_to_rank_subtasks].mean()\n",
    "                    group_rankings_list = []\n",
    "                    for grp_name in avg_scores_group.index:\n",
    "                        scores = avg_scores_group.loc[grp_name]\n",
    "                        if scores.isna().all():\n",
    "                            print(f\"Warning: All scores for group '{grp_name}' are NaN after averaging. Skipping group ranking.\")\n",
    "                            continue\n",
    "                        ranked_models = scores.sort_values(ascending=False, na_position='last').index.tolist()\n",
    "                        ranked_short_names_for_table = [clean_plot_name(short_names_map.get(m, m)) for m in ranked_models] # Apply clean_plot_name\n",
    "                        row = {'Group': grp_name}\n",
    "                        for i, name in enumerate(ranked_short_names_for_table):\n",
    "                            row[f'Rank {i+1}'] = name\n",
    "                        group_rankings_list.append(row)\n",
    "                    if group_rankings_list:\n",
    "                        ranking_results['group_avg'] = pd.DataFrame(group_rankings_list).set_index('Group')\n",
    "                else:\n",
    "                    print(\"No data for group average ranking after NaN removal (all model scores were NaN).\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not calculate group average rankings: {e}\")\n",
    "    return ranking_results\n",
    "\n",
    "rankings = generate_rankings(summary_comp_df, subtasks_comp_df, comparison_models, short_names, main_task_index_name=MAIN_TASK_COL, group_col_in_df='group')\n",
    "\n",
    "print(\"\\n\" + \"=\"*20 + \" MODEL RANKINGS (from Absolute Scores) \" + \"=\"*20)\n",
    "if 'main_tasks' in rankings and not rankings['main_tasks'].empty: print(\"\\n--- Main Task Rankings ---\"); print(rankings['main_tasks'])\n",
    "if 'subtasks' in rankings and not rankings['subtasks'].empty: print(\"\\n--- Subtask Rankings (Head)---\"); print(rankings['subtasks'].head())\n",
    "if 'group_avg' in rankings and not rankings['group_avg'].empty: print(\"\\n--- Group Average Rankings ---\"); print(rankings['group_avg'])\n",
    "output_dir = \"rankings_output_flash_holmes_absolute\"; os.makedirs(output_dir, exist_ok=True); print(f\"\\n--- Saving Rankings to CSV in '{output_dir}/' ---\")\n",
    "for name, df_to_save in rankings.items():\n",
    "    if isinstance(df_to_save, pd.DataFrame) and not df_to_save.empty:\n",
    "        try: csv_filename = os.path.join(output_dir, f\"{name}_rankings_fh_absolute.csv\"); df_to_save.to_csv(csv_filename, index=True); print(f\"Saved {name} rankings to {csv_filename}\")\n",
    "        except Exception as e: print(f\"Error saving {name} rankings: {e}\")\n",
    "print(\"=\"*58)\n",
    "\n",
    "# --- 5. Merged Model Performance Categorization (vs Instruct & Coder) on Main Tasks ---\n",
    "print(\"\\n--- Merged Model Performance Categorization (vs Instruct & Coder) on Main Tasks (from Absolute Scores) ---\")\n",
    "merged_model_comparison_counts = []\n",
    "task_scenario_ranking_data = []\n",
    "\n",
    "if instruct_model and coder_model and \\\n",
    "    (not summary_comp_df.empty and instruct_model in summary_comp_df.columns and coder_model in summary_comp_df.columns):\n",
    "\n",
    "    tasks_categorization = {task: {\"Better_than_both\": [], \"Worse_than_both\": [], \"Between_Equal\": []} for task in summary_comp_df.index}\n",
    "\n",
    "    for merged_m in merged_models:\n",
    "        if merged_m in summary_comp_df.columns:\n",
    "            better_than_both_count = 0; worse_than_both_count = 0; between_equal_count = 0\n",
    "            for task_idx in summary_comp_df.index:\n",
    "                merged_score = summary_comp_df.loc[task_idx, merged_m]\n",
    "                instruct_score = summary_comp_df.loc[task_idx, instruct_model]\n",
    "                coder_score = summary_comp_df.loc[task_idx, coder_model]\n",
    "\n",
    "                if pd.isna(merged_score) or pd.isna(instruct_score) or pd.isna(coder_score): continue\n",
    "\n",
    "                min_ic = min(instruct_score, coder_score); max_ic = max(instruct_score, coder_score)\n",
    "                merged_m_short_cleaned = clean_plot_name(short_names.get(merged_m, merged_m))\n",
    "\n",
    "                if merged_score > max_ic:\n",
    "                    better_than_both_count += 1\n",
    "                    tasks_categorization[task_idx][\"Better_than_both\"].append(merged_m_short_cleaned)\n",
    "                elif merged_score < min_ic:\n",
    "                    worse_than_both_count += 1\n",
    "                    tasks_categorization[task_idx][\"Worse_than_both\"].append(merged_m_short_cleaned)\n",
    "                elif min_ic <= merged_score <= max_ic:\n",
    "                    between_equal_count += 1\n",
    "                    tasks_categorization[task_idx][\"Between_Equal\"].append(merged_m_short_cleaned)\n",
    "            merged_model_comparison_counts.append({\"Merged Model\": clean_plot_name(short_names.get(merged_m, merged_m)), \"Better than Instruct & Coder\": better_than_both_count, \"Worse than Instruct & Coder\": worse_than_both_count, \"Between/Equal to Instruct & Coder\": between_equal_count})\n",
    "        else: print(f\"Skipping categorization for {merged_m} as it's not in summary_comp_df.\")\n",
    "\n",
    "    for task_name, categories in tasks_categorization.items():\n",
    "        task_scenario_ranking_data.append({\n",
    "            \"Task\": task_name,\n",
    "            \"Better_Count\": len(categories[\"Better_than_both\"]),\n",
    "            \"Better_Models\": \", \".join(sorted(list(set(categories[\"Better_than_both\"])))),\n",
    "            \"Worse_Count\": len(categories[\"Worse_than_both\"]),\n",
    "            \"Worse_Models\": \", \".join(sorted(list(set(categories[\"Worse_than_both\"])))),\n",
    "            \"Between_Equal_Count\": len(categories[\"Between_Equal\"]),\n",
    "            \"Between_Equal_Models\": \", \".join(sorted(list(set(categories[\"Between_Equal\"]))))\n",
    "        })\n",
    "    task_scenario_df = pd.DataFrame(task_scenario_ranking_data)\n",
    "    if not task_scenario_df.empty:\n",
    "        print(\"\\n--- Task Scenario Ranking Table (Merged Models vs Instruct & Coder) ---\"); print(task_scenario_df)\n",
    "        try: task_scenario_csv_filename = os.path.join(output_dir, \"task_scenario_merged_model_rankings_fh_absolute.csv\"); task_scenario_df.to_csv(task_scenario_csv_filename, index=False); print(f\"Saved task scenario rankings to {task_scenario_csv_filename}\")\n",
    "        except Exception as e: print(f\"Error saving task scenario rankings to CSV: {e}\")\n",
    "    else: print(\"Task Scenario Ranking Table is empty.\")\n",
    "    print(\"=\"*58)\n",
    "\n",
    "    if merged_model_comparison_counts:\n",
    "        counts_df = pd.DataFrame(merged_model_comparison_counts)\n",
    "        print(\"\\n--- Counts of Merged Model Performance Categories (Main Tasks Overall) ---\"); print(counts_df)\n",
    "        if not counts_df.empty:\n",
    "            counts_df_melted = counts_df.melt(id_vars=\"Merged Model\", value_vars=[\"Better than Instruct & Coder\", \"Worse than Instruct & Coder\", \"Between/Equal to Instruct & Coder\"], var_name=\"Category\", value_name=\"Number of Tasks\")\n",
    "            fig_counts = px.bar(counts_df_melted, x=\"Merged Model\", y=\"Number of Tasks\", color=\"Category\", title=\"Merged Model Performance vs. Instruct & Coder (Main Tasks Overall)\", barmode='stack', labels={\"Number of Tasks\": \"Number of Main Tasks\"})\n",
    "            fig_counts.update_xaxes(categoryorder=\"array\", categoryarray=counts_df[\"Merged Model\"].tolist())\n",
    "            fig_counts.update_layout(\n",
    "                height=default_plot_height,\n",
    "                width=default_plot_width,\n",
    "                legend=dict(orientation=\"h\", yanchor=\"bottom\", y=-0.3, xanchor=\"center\", x=0.5),\n",
    "                **font_config\n",
    "            )\n",
    "            fig_counts.show(); print(\"Generated Plot: Merged Model Performance Category Counts\")\n",
    "            \n",
    "            # Export data for this plot\n",
    "            counts_df_melted.to_csv(os.path.join(csv_export_dir, \"merged_model_performance_counts.csv\"), index=False)\n",
    "            print(f\"Exported plot data to: {csv_export_dir}/merged_model_performance_counts.csv\")\n",
    "\n",
    "            if len(counts_df[\"Merged Model\"].unique()) > 1:\n",
    "                pivot_counts_df = counts_df.set_index(\"Merged Model\")\n",
    "                categories_to_correlate = [\"Better than Instruct & Coder\", \"Worse than Instruct & Coder\", \"Between/Equal to Instruct & Coder\"]\n",
    "                categories_present = [cat for cat in categories_to_correlate if cat in pivot_counts_df.columns]\n",
    "                if len(categories_present) > 1 and not pivot_counts_df[categories_present].T.empty and len(pivot_counts_df[categories_present].T.columns) > 1 :\n",
    "                    correlation_matrix = pivot_counts_df[categories_present].T.corr()\n",
    "                    print(\"\\n--- Correlation Matrix of Performance Categories Between Merged Models (Main Tasks Overall) ---\"); print(correlation_matrix)\n",
    "                    \n",
    "                    corr_heatmap_height = 600\n",
    "                    fig_corr_heatmap = px.imshow(correlation_matrix, text_auto=True, aspect=\"auto\", color_continuous_scale='RdBu_r', range_color=[-1,1], title=\"Correlation of Performance Categories Between Merged Models (Main Tasks Overall)\")\n",
    "                    fig_corr_heatmap.update_layout(height=corr_heatmap_height, width=default_plot_width, **font_config)\n",
    "                    fig_corr_heatmap.show(); print(\"Generated Plot: Correlation Heatmap of Performance Categories\")\n",
    "                    \n",
    "                    # Export correlation matrix\n",
    "                    correlation_matrix.to_csv(os.path.join(csv_export_dir, \"performance_categories_correlation_matrix.csv\"))\n",
    "                    print(f\"Exported correlation matrix to: {csv_export_dir}/performance_categories_correlation_matrix.csv\")\n",
    "                else: print(\"Not enough categories/models with data for correlation matrix.\")\n",
    "            else: print(\"Not enough unique merged models for category correlation.\")\n",
    "        else: print(\"Counts DataFrame empty, cannot generate plots/correlations.\")\n",
    "    else: print(\"No merged model comparison counts generated.\")\n",
    "elif summary_comp_df.empty: print(\"Skipping Merged Model Performance Categorization & Task Scenario Table: summary_comp_df is empty.\")\n",
    "else: print(f\"Skipping Merged Model Performance Categorization & Task Scenario Table: Key models (Instruct: {instruct_model in summary_comp_df.columns if instruct_model else 'N/A'}, Coder: {coder_model in summary_comp_df.columns if coder_model else 'N/A'}) or data missing from summary_comp_df.\")\n",
    "print(\"=\"*58)\n",
    "\n",
    "# --- 6. Original Plotting Section (Plots 1-8 from previous script) ---\n",
    "print(\"\\n--- Generating Original Plots (Based on Absolute Scores) ---\")\n",
    "instruct_short_label = clean_plot_name(short_names.get(instruct_model, \"Instruct\")) if instruct_model else \"Instruct\"\n",
    "coder_short_label = clean_plot_name(short_names.get(coder_model, \"Coder\")) if coder_model else \"Coder\"\n",
    "xaxis_title_main_tasks = summary_comp_df.index.name if not summary_comp_df.empty and summary_comp_df.index.name else 'Main Task Category'\n",
    "\n",
    "# Plot: Difference Trends on Main Tasks\n",
    "if can_calc_diffs and not summary_comp_df.empty and instruct_model in summary_comp_df.columns :\n",
    "    fig1 = go.Figure()\n",
    "    plot_data_diff_trends = []\n",
    "    \n",
    "    if coder_model and 'd_coder' in summary_comp_df.columns and not summary_comp_df['d_coder'].isna().all():\n",
    "        fig1.add_trace(go.Scatter(x=summary_comp_df.index, y=summary_comp_df['d_coder'], mode='lines+markers', name=f'{instruct_short_label}–{coder_short_label}', marker=dict(symbol='circle', size=8), line=dict(dash='dash'), hovertemplate=f'Task: %{{x}}<br>{instruct_short_label}–{coder_short_label}: %{{y:.2f}}%<extra></extra>'))\n",
    "        for idx, task in enumerate(summary_comp_df.index):\n",
    "            plot_data_diff_trends.append({\n",
    "                'Task': task,\n",
    "                'Difference_Type': f'{instruct_short_label}–{coder_short_label}',\n",
    "                'Difference_Value': summary_comp_df.loc[task, 'd_coder']\n",
    "            })\n",
    "    \n",
    "    colors = px.colors.qualitative.Plotly; merged_plot_idx = 0\n",
    "    for diff_col in diff_cols_main:\n",
    "        if diff_col.startswith('d_merged_') and diff_col in summary_comp_df.columns and not summary_comp_df[diff_col].isna().all():\n",
    "            original_merged_short_name = diff_col.replace('d_merged_', '')\n",
    "            cleaned_merged_short_name_plot = clean_plot_name(original_merged_short_name)\n",
    "            fig1.add_trace(go.Scatter(x=summary_comp_df.index, y=summary_comp_df[diff_col], mode='lines+markers', name=f'{instruct_short_label}–{cleaned_merged_short_name_plot}', marker=dict(symbol='square', size=8, color=colors[merged_plot_idx % len(colors)]), hovertemplate=f'Task: %{{x}}<br>{instruct_short_label}–{cleaned_merged_short_name_plot}: %{{y:.2f}}%<extra></extra>')); merged_plot_idx +=1\n",
    "            for idx, task in enumerate(summary_comp_df.index):\n",
    "                plot_data_diff_trends.append({\n",
    "                    'Task': task,\n",
    "                    'Difference_Type': f'{instruct_short_label}–{cleaned_merged_short_name_plot}',\n",
    "                    'Difference_Value': summary_comp_df.loc[task, diff_col]\n",
    "                })\n",
    "    \n",
    "    if fig1.data:\n",
    "        fig1_title = f'Difference Trends on Main Tasks (vs {instruct_short_label})'\n",
    "        fig1.update_layout(title=fig1_title, xaxis_title=xaxis_title_main_tasks, yaxis_title='Performance Difference (%)', legend_title_text='Difference Type', hovermode='x unified', height=default_plot_height, width=default_plot_width, **font_config); fig1.show()\n",
    "        print(f\"Generated plot: {fig1_title} (X-axis: {xaxis_title_main_tasks})\")\n",
    "        \n",
    "        # Export data\n",
    "        if plot_data_diff_trends:\n",
    "            pd.DataFrame(plot_data_diff_trends).to_csv(os.path.join(csv_export_dir, \"difference_trends_main_tasks.csv\"), index=False)\n",
    "            print(f\"Exported plot data to: {csv_export_dir}/difference_trends_main_tasks.csv\")\n",
    "    else: print(\"Skipping Difference Trends plot: No data to plot (all difference series might be NaN or no valid diff columns).\")\n",
    "else: print(f\"Skipping Difference Trends plot: Conditions not met (can_calc_diffs: {can_calc_diffs}, summary_empty: {summary_comp_df.empty}, instruct_in_summary: {instruct_model in summary_comp_df.columns if instruct_model else False}).\")\n",
    "\n",
    "# Plot: Absolute Performance on Main Tasks (Line Chart)\n",
    "if not summary_comp_df.empty:\n",
    "    fig1_abs = go.Figure(); colors = px.colors.qualitative.Plotly; plot_idx = 0\n",
    "    models_to_plot_abs = [m for m in comparison_models if m and m in summary_comp_df.columns and not summary_comp_df[m].isna().all()]\n",
    "    plot_data_abs_main = []\n",
    "\n",
    "    for model_name_abs in models_to_plot_abs:\n",
    "        short_name_abs_cleaned = clean_plot_name(short_names.get(model_name_abs, model_name_abs))\n",
    "        current_symbol = 'circle'; current_line_style = 'solid'\n",
    "\n",
    "        if model_name_abs == base_model:\n",
    "            current_symbol = 'star'; current_line_style = 'dashdot'\n",
    "        elif model_name_abs == coder_model:\n",
    "            current_symbol = 'diamond'; current_line_style = 'dash'\n",
    "        elif model_name_abs == instruct_model:\n",
    "            pass\n",
    "        elif model_name_abs in merged_models:\n",
    "            current_symbol = 'square'; current_line_style = 'dot'\n",
    "        \n",
    "        fig1_abs.add_trace(go.Scatter(x=summary_comp_df.index, y=summary_comp_df[model_name_abs], mode='lines+markers', name=short_name_abs_cleaned, marker=dict(symbol=current_symbol, size=8, color=colors[plot_idx % len(colors)]), line=dict(dash=current_line_style), hovertemplate=f'Task: %{{x}}<br>{short_name_abs_cleaned} Score: %{{y:.2f}}%<extra></extra>')); plot_idx += 1\n",
    "        \n",
    "        for task in summary_comp_df.index:\n",
    "            plot_data_abs_main.append({\n",
    "                'Task': task,\n",
    "                'Model': short_name_abs_cleaned,\n",
    "                'Score': summary_comp_df.loc[task, model_name_abs]\n",
    "            })\n",
    "    \n",
    "    if fig1_abs.data:\n",
    "        fig1_abs_title = 'Absolute Performance on Main Tasks'\n",
    "        fig1_abs.update_layout(title=fig1_abs_title, xaxis_title=xaxis_title_main_tasks, yaxis_title='Performance Score (%)', legend_title_text='Model', hovermode='x unified', height=default_plot_height, width=default_plot_width, **font_config); fig1_abs.show()\n",
    "        print(f\"Generated plot: {fig1_abs_title} (X-axis: {xaxis_title_main_tasks})\")\n",
    "        \n",
    "        # Export data\n",
    "        if plot_data_abs_main:\n",
    "            pd.DataFrame(plot_data_abs_main).to_csv(os.path.join(csv_export_dir, \"absolute_performance_main_tasks_line.csv\"), index=False)\n",
    "            print(f\"Exported plot data to: {csv_export_dir}/absolute_performance_main_tasks_line.csv\")\n",
    "    else: print(\"Skipping Absolute Performance plot: No data to plot (all model series might be NaN).\")\n",
    "else: print(\"Skipping Absolute Performance plot (Absolute Scores): summary_comp_df is empty.\")\n",
    "\n",
    "# UPDATED Bar Chart for Main Task Categories (Linguistic Competencies from 'probing dataset' in summary_comp_df)\n",
    "if not summary_comp_df.empty:\n",
    "    models_to_plot_bar_main_holmes = [m for m in comparison_models if m in summary_comp_df.columns and not summary_comp_df[m].isna().all()]\n",
    "    \n",
    "    color_map_main_bar_holmes = {}\n",
    "    if base_model: color_map_main_bar_holmes[\"Qwen2.5 Base\"] = 'rgb(100, 149, 237)'\n",
    "    if instruct_model: color_map_main_bar_holmes[\"Qwen2.5 Instruct\"] = 'rgb(50, 205, 50)'\n",
    "    if coder_model: color_map_main_bar_holmes[\"Qwen2.5 Coder\"] = 'rgb(255, 165, 0)'\n",
    "    color_map_main_bar_holmes['Merged'] = 'rgb(192, 192, 192)'\n",
    "\n",
    "    if models_to_plot_bar_main_holmes:\n",
    "        plot_data_list_main_bar = []\n",
    "        competency_order_holmes = tasks\n",
    "\n",
    "        for comp_idx, competency_name in enumerate(competency_order_holmes):\n",
    "            if competency_name in summary_comp_df.index:\n",
    "                for model_idx, model_full_name in enumerate(comparison_models):\n",
    "                    if model_full_name in models_to_plot_bar_main_holmes:\n",
    "                        score = summary_comp_df.loc[competency_name, model_full_name]\n",
    "                        if not pd.isna(score):\n",
    "                            model_short_clean = clean_plot_name(short_names.get(model_full_name, model_full_name))\n",
    "                            \n",
    "                            model_type_for_color = \"Merged\"\n",
    "                            if model_full_name == base_model: model_type_for_color = \"Qwen2.5 Base\"\n",
    "                            elif model_full_name == instruct_model: model_type_for_color = \"Qwen2.5 Instruct\"\n",
    "                            elif model_full_name == coder_model: model_type_for_color = \"Qwen2.5 Coder\"\n",
    "                            \n",
    "                            plot_data_list_main_bar.append({\n",
    "                                'Linguistic Competency': competency_name,\n",
    "                                'Model Short Name': model_short_clean,\n",
    "                                'Score': score,\n",
    "                                'Model Type for Color': model_type_for_color,\n",
    "                                'Competency Index': comp_idx,\n",
    "                                'Model Index': model_idx\n",
    "                            })\n",
    "        \n",
    "        if not plot_data_list_main_bar:\n",
    "            print(f\"Skipping 'Absolute Performance by {MAIN_TASK_COL} (Bar Chart)': No data to plot after filtering.\")\n",
    "        else:\n",
    "            main_bar_df_holmes = pd.DataFrame(plot_data_list_main_bar)\n",
    "            main_bar_df_holmes.sort_values(by=['Competency Index', 'Model Index'], ascending=[True, True], inplace=True)\n",
    "\n",
    "            # Create a new column for the text inside the bar\n",
    "            main_bar_df_holmes['bar_label'] = main_bar_df_holmes.apply(\n",
    "                lambda row: f\"{row['Model Short Name']}: {row['Score']:.2f}%\", axis=1\n",
    "            )\n",
    "\n",
    "            fig_main_bar_title_holmes = f'Absolute Performance Comparison by {MAIN_TASK_COL}'\n",
    "            \n",
    "            num_competencies_holmes = main_bar_df_holmes['Linguistic Competency'].nunique()\n",
    "            row_spacing_val_holmes = 0.02\n",
    "            if num_competencies_holmes > 1:\n",
    "                max_allowed_spacing_h = 1.0 / (num_competencies_holmes -1) if (num_competencies_holmes -1) > 0 else 1.0\n",
    "                row_spacing_val_holmes = min(0.02, max_allowed_spacing_h * 0.9)\n",
    "\n",
    "            fig_main_bar_holmes = px.bar(\n",
    "                main_bar_df_holmes, x='Score', y='Model Short Name',\n",
    "                color='Model Type for Color', color_discrete_map=color_map_main_bar_holmes,\n",
    "                orientation='h', title=fig_main_bar_title_holmes,\n",
    "                labels={'Score': 'Performance Score (%)', 'Model Short Name': '', \n",
    "                        'Model Type for Color': 'Model Category',\n",
    "                        'Linguistic Competency': MAIN_TASK_COL},\n",
    "                text='bar_label', # Use the new combined label\n",
    "                facet_row='Linguistic Competency',\n",
    "                category_orders={\"Linguistic Competency\": competency_order_holmes},\n",
    "                facet_row_spacing=row_spacing_val_holmes\n",
    "            )\n",
    "            # Move text inside the bars and hide y-axis labels\n",
    "            fig_main_bar_holmes.update_traces(textposition='inside', insidetextanchor='middle')\n",
    "            model_order_y_main_holmes = [clean_plot_name(short_names.get(m,m)) for m in comparison_models if m in models_to_plot_bar_main_holmes]\n",
    "            fig_main_bar_holmes.update_yaxes(categoryorder='array', categoryarray=model_order_y_main_holmes, title=None, showticklabels=False, ticks=\"\")\n",
    "            fig_main_bar_holmes.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "\n",
    "            num_y_cats_main_holmes = len(model_order_y_main_holmes)\n",
    "            competencies_actually_plotted_holmes = [c for c in competency_order_holmes if c in main_bar_df_holmes['Linguistic Competency'].unique()]\n",
    "            num_total_facets_main_holmes = len(competencies_actually_plotted_holmes)\n",
    "\n",
    "            for facet_idx, comp_name in enumerate(competencies_actually_plotted_holmes):\n",
    "                instruct_s_val = summary_comp_df.loc[comp_name, instruct_model] if instruct_model in summary_comp_df.columns and comp_name in summary_comp_df.index and not pd.isna(summary_comp_df.loc[comp_name, instruct_model]) else np.nan\n",
    "                coder_s_val = summary_comp_df.loc[comp_name, coder_model] if coder_model in summary_comp_df.columns and comp_name in summary_comp_df.index and not pd.isna(summary_comp_df.loc[comp_name, coder_model]) else np.nan\n",
    "                \n",
    "                if pd.isna(instruct_s_val) or pd.isna(coder_s_val): continue\n",
    "\n",
    "                axis_suffix = num_total_facets_main_holmes - facet_idx\n",
    "                xaxis_ref = f'x{axis_suffix}' if axis_suffix > 1 else 'x'\n",
    "                yaxis_ref = f'y{axis_suffix}' if axis_suffix > 1 else 'y'\n",
    "                \n",
    "                fig_main_bar_holmes.add_shape(type=\"rect\", xref=xaxis_ref, yref=yaxis_ref, x0=min(instruct_s_val, coder_s_val), x1=max(instruct_s_val, coder_s_val), y0=-0.5, y1=num_y_cats_main_holmes - 0.5, fillcolor=\"rgba(128, 128, 128, 0.2)\", line_width=0, layer=\"below\")\n",
    "                fig_main_bar_holmes.add_shape(type=\"line\", xref=xaxis_ref, yref=yaxis_ref, x0=instruct_s_val, y0=-0.5, x1=instruct_s_val, y1=num_y_cats_main_holmes - 0.5, line=dict(color=color_map_main_bar_holmes.get('Qwen2.5 Instruct', 'green'), dash=\"dash\", width=2), layer=\"above\")\n",
    "                fig_main_bar_holmes.add_shape(type=\"line\", xref=xaxis_ref, yref=yaxis_ref, x0=coder_s_val, y0=-0.5, x1=coder_s_val, y1=num_y_cats_main_holmes - 0.5, line=dict(color=color_map_main_bar_holmes.get('Qwen2.5 Coder', 'orange'), dash=\"dash\", width=2), layer=\"above\")\n",
    "\n",
    "            fig_main_bar_holmes.update_layout(xaxis_showgrid=True, yaxis_showgrid=False, showlegend=False, **font_config)\n",
    "            plot_h_main_holmes = max(500, num_total_facets_main_holmes * (num_y_cats_main_holmes * 25 + 70))\n",
    "            compact_plot_width = 1000 # Use a more compact width\n",
    "            fig_main_bar_holmes.update_layout(height=plot_h_main_holmes, width=compact_plot_width, margin=dict(l=50, r=50, t=50, b=50))\n",
    "            fig_main_bar_holmes.show()\n",
    "            print(f\"Generated plot: {fig_main_bar_title_holmes} (Faceted by {MAIN_TASK_COL})\")\n",
    "            \n",
    "            # Export data\n",
    "            main_bar_df_holmes.to_csv(os.path.join(csv_export_dir, \"absolute_performance_main_tasks_bar.csv\"), index=False)\n",
    "            print(f\"Exported plot data to: {csv_export_dir}/absolute_performance_main_tasks_bar.csv\")\n",
    "    else:\n",
    "        print(f\"Skipping 'Absolute Performance by {MAIN_TASK_COL} (Bar Chart)': No models with data to plot.\")\n",
    "else:\n",
    "    print(f\"Skipping 'Absolute Performance by {MAIN_TASK_COL} (Bar Chart)': summary_comp_df is empty.\")\n",
    "\n",
    "# UPDATED Bar Chart for Linguistic Competency Groups\n",
    "if not subtasks_comp_df.empty and 'group' in subtasks_comp_df.columns:\n",
    "    print(f\"\\n--- Generating Bar Chart by {SUBTASK_GROUP_COL} (Mean Scores) ---\")\n",
    "    \n",
    "    models_to_plot_group_bar = [m for m in comparison_models if m in subtasks_comp_df.columns and not subtasks_comp_df[m].isna().all()]\n",
    "    \n",
    "    if models_to_plot_group_bar:\n",
    "        subtasks_filtered_for_group_plot = subtasks_comp_df[subtasks_comp_df['group'].astype(str).str.lower() != 'nan'].copy()\n",
    "        subtasks_filtered_for_group_plot.dropna(subset=models_to_plot_group_bar, how='all', inplace=True)\n",
    "\n",
    "        if not subtasks_filtered_for_group_plot.empty:\n",
    "            for model_col in models_to_plot_group_bar:\n",
    "                subtasks_filtered_for_group_plot[model_col] = pd.to_numeric(subtasks_filtered_for_group_plot[model_col], errors='coerce')\n",
    "            \n",
    "            grouped_scores_by_competency = subtasks_filtered_for_group_plot.groupby('group')[models_to_plot_group_bar].mean().reset_index()\n",
    "\n",
    "            plot_data_list_group_bar = []\n",
    "            competency_group_order = sorted(grouped_scores_by_competency['group'].astype(str).unique())\n",
    "\n",
    "            color_map_group_bar = {\n",
    "                \"Qwen2.5 Base\": 'rgb(100, 149, 237)', \"Qwen2.5 Instruct\": 'rgb(50, 205, 50)',\n",
    "                \"Qwen2.5 Coder\": 'rgb(255, 165, 0)', 'Merged': 'rgb(192, 192, 192)'\n",
    "            }\n",
    "\n",
    "            for comp_idx, competency_group_name in enumerate(competency_group_order):\n",
    "                group_data = grouped_scores_by_competency[grouped_scores_by_competency['group'] == competency_group_name]\n",
    "                if not group_data.empty:\n",
    "                    for model_idx, model_full_name in enumerate(comparison_models):\n",
    "                        if model_full_name in models_to_plot_group_bar and model_full_name in group_data.columns:\n",
    "                            score = group_data[model_full_name].iloc[0]\n",
    "                            if not pd.isna(score):\n",
    "                                model_short_clean = clean_plot_name(short_names.get(model_full_name, model_full_name))\n",
    "                                \n",
    "                                model_type_for_color = \"Merged\"\n",
    "                                if model_full_name == base_model: model_type_for_color = \"Qwen2.5 Base\"\n",
    "                                elif model_full_name == instruct_model: model_type_for_color = \"Qwen2.5 Instruct\"\n",
    "                                elif model_full_name == coder_model: model_type_for_color = \"Qwen2.5 Coder\"\n",
    "                                \n",
    "                                plot_data_list_group_bar.append({\n",
    "                                    'Linguistic Competency Group': competency_group_name,\n",
    "                                    'Model Short Name': model_short_clean,\n",
    "                                    'Mean Score': score,\n",
    "                                    'Model Type for Color': model_type_for_color,\n",
    "                                    'Competency Index': comp_idx,\n",
    "                                    'Model Index': model_idx\n",
    "                                })\n",
    "            \n",
    "            if not plot_data_list_group_bar:\n",
    "                print(f\"Skipping 'Absolute Performance by {SUBTASK_GROUP_COL} (Bar Chart)': No data to plot after processing.\")\n",
    "            else:\n",
    "                group_bar_df = pd.DataFrame(plot_data_list_group_bar)\n",
    "                group_bar_df.sort_values(by=['Competency Index', 'Model Index'], ascending=[True, True], inplace=True)\n",
    "                \n",
    "                # Create a new column for the text inside the bar\n",
    "                group_bar_df['bar_label'] = group_bar_df.apply(\n",
    "                    lambda row: f\"{row['Model Short Name']}: {row['Mean Score']:.2f}%\", axis=1\n",
    "                )\n",
    "\n",
    "                fig_group_bar_title = f'Mean Absolute Performance by {SUBTASK_GROUP_COL}'\n",
    "                \n",
    "                num_competency_groups = group_bar_df['Linguistic Competency Group'].nunique()\n",
    "                row_spacing_group_bar = 0.02\n",
    "                if num_competency_groups > 1:\n",
    "                    max_allowed_spacing_gb = 1.0 / (num_competency_groups - 1) if (num_competency_groups - 1) > 0 else 1.0\n",
    "                    row_spacing_group_bar = min(0.03, max_allowed_spacing_gb * 0.9)\n",
    "\n",
    "                fig_group_bar = px.bar(\n",
    "                    group_bar_df, x='Mean Score', y='Model Short Name',\n",
    "                    color='Model Type for Color', color_discrete_map=color_map_group_bar,\n",
    "                    orientation='h', title=fig_group_bar_title,\n",
    "                    labels={'Mean Score': 'Mean Performance Score (%)', 'Model Short Name': '',\n",
    "                            'Model Type for Color': 'Model Category',\n",
    "                            'Linguistic Competency Group': SUBTASK_GROUP_COL},\n",
    "                    text='bar_label', # Use new combined label\n",
    "                    facet_row='Linguistic Competency Group',\n",
    "                    category_orders={\"Linguistic Competency Group\": competency_group_order},\n",
    "                    facet_row_spacing=row_spacing_group_bar\n",
    "                )\n",
    "                fig_group_bar.update_traces(textposition='inside', insidetextanchor='middle')\n",
    "                model_order_y_group = [clean_plot_name(short_names.get(m,m)) for m in comparison_models if m in models_to_plot_group_bar]\n",
    "                fig_group_bar.update_yaxes(categoryorder='array', categoryarray=model_order_y_group, title=None, showticklabels=False, ticks=\"\")\n",
    "                fig_group_bar.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "\n",
    "                num_y_cats_group = len(model_order_y_group)\n",
    "                num_total_facets_group = len(competency_group_order)\n",
    "\n",
    "                for facet_idx, comp_group_name in enumerate(competency_group_order):\n",
    "                    instruct_s_val_group_df = grouped_scores_by_competency[grouped_scores_by_competency['group'] == comp_group_name]\n",
    "                    instruct_s_val_group = instruct_s_val_group_df[instruct_model].iloc[0] if instruct_model in instruct_s_val_group_df.columns and not instruct_s_val_group_df[instruct_model].empty and not pd.isna(instruct_s_val_group_df[instruct_model].iloc[0]) else np.nan\n",
    "                    coder_s_val_group = instruct_s_val_group_df[coder_model].iloc[0] if coder_model in instruct_s_val_group_df.columns and not instruct_s_val_group_df[coder_model].empty and not pd.isna(instruct_s_val_group_df[coder_model].iloc[0]) else np.nan\n",
    "                    \n",
    "                    if pd.isna(instruct_s_val_group) or pd.isna(coder_s_val_group): continue\n",
    "\n",
    "                    axis_suffix_group = num_total_facets_group - facet_idx\n",
    "                    xaxis_ref_group = f'x{axis_suffix_group}' if axis_suffix_group > 1 else 'x'\n",
    "                    yaxis_ref_group = f'y{axis_suffix_group}' if axis_suffix_group > 1 else 'y'\n",
    "\n",
    "                    fig_group_bar.add_shape(type=\"rect\", xref=xaxis_ref_group, yref=yaxis_ref_group, x0=min(instruct_s_val_group, coder_s_val_group), x1=max(instruct_s_val_group, coder_s_val_group), y0=-0.5, y1=num_y_cats_group - 0.5, fillcolor=\"rgba(255, 128, 128, 0.2)\", line_width=0, layer=\"below\")\n",
    "                    fig_group_bar.add_shape(type=\"line\", xref=xaxis_ref_group, yref=yaxis_ref_group, x0=instruct_s_val_group, y0=-0.5, x1=instruct_s_val_group, y1=num_y_cats_group - 0.5, line=dict(color=color_map_group_bar.get('Qwen2.5 Instruct', 'green'), dash=\"dash\", width=2), layer=\"above\")\n",
    "                    fig_group_bar.add_shape(type=\"line\", xref=xaxis_ref_group, yref=yaxis_ref_group, x0=coder_s_val_group, y0=-0.5, x1=coder_s_val_group, y1=num_y_cats_group - 0.5, line=dict(color=color_map_group_bar.get('Qwen2.5 Coder', 'orange'), dash=\"dash\", width=2), layer=\"above\")\n",
    "                \n",
    "                fig_group_bar.update_layout(xaxis_showgrid=True, yaxis_showgrid=False, showlegend=False, **font_config)\n",
    "                plot_h_group = max(420, num_total_facets_group * (num_y_cats_group * 25 + 75))\n",
    "                compact_plot_width = 1000 # Use a more compact width\n",
    "                fig_group_bar.update_layout(height=plot_h_group, width=compact_plot_width, margin=dict(l=50, r=50, t=50, b=50))\n",
    "                fig_group_bar.show()\n",
    "                print(f\"Generated plot: {fig_group_bar_title} (Faceted by {SUBTASK_GROUP_COL})\")\n",
    "                \n",
    "                # Export data\n",
    "                group_bar_df.to_csv(os.path.join(csv_export_dir, \"mean_performance_by_competency_groups_bar.csv\"), index=False)\n",
    "                print(f\"Exported plot data to: {csv_export_dir}/mean_performance_by_competency_groups_bar.csv\")\n",
    "        else:\n",
    "            print(f\"Skipping 'Absolute Performance by {SUBTASK_GROUP_COL} (Bar Chart)': No data to plot after grouping.\")\n",
    "    else:\n",
    "        print(f\"Skipping 'Absolute Performance by {SUBTASK_GROUP_COL} (Bar Chart)': No models with data to plot.\")\n",
    "else:\n",
    "    print(f\"Skipping 'Absolute Performance by {SUBTASK_GROUP_COL} (Bar Chart)': subtasks_comp_df is empty or 'group' column missing.\")\n",
    "\n",
    "# Plot 2: Subtask Difference Boxplots\n",
    "plot_dcoder_col_sub = 'd_coder' if 'd_coder' in diff_cols_subtasks and not subtasks_comp_df.empty and 'd_coder' in subtasks_comp_df.columns else None\n",
    "if not subtasks_comp_df.empty and 'group' in subtasks_comp_df.columns and instruct_model in subtasks_comp_df.columns and can_calc_diffs :\n",
    "    plot_title_prefix = \"Subtask Difference Boxplots\"\n",
    "    print(f\"Generating {plot_title_prefix}: Subtask Difference Boxplot(s) grouped by '{SUBTASK_GROUP_COL}' (excluding 'nan' group)...\")\n",
    "    merged_diff_cols_sub_present = [col for col in diff_cols_subtasks if col.startswith('d_merged_') and col in subtasks_comp_df.columns and not subtasks_comp_df[col].isna().all()]\n",
    "\n",
    "    if plot_dcoder_col_sub and not subtasks_comp_df[plot_dcoder_col_sub].isna().all():\n",
    "        if not merged_diff_cols_sub_present :\n",
    "            plot_data_box_ic_only = subtasks_comp_df.melt(id_vars=['group', 'subtask_cleaned'], value_vars=[plot_dcoder_col_sub], var_name='difference_type', value_name='difference_value')\n",
    "            plot_data_box_ic_only.dropna(subset=['difference_value'], inplace=True)\n",
    "            plot_data_box_ic_only = plot_data_box_ic_only[plot_data_box_ic_only['group'].astype(str).str.lower() != 'nan']\n",
    "            if not plot_data_box_ic_only.empty:\n",
    "                label_map_ic_only = {plot_dcoder_col_sub: f'{instruct_short_label}–{coder_short_label}'}\n",
    "                plot_data_box_ic_only['difference_label'] = plot_data_box_ic_only['difference_type'].map(label_map_ic_only)\n",
    "                fig2_ic_only_title = f'{plot_title_prefix} ({SUBTASK_GROUP_COL}): {instruct_short_label}–{coder_short_label}'\n",
    "                fig2_ic_only = px.box(plot_data_box_ic_only, x='group', y='difference_value', color='difference_label', hover_data=['subtask_cleaned'], labels={'group': SUBTASK_GROUP_COL, 'difference_value': 'Difference (%)', 'difference_label': 'Difference Type', 'subtask_cleaned': SUBTASK_COL}, title=fig2_ic_only_title, category_orders={\"group\": sorted(plot_data_box_ic_only['group'].astype(str).unique())})\n",
    "                fig2_ic_only.update_xaxes(tickangle=45); fig2_ic_only.update_layout(height=default_plot_height, width=default_plot_width, **font_config); fig2_ic_only.show();\n",
    "                print(f\"  - Generated plot: {fig2_ic_only_title}\")\n",
    "                \n",
    "                # Export data\n",
    "                plot_data_box_ic_only.to_csv(os.path.join(csv_export_dir, \"subtask_difference_boxplot_instruct_coder_only.csv\"), index=False)\n",
    "                print(f\"Exported plot data to: {csv_export_dir}/subtask_difference_boxplot_instruct_coder_only.csv\")\n",
    "            else: print(\"  - No valid data for Instruct-Coder difference boxplot (after 'nan' group filter or all values were NaN).\")\n",
    "        else:\n",
    "            for merged_diff_col in merged_diff_cols_sub_present:\n",
    "                original_merged_short_name = merged_diff_col.replace('d_merged_', '')\n",
    "                cleaned_merged_short_name_plot = clean_plot_name(original_merged_short_name)\n",
    "                cols_for_this_plot = [plot_dcoder_col_sub, merged_diff_col]\n",
    "                plot_data_box_single = subtasks_comp_df.melt(id_vars=['group', 'subtask_cleaned'], value_vars=cols_for_this_plot, var_name='difference_type', value_name='difference_value')\n",
    "                plot_data_box_single.dropna(subset=['difference_value'], inplace=True)\n",
    "                plot_data_box_single = plot_data_box_single[plot_data_box_single['group'].astype(str).str.lower() != 'nan']\n",
    "                if not plot_data_box_single.empty:\n",
    "                    label_map = { col: (f'{instruct_short_label}–{coder_short_label}' if col == plot_dcoder_col_sub else f'{instruct_short_label}–{clean_plot_name(col.replace(\"d_merged_\",\"\"))}') for col in cols_for_this_plot}\n",
    "                    plot_data_box_single['difference_label'] = plot_data_box_single['difference_type'].map(label_map)\n",
    "                    fig2_single_title = f'{plot_title_prefix} ({SUBTASK_GROUP_COL}): {instruct_short_label}–{coder_short_label} vs {instruct_short_label}–{cleaned_merged_short_name_plot}'\n",
    "                    fig2_single = px.box(plot_data_box_single, x='group', y='difference_value', color='difference_label', hover_data=['subtask_cleaned'], labels={'group': SUBTASK_GROUP_COL, 'difference_value': 'Difference (%)', 'difference_label': 'Difference Type', 'subtask_cleaned': SUBTASK_COL}, title=fig2_single_title, category_orders={\"group\": sorted(plot_data_box_single['group'].astype(str).unique())})\n",
    "                    fig2_single.update_xaxes(tickangle=45); fig2_single.update_layout(boxmode='group', height=default_plot_height, width=default_plot_width, **font_config); fig2_single.show();\n",
    "                    print(f\"  - Generated plot: {fig2_single_title}\")\n",
    "                    \n",
    "                    # Export data\n",
    "                    plot_data_box_single.to_csv(os.path.join(csv_export_dir, f\"subtask_difference_boxplot_{cleaned_merged_short_name_plot.lower().replace(' ', '_')}.csv\"), index=False)\n",
    "                    print(f\"Exported plot data to: {csv_export_dir}/subtask_difference_boxplot_{cleaned_merged_short_name_plot.lower().replace(' ', '_')}.csv\")\n",
    "                else: print(f\"  - No valid data for difference boxplot including {cleaned_merged_short_name_plot} (after 'nan' group filter or all values were NaN).\")\n",
    "    else: print(f\"Skipping {plot_title_prefix} details: 'd_coder' for subtasks not available/all NaN, or merged diffs not available.\")\n",
    "elif not subtasks_comp_df.empty and 'group' not in subtasks_comp_df.columns: print(f\"Skipping Subtask Difference Boxplots: Group column ('{SUBTASK_GROUP_COL}') missing.\")\n",
    "elif not can_calc_diffs: print(\"Skipping Subtask Difference Boxplots: Difference calculation disabled.\")\n",
    "else: print(f\"Skipping Subtask Difference Boxplots: Other unmet conditions (subtasks_empty: {subtasks_comp_df.empty}, instruct_in_subtasks: {instruct_model in subtasks_comp_df.columns if instruct_model and not subtasks_comp_df.empty else 'N/A'}).\")\n",
    "\n",
    "# Plot 3: Absolute Score Boxplots\n",
    "base_abs_models_plot = [m for m in [base_model, instruct_model, coder_model] if m and not subtasks_comp_df.empty and m in subtasks_comp_df.columns and not subtasks_comp_df[m].isna().all()]\n",
    "if not subtasks_comp_df.empty and 'group' in subtasks_comp_df.columns and base_abs_models_plot:\n",
    "    plot_title_prefix_abs = \"Absolute Score Boxplots\"\n",
    "    print(f\"Generating {plot_title_prefix_abs}: Absolute Score Boxplot(s) grouped by '{SUBTASK_GROUP_COL}' (excluding 'nan' group)...\")\n",
    "    present_merged_subtasks = [m for m in merged_models if m in subtasks_comp_df.columns and not subtasks_comp_df[m].isna().all()]\n",
    "\n",
    "    if not present_merged_subtasks :\n",
    "        plot_data_abs_base_only = subtasks_comp_df.melt(id_vars=['group', 'subtask_cleaned'], value_vars=base_abs_models_plot, var_name='model_full_name', value_name='score')\n",
    "        plot_data_abs_base_only.dropna(subset=['score'], inplace=True)\n",
    "        plot_data_abs_base_only = plot_data_abs_base_only[plot_data_abs_base_only['group'].astype(str).str.lower() != 'nan']\n",
    "        if not plot_data_abs_base_only.empty:\n",
    "            plot_data_abs_base_only['model_short_name'] = plot_data_abs_base_only['model_full_name'].map(lambda x: clean_plot_name(short_names.get(x,x)))\n",
    "            fig_abs_base_title = f'{plot_title_prefix_abs} ({SUBTASK_GROUP_COL}): Base, Instruct & Coder'\n",
    "            fig_abs_base = px.box(plot_data_abs_base_only, x='group', y='score', color='model_short_name', hover_data=['subtask_cleaned'], labels={'group': SUBTASK_GROUP_COL, 'score': 'Absolute Score (%)', 'model_short_name': 'Model', 'subtask_cleaned': SUBTASK_COL}, title=fig_abs_base_title, category_orders={\"group\": sorted(plot_data_abs_base_only['group'].astype(str).unique())})\n",
    "            fig_abs_base.update_xaxes(tickangle=45); fig_abs_base.update_layout(boxmode='group', height=default_plot_height, width=default_plot_width, **font_config); fig_abs_base.show();\n",
    "            print(f\"  - Generated plot: {fig_abs_base_title}\")\n",
    "            \n",
    "            # Export data\n",
    "            plot_data_abs_base_only.to_csv(os.path.join(csv_export_dir, \"absolute_score_boxplot_base_instruct_coder.csv\"), index=False)\n",
    "            print(f\"Exported plot data to: {csv_export_dir}/absolute_score_boxplot_base_instruct_coder.csv\")\n",
    "        else: print(\"  - No valid data for Base/Instruct/Coder absolute score boxplot (after 'nan' group filter or all scores were NaN).\")\n",
    "    else:\n",
    "        for merged_m_plot in present_merged_subtasks:\n",
    "            cleaned_merged_short_name_plot = clean_plot_name(short_names.get(merged_m_plot, merged_m_plot))\n",
    "            models_for_this_plot = base_abs_models_plot + [merged_m_plot]\n",
    "            plot_data_abs_single = subtasks_comp_df.melt(id_vars=['group', 'subtask_cleaned'], value_vars=models_for_this_plot, var_name='model_full_name', value_name='score')\n",
    "            plot_data_abs_single.dropna(subset=['score'], inplace=True)\n",
    "            plot_data_abs_single = plot_data_abs_single[plot_data_abs_single['group'].astype(str).str.lower() != 'nan']\n",
    "            if not plot_data_abs_single.empty:\n",
    "                plot_data_abs_single['model_short_name'] = plot_data_abs_single['model_full_name'].map(lambda x: clean_plot_name(short_names.get(x,x)))\n",
    "                fig_abs_single_title = f'{plot_title_prefix_abs} ({SUBTASK_GROUP_COL}): Incl. {cleaned_merged_short_name_plot}'\n",
    "                fig_abs_single = px.box(plot_data_abs_single, x='group', y='score', color='model_short_name', hover_data=['subtask_cleaned'], labels={'group': SUBTASK_GROUP_COL, 'score': 'Absolute Score (%)', 'model_short_name': 'Model', 'subtask_cleaned': SUBTASK_COL}, title=fig_abs_single_title, category_orders={\"group\": sorted(plot_data_abs_single['group'].astype(str).unique())})\n",
    "                fig_abs_single.update_xaxes(tickangle=45); fig_abs_single.update_layout(boxmode='group', height=default_plot_height, width=default_plot_width, **font_config); fig_abs_single.show();\n",
    "                print(f\"  - Generated plot: {fig_abs_single_title}\")\n",
    "                \n",
    "                # Export data\n",
    "                plot_data_abs_single.to_csv(os.path.join(csv_export_dir, f\"absolute_score_boxplot_incl_{cleaned_merged_short_name_plot.lower().replace(' ', '_')}.csv\"), index=False)\n",
    "                print(f\"Exported plot data to: {csv_export_dir}/absolute_score_boxplot_incl_{cleaned_merged_short_name_plot.lower().replace(' ', '_')}.csv\")\n",
    "            else: print(f\"  - No valid data for absolute score boxplot including {cleaned_merged_short_name_plot} (after 'nan' group filter or all scores were NaN).\")\n",
    "elif not subtasks_comp_df.empty and 'group' not in subtasks_comp_df.columns: print(f\"Skipping Absolute Score Boxplots: Group column ('{SUBTASK_GROUP_COL}') missing.\")\n",
    "else: print(f\"Skipping Absolute Score Boxplots: Other unmet conditions (subtasks_empty: {subtasks_comp_df.empty}, base_abs_models_plot_empty: {not base_abs_models_plot}).\")\n",
    "\n",
    "# Plot 4/5: Jointplot/Scatter for Subtask Differences\n",
    "plot_dcoder_col_sub = 'd_coder' if 'd_coder' in diff_cols_subtasks and not subtasks_comp_df.empty and 'd_coder' in subtasks_comp_df.columns and not subtasks_comp_df['d_coder'].isna().all() else None\n",
    "plot_merged_diff_cols_sub_valid = [c for c in diff_cols_subtasks if not subtasks_comp_df.empty and c.startswith('d_merged_') and c in subtasks_comp_df.columns and not subtasks_comp_df[c].isna().all()]\n",
    "if not subtasks_comp_df.empty and plot_dcoder_col_sub and plot_merged_diff_cols_sub_valid and can_calc_diffs and 'group' in subtasks_comp_df.columns and 'subtask_cleaned' in subtasks_comp_df.columns:\n",
    "    print(\"Generating Jointplot/Scatter(s) for Subtask Differences (I-C vs I-MergedX) (excluding 'nan' group)...\")\n",
    "    for merged_diff_col in plot_merged_diff_cols_sub_valid:\n",
    "        original_merged_short_name = merged_diff_col.replace('d_merged_', '')\n",
    "        cleaned_merged_short_name_plot = clean_plot_name(original_merged_short_name)\n",
    "        required_cols_joint = [plot_dcoder_col_sub, merged_diff_col, 'subtask_cleaned', 'group']\n",
    "        plot_data_joint = subtasks_comp_df[required_cols_joint].copy()\n",
    "        plot_data_joint.dropna(subset=[plot_dcoder_col_sub, merged_diff_col], inplace=True)\n",
    "        plot_data_joint = plot_data_joint[plot_data_joint['group'].astype(str).str.lower() != 'nan']\n",
    "        if not plot_data_joint.empty:\n",
    "            plot_data_joint['hover_name_joint'] = plot_data_joint['subtask_cleaned'] + \" (Group: \" + plot_data_joint['group'].astype(str) + \")\"\n",
    "            title_joint = f'Joint Dist: ({instruct_short_label}-{coder_short_label}) vs ({instruct_short_label}-{cleaned_merged_short_name_plot}) Subtask Diffs'; labels_joint = {plot_dcoder_col_sub: f'{instruct_short_label}–{coder_short_label} Diff (%)', merged_diff_col: f'{instruct_short_label}–{cleaned_merged_short_name_plot} Diff (%)'}\n",
    "\n",
    "            fig_joint = px.scatter(plot_data_joint, x=plot_dcoder_col_sub, y=merged_diff_col, marginal_x=\"histogram\", marginal_y=\"histogram\", trendline=\"ols\", hover_name='hover_name_joint', hover_data={plot_dcoder_col_sub:':.2f', merged_diff_col:':.2f', 'group':True, 'subtask_cleaned':False, 'hover_name_joint': False}, labels=labels_joint, title=title_joint); fig_joint.update_layout(height=default_plot_height, width=default_plot_width, **font_config); fig_joint.show();\n",
    "            print(f\"  - Generated Jointplot for {cleaned_merged_short_name_plot}\")\n",
    "\n",
    "            fig_scatter_comp = px.scatter(plot_data_joint, x=plot_dcoder_col_sub, y=merged_diff_col, color='group', trendline=\"ols\", hover_name='subtask_cleaned', hover_data={'group':True, plot_dcoder_col_sub:':.2f', merged_diff_col:':.2f'}, labels=labels_joint, title=title_joint.replace(\"Joint Dist\", \"Scatter Comp\"), color_discrete_map={'nan': 'rgba(0,0,0,0)'})\n",
    "            fig_scatter_comp.update_layout(height=default_plot_height, width=default_plot_width, **font_config); fig_scatter_comp.show();\n",
    "            print(f\"  - Generated Scatter Plot for {cleaned_merged_short_name_plot}\")\n",
    "            \n",
    "            # Export data for both plots (same data)\n",
    "            plot_data_joint.to_csv(os.path.join(csv_export_dir, f\"jointplot_scatter_{cleaned_merged_short_name_plot.lower().replace(' ', '_')}_vs_coder.csv\"), index=False)\n",
    "            print(f\"Exported plot data to: {csv_export_dir}/jointplot_scatter_{cleaned_merged_short_name_plot.lower().replace(' ', '_')}_vs_coder.csv\")\n",
    "        else: print(f\"  - No valid data for Jointplot/Scatter for {cleaned_merged_short_name_plot} after NaN drop and 'nan' group filter.\")\n",
    "else: print(f\"Skipping Jointplot/Scatter plots: Conditions not met.\")\n",
    "\n",
    "# Plot 6: Top/Bottom Subtask Impact Plot\n",
    "if not subtasks_comp_df.empty and plot_dcoder_col_sub and plot_merged_diff_cols_sub_valid and 'subtask_cleaned' in subtasks_comp_df.columns and can_calc_diffs and 'group' in subtasks_comp_df.columns:\n",
    "    plot_title_prefix_impact = \"Top/Bottom Subtask Impact\"\n",
    "    print(f\"Generating {plot_title_prefix_impact} Plot(s) (excluding 'nan' group)...\"); N_top_bottom = 5\n",
    "    for merged_diff_col in plot_merged_diff_cols_sub_valid:\n",
    "        original_merged_short_name = merged_diff_col.replace('d_merged_', '')\n",
    "        cleaned_merged_short_name_plot = clean_plot_name(original_merged_short_name)\n",
    "        impact_col_name = f'impact_{cleaned_merged_short_name_plot}'\n",
    "        temp_df_impact = subtasks_comp_df.copy()\n",
    "        if 'group' in temp_df_impact.columns:\n",
    "            temp_df_impact = temp_df_impact[temp_df_impact['group'].astype(str).str.lower() != 'nan']\n",
    "\n",
    "        if temp_df_impact.empty:\n",
    "            print(f\"  - Skipping Impact Bar Plot for {cleaned_merged_short_name_plot} as data is empty after 'nan' group filter.\")\n",
    "            continue\n",
    "\n",
    "        temp_df_impact[merged_diff_col] = pd.to_numeric(temp_df_impact[merged_diff_col], errors='coerce')\n",
    "        temp_df_impact[plot_dcoder_col_sub] = pd.to_numeric(temp_df_impact[plot_dcoder_col_sub], errors='coerce')\n",
    "\n",
    "        temp_df_impact[impact_col_name] = temp_df_impact[plot_dcoder_col_sub] - temp_df_impact[merged_diff_col]\n",
    "        subtasks_sorted_impact = temp_df_impact.dropna(subset=[impact_col_name]).sort_values(impact_col_name)\n",
    "\n",
    "        if len(subtasks_sorted_impact) >= N_top_bottom * 2 :\n",
    "            top_n_impact = subtasks_sorted_impact.nlargest(N_top_bottom, impact_col_name)\n",
    "            bottom_n_impact = subtasks_sorted_impact.nsmallest(N_top_bottom, impact_col_name)\n",
    "            plot_data_bar_impact = pd.concat([top_n_impact, bottom_n_impact]).drop_duplicates(subset=['subtask_cleaned'])\n",
    "            if not plot_data_bar_impact.empty:\n",
    "                hover_cols_bar = {'subtask_cleaned': False, 'group': True, impact_col_name: ':.2f', plot_dcoder_col_sub: ':.2f', merged_diff_col: ':.2f'};\n",
    "                hover_cols_bar_present = {k:v for k,v in hover_cols_bar.items() if k in plot_data_bar_impact.columns or k in [plot_dcoder_col_sub, merged_diff_col]}\n",
    "                labels_bar = {'subtask_cleaned': SUBTASK_COL, 'group':SUBTASK_GROUP_COL, impact_col_name: f'Impact ({cleaned_merged_short_name_plot} vs {coder_short_label}) Rel. to {instruct_short_label} (%)'}\n",
    "                fig_bar_impact_title = f'{plot_title_prefix_impact}: Top/Bottom {N_top_bottom} Subtasks - Rel. Impact of {cleaned_merged_short_name_plot} vs {coder_short_label}'\n",
    "                fig_bar_impact = px.bar(plot_data_bar_impact, x=impact_col_name, y='subtask_cleaned', orientation='h', color=impact_col_name, color_continuous_scale=px.colors.diverging.RdBu, color_continuous_midpoint=0, hover_data=hover_cols_bar_present, labels=labels_bar, title=fig_bar_impact_title)\n",
    "                fig_bar_impact.update_layout(yaxis={'categoryorder':'total ascending'}, height=default_plot_height, width=default_plot_width, **font_config); fig_bar_impact.show();\n",
    "                print(f\"  - Generated plot: {fig_bar_impact_title}\")\n",
    "                \n",
    "                # Export data\n",
    "                plot_data_bar_impact.to_csv(os.path.join(csv_export_dir, f\"top_bottom_impact_{cleaned_merged_short_name_plot.lower().replace(' ', '_')}.csv\"), index=False)\n",
    "                print(f\"Exported plot data to: {csv_export_dir}/top_bottom_impact_{cleaned_merged_short_name_plot.lower().replace(' ', '_')}.csv\")\n",
    "            else: print(f\"  - No data for Impact Bar Plot for {cleaned_merged_short_name_plot} after selecting top/bottom.\")\n",
    "        else: print(f\"  - Not enough data for Top/Bottom {N_top_bottom} Impact plot for {cleaned_merged_short_name_plot}. Found {len(subtasks_sorted_impact)} after filtering and NaN drop.\")\n",
    "else: print(f\"Skipping Top/Bottom Subtask Impact Plot(s): Conditions not met.\")\n",
    "\n",
    "# Plot 7: Clustermap/Dendrogram for Subtasks\n",
    "if (not subtasks_comp_df.empty and 'subtask_cleaned' in subtasks_comp_df.columns and\n",
    "    plot_dcoder_col_sub and plot_merged_diff_cols_sub_valid and can_calc_diffs):\n",
    "    plot_title_prefix_cluster = \"Clustered Heatmap/Dendrogram\"\n",
    "    print(f\"Generating {plot_title_prefix_cluster}(s) for Subtasks (I-C vs I-MergedX) (excluding 'nan' group)...\")\n",
    "    for merged_diff_col in plot_merged_diff_cols_sub_valid:\n",
    "        original_merged_short_name = merged_diff_col.replace('d_merged_', '')\n",
    "        cleaned_merged_short_name_plot = clean_plot_name(original_merged_short_name)\n",
    "        cluster_cols_single = [plot_dcoder_col_sub, merged_diff_col]\n",
    "\n",
    "        plot7_data_source_df = subtasks_comp_df.copy()\n",
    "        if 'group' in plot7_data_source_df.columns:\n",
    "            plot7_data_source_df = plot7_data_source_df[plot7_data_source_df['group'].astype(str).str.lower() != 'nan']\n",
    "\n",
    "        if plot7_data_source_df.empty:\n",
    "            print(f\"  - Skipping clustering for {cleaned_merged_short_name_plot} as data is empty after 'nan' group filter.\")\n",
    "            continue\n",
    "\n",
    "        diff_matrix_single_prep = plot7_data_source_df.set_index('subtask_cleaned')[cluster_cols_single].copy()\n",
    "        for col in cluster_cols_single: diff_matrix_single_prep[col] = pd.to_numeric(diff_matrix_single_prep[col], errors='coerce')\n",
    "        diff_matrix_single = diff_matrix_single_prep.dropna(how='any')\n",
    "\n",
    "        if len(diff_matrix_single) > 1:\n",
    "            try:\n",
    "                scaler = StandardScaler(); scaled_data_single = scaler.fit_transform(diff_matrix_single.values)\n",
    "                if np.any(np.std(scaled_data_single, axis=0) < 1e-9):\n",
    "                    print(f\"  - Skipping clustering for {cleaned_merged_short_name_plot} due to near-zero standard deviation in one or more dimensions after scaling (possibly identical values).\"); continue\n",
    "                row_linkage_single = linkage(pdist(scaled_data_single), method='average', metric='euclidean')\n",
    "                ordered_row_indices_single = leaves_list(row_linkage_single)\n",
    "                heatmap_data_ordered_single = scaled_data_single[ordered_row_indices_single]\n",
    "                ordered_row_labels_single = diff_matrix_single.index[ordered_row_indices_single].tolist()\n",
    "                heatmap_col_labels_single = [f'{instruct_short_label}-{coder_short_label}', f'{instruct_short_label}-{cleaned_merged_short_name_plot}']\n",
    "\n",
    "                fig_heatmap_single_title = f'{plot_title_prefix_cluster}: {cleaned_merged_short_name_plot} Profile (Scaled)'\n",
    "                fig_heatmap_single = px.imshow(heatmap_data_ordered_single, labels=dict(x=\"Difference Type (vs Instruct)\", y=SUBTASK_COL, color=\"Scaled Value\"), x=heatmap_col_labels_single, y=ordered_row_labels_single, aspect=\"auto\", color_continuous_scale='RdBu_r', title=fig_heatmap_single_title)\n",
    "                fig_heatmap_single.update_xaxes(side=\"top\"); fig_heatmap_single.update_layout(height=max(default_plot_height, 20*len(ordered_row_labels_single)), width=default_plot_width, **font_config); fig_heatmap_single.show();\n",
    "                print(f\"  - Generated plot: {fig_heatmap_single_title}\")\n",
    "\n",
    "                fig_dendro_row_single_title = f'{plot_title_prefix_cluster}: Row Dendrogram - {cleaned_merged_short_name_plot} Profile (Scaled Subtask Differences)'\n",
    "                fig_dendro_row_single = ff.create_dendrogram(scaled_data_single, orientation='right', labels=diff_matrix_single.index.tolist(), linkagefun=lambda x: linkage(x, method='average', metric='euclidean'))\n",
    "                fig_dendro_row_single.update_layout(title=fig_dendro_row_single_title, height=max(default_plot_height, 20*len(diff_matrix_single)), width=default_plot_width, **font_config); fig_dendro_row_single.show();\n",
    "                print(f\"  - Generated plot: {fig_dendro_row_single_title}\")\n",
    "                \n",
    "                # Export clustering data\n",
    "                clustering_data = pd.DataFrame(heatmap_data_ordered_single, \n",
    "                                               columns=heatmap_col_labels_single, \n",
    "                                               index=ordered_row_labels_single)\n",
    "                clustering_data.to_csv(os.path.join(csv_export_dir, f\"clustering_heatmap_{cleaned_merged_short_name_plot.lower().replace(' ', '_')}.csv\"))\n",
    "                print(f\"Exported clustering data to: {csv_export_dir}/clustering_heatmap_{cleaned_merged_short_name_plot.lower().replace(' ', '_')}.csv\")\n",
    "                \n",
    "                # Export original data used for clustering\n",
    "                diff_matrix_single.to_csv(os.path.join(csv_export_dir, f\"clustering_original_data_{cleaned_merged_short_name_plot.lower().replace(' ', '_')}.csv\"))\n",
    "                print(f\"Exported original clustering data to: {csv_export_dir}/clustering_original_data_{cleaned_merged_short_name_plot.lower().replace(' ', '_')}.csv\")\n",
    "            except Exception as e: print(f\"Error clustering/plotting for {cleaned_merged_short_name_plot}: {e}\")\n",
    "        else: print(f\"  - Not enough data points (>1) for {cleaned_merged_short_name_plot} for clustering (after NaN drop and 'nan' group filter).\")\n",
    "else: print(f\"Skipping Clustered Heatmap/Dendrogram plots: Conditions not met.\")\n",
    "\n",
    "# Plot 8a: Main Task Dendrograms\n",
    "plot_dcoder_col_main = 'd_coder' if not summary_comp_df.empty and 'd_coder' in summary_comp_df.columns and not summary_comp_df['d_coder'].isna().all() else None\n",
    "plot_merged_diff_cols_main_valid = [c for c in diff_cols_main if not summary_comp_df.empty and c.startswith('d_merged_') and c in summary_comp_df.columns and not summary_comp_df[c].isna().all()]\n",
    "if plot_dcoder_col_main and plot_merged_diff_cols_main_valid and not summary_comp_df.empty and can_calc_diffs:\n",
    "    plot_title_prefix_dendro_main = \"Main Task Dendrogram\"\n",
    "    print(f\"Generating {plot_title_prefix_dendro_main}(s) (I-C vs I-MergedX)...\")\n",
    "    for merged_diff_col_main in plot_merged_diff_cols_main_valid:\n",
    "        original_merged_short_name = merged_diff_col_main.replace('d_merged_', '')\n",
    "        cleaned_merged_short_name_plot = clean_plot_name(original_merged_short_name)\n",
    "        cols_main_single = [plot_dcoder_col_main, merged_diff_col_main]\n",
    "        main_matrix_data_single_prep = summary_comp_df[cols_main_single].copy()\n",
    "        for col in cols_main_single: main_matrix_data_single_prep[col] = pd.to_numeric(main_matrix_data_single_prep[col], errors='coerce')\n",
    "        main_matrix_data_single = main_matrix_data_single_prep.dropna(how='any')\n",
    "\n",
    "        if len(main_matrix_data_single) >= 2:\n",
    "            try:\n",
    "                fig_dendro_main_s_title = f'{plot_title_prefix_dendro_main}: {xaxis_title_main_tasks} based on {cleaned_merged_short_name_plot} Profile'\n",
    "                fig_dendro_main_s = ff.create_dendrogram(main_matrix_data_single.values, labels=main_matrix_data_single.index.tolist(), linkagefun=lambda x: linkage(x, method='ward'))\n",
    "                dynamic_width_dendro_main = max(default_plot_width, 30 * len(main_matrix_data_single.index))\n",
    "                fig_dendro_main_s.update_layout(title=fig_dendro_main_s_title, yaxis_title='Distance', xaxis_title='Task', width=dynamic_width_dendro_main, height=default_plot_height, **font_config); fig_dendro_main_s.show();\n",
    "                print(f\"  - Generated plot: {fig_dendro_main_s_title}\")\n",
    "                \n",
    "                # Export dendrogram data\n",
    "                main_matrix_data_single.to_csv(os.path.join(csv_export_dir, f\"main_task_dendrogram_{cleaned_merged_short_name_plot.lower().replace(' ', '_')}.csv\"))\n",
    "                print(f\"Exported dendrogram data to: {csv_export_dir}/main_task_dendrogram_{cleaned_merged_short_name_plot.lower().replace(' ', '_')}.csv\")\n",
    "            except Exception as e: print(f\"Could not generate Main Tasks Dendrogram for {cleaned_merged_short_name_plot}: {e}\")\n",
    "        else: print(f\"  - Not enough data points (>1) for Main Task Dendrogram for {cleaned_merged_short_name_plot} after NaN drop.\")\n",
    "else: print(f\"Skipping Main Task Dendrogram(s): Conditions not met (plot_dcoder_col_main: {plot_dcoder_col_main is None}, plot_merged_diff_cols_main_valid_empty: {not plot_merged_diff_cols_main_valid}, or summary_comp_df empty/diffs not calculable).\")\n",
    "\n",
    "print(\"Skipping Subtask Dendrograms (previously Plot 8b) as it's covered by Plot 7's row dendrograms if subtask data is used for clustering.\")\n",
    "\n",
    "# Plot 9: Absolute Scores by Linguistic Competency Group (Box Plot)\n",
    "plot_title_prefix_abs_group_box = f\"Absolute Score Distribution by {SUBTASK_GROUP_COL}\"\n",
    "print(f\"\\n--- Generating {plot_title_prefix_abs_group_box} (Box Plot) ---\")\n",
    "\n",
    "if not subtasks_comp_df.empty and 'group' in subtasks_comp_df.columns and SUBTASK_GROUP_COL:\n",
    "    plot_data_p9_box_filtered = subtasks_comp_df[subtasks_comp_df['group'].astype(str).str.lower() != 'nan'].copy()\n",
    "\n",
    "    if not plot_data_p9_box_filtered.empty:\n",
    "        models_to_plot_p9_box = [m for m in comparison_models if m in plot_data_p9_box_filtered.columns and not plot_data_p9_box_filtered[m].isna().all()]\n",
    "\n",
    "        if models_to_plot_p9_box:\n",
    "            melted_data_p9_box = plot_data_p9_box_filtered.melt(\n",
    "                id_vars=['group', 'subtask_cleaned'],\n",
    "                value_vars=models_to_plot_p9_box,\n",
    "                var_name='model_full_name',\n",
    "                value_name='score'\n",
    "            )\n",
    "            melted_data_p9_box.dropna(subset=['score'], inplace=True)\n",
    "\n",
    "            if not melted_data_p9_box.empty:\n",
    "                melted_data_p9_box['Model Short Name'] = melted_data_p9_box['model_full_name'].map(lambda x: clean_plot_name(short_names.get(x, x)))\n",
    "                \n",
    "                def get_model_type_for_color(full_name):\n",
    "                    if full_name == base_model: return \"Qwen2.5 Base\"\n",
    "                    if full_name == instruct_model: return \"Qwen2.5 Instruct\"\n",
    "                    if full_name == coder_model: return \"Qwen2.5 Coder\"\n",
    "                    if full_name in merged_models: return 'Merged'\n",
    "                    return 'Other'\n",
    "                melted_data_p9_box['Model Type for Color'] = melted_data_p9_box['model_full_name'].apply(get_model_type_for_color)\n",
    "\n",
    "                color_map_p9_box = {\n",
    "                    \"Qwen2.5 Base\": 'rgb(100, 149, 237)', \n",
    "                    \"Qwen2.5 Instruct\": 'rgb(50, 205, 50)',\n",
    "                    \"Qwen2.5 Coder\": 'rgb(255, 165, 0)', \n",
    "                    'Merged': 'rgb(192, 192, 192)', \n",
    "                    'Other': 'grey'\n",
    "                }\n",
    "\n",
    "                fig9_box_title = f'{plot_title_prefix_abs_group_box}'\n",
    "                fig9_box = px.box(\n",
    "                    melted_data_p9_box,\n",
    "                    x='group',              \n",
    "                    y='score',              \n",
    "                    color='Model Type for Color',\n",
    "                    color_discrete_map=color_map_p9_box,\n",
    "                    hover_data=['subtask_cleaned', 'Model Short Name'],\n",
    "                    labels={'group': SUBTASK_GROUP_COL, 'score': 'Absolute Score (%)', \n",
    "                            'Model Type for Color': 'Model Category',\n",
    "                            'subtask_cleaned': SUBTASK_COL},\n",
    "                    title=fig9_box_title,\n",
    "                    category_orders={\"group\": sorted(melted_data_p9_box['group'].astype(str).unique())}\n",
    "                )\n",
    "                \n",
    "                fig9_box.update_xaxes(tickangle=45)\n",
    "                fig9_box.update_layout(\n",
    "                    boxmode='group',\n",
    "                    height=max(default_plot_height, 700),\n",
    "                    width=default_plot_width,\n",
    "                    showlegend=False, # Removed legend\n",
    "                    **font_config\n",
    "                )\n",
    "                fig9_box.show()\n",
    "                print(f\"Generated plot: {fig9_box_title}\")\n",
    "                \n",
    "                # Export data\n",
    "                melted_data_p9_box.to_csv(os.path.join(csv_export_dir, \"absolute_score_distribution_by_competency_groups.csv\"), index=False)\n",
    "                print(f\"Exported plot data to: {csv_export_dir}/absolute_score_distribution_by_competency_groups.csv\")\n",
    "            else:\n",
    "                print(f\"Skipping {plot_title_prefix_abs_group_box}: No data to plot after melting and NaN removal.\")\n",
    "        else:\n",
    "            print(f\"Skipping {plot_title_prefix_abs_group_box}: No valid models found in filtered subtask data for plotting.\")\n",
    "    else:\n",
    "        print(f\"Skipping {plot_title_prefix_abs_group_box}: Data is empty after filtering 'nan' group from {SUBTASK_GROUP_COL} column.\")\n",
    "else:\n",
    "    if subtasks_comp_df.empty:\n",
    "        print(f\"Skipping {plot_title_prefix_abs_group_box}: subtasks_comp_df is empty.\")\n",
    "    elif 'group' not in subtasks_comp_df.columns:\n",
    "        print(f\"Skipping {plot_title_prefix_abs_group_box}: 'group' column (expected from {SUBTASK_GROUP_COL}) not in subtasks_comp_df.\")\n",
    "    elif not SUBTASK_GROUP_COL:\n",
    "        print(f\"Skipping {plot_title_prefix_abs_group_box}: SUBTASK_GROUP_COL is not defined.\")\n",
    "\n",
    "print(f\"\\n--- CSV Export Summary ---\")\n",
    "print(f\"All plot data has been exported to the '{csv_export_dir}' directory.\")\n",
    "print(f\"Each plot has its own CSV file with the data used to generate the visualization.\")\n",
    "print(\"The exported files include:\")\n",
    "print(\"- Line charts: data in long format with tasks/models/values\")\n",
    "print(\"- Bar charts: data with grouping variables and scores\")\n",
    "print(\"- Box plots: melted data with grouping and score variables\")\n",
    "print(\"- Scatter/joint plots: paired difference values with metadata\")\n",
    "print(\"- Impact plots: top/bottom ranked data with impact scores\")\n",
    "print(\"- Clustering plots: both original and processed (scaled/ordered) data\")\n",
    "print(\"- Dendrograms: data matrices used for hierarchical clustering\")\n",
    "print(\"- Correlation matrices: correlation coefficients between variables\")\n",
    "\n",
    "print(\"\\n--- Script Finished ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
